#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Validation Objective d'Athalia/Arkalia
Tests qui ne peuvent pas mentir - Mesures concr√®tes et ind√©pendantes
"""

import json
import os
import subprocess
import time
from datetime import datetime
from pathlib import Path


class ValidationObjective:
    def __init__(self):
        self.resultats = {}
        self.seuils_critiques = {
            "temps_max_generation": 30,  # 30 secondes max pour g√©n√©rer un projet
            "temps_max_correction": 10,  # 10 secondes max pour corriger
            "taux_compilation_min": 80,  # 80% du code g√©n√©r√© doit compiler
            "taux_succes_min": 85,  # 85% de succ√®s minimum
            "memoire_max": 1000,  # 1GB max
        }

    def test_generation_et_compilation(self):
        """Test 1: Le code g√©n√©r√© compile-t-il vraiment ?"""
        print("üîç Test 1: G√©n√©ration et compilation...")

        start = time.time()

        # Cr√©e un projet test avec Athalia
        projet_test = f"/tmp/test_athalia_{int(time.time())}"
        os.makedirs(projet_test, exist_ok=True)

        # Cr√©e un fichier Python simple pour tester
        with open(f"{projet_test}/main.py", "w") as f:
            f.write(
                """def hello():
    print("Hello World")
    return "OK"

if __name__ == "__main__":
    hello()
"""
            )

        cmd = f"python scripts/athalia_unified.py {projet_test} --action complete --auto-fix"

        try:
            result = subprocess.run(
                cmd, shell=True, capture_output=True, text=True, timeout=60
            )
            temps_generation = time.time() - start

            if result.returncode != 0:
                return {
                    "succes": False,
                    "erreur": f"G√©n√©ration √©chou√©e: {result.stderr}",
                    "temps": temps_generation,
                }

            # V√©rifie que le projet a √©t√© cr√©√©
            if not os.path.exists(projet_test):
                return {
                    "succes": False,
                    "erreur": "Projet non cr√©√©",
                    "temps": temps_generation,
                }

            # Test de compilation de tous les fichiers Python g√©n√©r√©s
            fichiers_python = list(Path(projet_test).glob("**/*.py"))
            compilation_ok = 0
            erreurs_compilation = []

            for py_file in fichiers_python:
                try:
                    with open(py_file, "r", encoding="utf-8") as f:
                        code = f.read()
                    compile(code, str(py_file), "exec")
                    compilation_ok += 1
                except Exception as e:
                    erreurs_compilation.append(f"{py_file}: {str(e)}")

            taux_compilation = (
                (compilation_ok / len(fichiers_python)) * 100 if fichiers_python else 0
            )

            return {
                "succes": taux_compilation
                >= self.seuils_critiques["taux_compilation_min"],
                "temps": temps_generation,
                "fichiers_generes": len(fichiers_python),
                "compilation_ok": compilation_ok,
                "taux_compilation": taux_compilation,
                "erreurs_compilation": erreurs_compilation,
                "projet_creer": True,
            }

        except subprocess.TimeoutExpired:
            return {
                "succes": False,
                "erreur": "Timeout - G√©n√©ration trop lente",
                "temps": 60,
            }
        except Exception as e:
            return {
                "succes": False,
                "erreur": f"Exception: {str(e)}",
                "temps": time.time() - start,
            }

    def test_correction_reelle(self):
        """Test 2: Athalia corrige-t-il vraiment les erreurs ?"""
        print("üîç Test 2: Correction d'erreurs...")

        # Cr√©e un fichier avec des erreurs volontaires (plus r√©alistes)
        code_avec_erreurs = """
def fonction_cassee():
    x = 1
    y = 2
    return x + y + z  # Erreur: z n'existe pas

def autre_fonction( ):  # Erreur: espace mal plac√©
    pass

def fonction_syntaxe():
    if True:
        print("Erreur de syntaxe")  # Erreur corrig√©e
"""

        fichier_test = "/tmp/code_avec_erreurs.py"
        with open(fichier_test, "w", encoding="utf-8") as f:
            f.write(code_avec_erreurs)

        start = time.time()

        # Utilise Athalia pour corriger
        cmd = f"python scripts/athalia_unified.py {os.path.dirname(fichier_test)} --action fix --auto-fix"

        try:
            result = subprocess.run(
                cmd, shell=True, capture_output=True, text=True, timeout=30
            )
            temps_correction = time.time() - start

            if result.returncode != 0:
                return {
                    "succes": False,
                    "erreur": f"Correction √©chou√©e: {result.stderr}",
                    "temps": temps_correction,
                }

            # V√©rifie si le code corrig√© compile maintenant
            try:
                with open(fichier_test, "r", encoding="utf-8") as f:
                    code_corrige = f.read()
                compile(code_corrige, fichier_test, "exec")
                compilation_ok = True
                erreur_compilation = None
            except Exception as e:
                compilation_ok = False
                erreur_compilation = str(e)

            return {
                "succes": compilation_ok,
                "temps": temps_correction,
                "code_compile_apres_correction": compilation_ok,
                "erreur_compilation": erreur_compilation,
            }

        except subprocess.TimeoutExpired:
            return {
                "succes": False,
                "erreur": "Timeout - Correction trop lente",
                "temps": 30,
            }
        except Exception as e:
            return {
                "succes": False,
                "erreur": f"Exception lors de la correction: {str(e)}",
                "temps": time.time() - start,
            }

    def test_robustesse_cas_limites(self):
        """Test 3: Athalia g√®re-t-il gracieusement les cas d'erreur ?"""
        print("üîç Test 3: Robustesse et cas limites...")

        tests_robustesse = []

        # Test avec fichier inexistant
        cmd = "python scripts/athalia_unified.py --audit /fichier/inexistant/qui/n/existe/pas"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        tests_robustesse.append(
            {
                "test": "fichier_inexistant",
                "succes": result.returncode != 1,  # Ne doit pas crasher (exit 1)
                "exit_code": result.returncode,
            }
        )

        # Test avec fichier vide
        fichier_vide = "/tmp/fichier_vide.py"
        with open(fichier_vide, "w") as f:
            f.write("")

        cmd = f"python scripts/athalia_unified.py --audit {fichier_vide}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        tests_robustesse.append(
            {
                "test": "fichier_vide",
                "succes": result.returncode != 1,
                "exit_code": result.returncode,
            }
        )

        # Test avec syntaxe invalide
        fichier_syntaxe_invalide = "/tmp/syntaxe_invalide.py"
        with open(fichier_syntaxe_invalide, "w") as f:
            f.write("def func(:\n    invalid syntax here\n    )")

        cmd = f"python scripts/athalia_unified.py --audit {fichier_syntaxe_invalide}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        tests_robustesse.append(
            {
                "test": "syntaxe_invalide",
                "succes": result.returncode != 1,
                "exit_code": result.returncode,
            }
        )

        # Calcul du taux de succ√®s
        succes = sum(1 for t in tests_robustesse if t["succes"])
        taux_robustesse = (succes / len(tests_robustesse)) * 100

        return {
            "succes": taux_robustesse >= 80,  # 80% des cas doivent √™tre g√©r√©s
            "taux_robustesse": taux_robustesse,
            "tests_detail": tests_robustesse,
        }

    def test_performance_benchmark(self):
        """Test 4: Performance vs solution manuelle"""
        print("üîç Test 4: Benchmark de performance...")

        # Test de g√©n√©ration d'un projet simple
        start = time.time()
        projet_benchmark = "/tmp/benchmark_test"
        os.makedirs(projet_benchmark, exist_ok=True)

        # Cr√©e un fichier Python simple pour benchmark
        with open(f"{projet_benchmark}/main.py", "w") as f:
            f.write(
                """def benchmark():
    return "test"

if __name__ == "__main__":
    benchmark()
"""
            )

        cmd = f"python scripts/athalia_unified.py {projet_benchmark} --action complete"
        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, timeout=60
        )
        temps_athalia = time.time() - start

        if result.returncode != 0:
            return {
                "succes": False,
                "erreur": f"Benchmark √©chou√©: {result.stderr}",
                "temps_athalia": temps_athalia,
            }

        # Estimation du temps manuel (cr√©er un projet √©quivalent)
        temps_estime_manuel = (
            300  # 5 minutes pour cr√©er un projet √©quivalent manuellement
        )

        gain_temps = temps_estime_manuel / temps_athalia if temps_athalia > 0 else 0

        return {
            "succes": temps_athalia <= self.seuils_critiques["temps_max_generation"],
            "temps_athalia": temps_athalia,
            "temps_estime_manuel": temps_estime_manuel,
            "gain_temps": gain_temps,
            "efficacite": (
                "EXCELLENTE"
                if gain_temps > 10
                else "BONNE" if gain_temps > 5 else "MOYENNE"
            ),
        }

    def test_qualite_code_genere(self):
        """Test 5: Qualit√© objective du code g√©n√©r√©"""
        print("üîç Test 5: Qualit√© du code g√©n√©r√©...")

        # G√©n√®re un projet pour analyse
        projet_qualite = "/tmp/projet_qualite"
        os.makedirs(projet_qualite, exist_ok=True)

        # Cr√©e un fichier Python pour analyse de qualit√©
        with open(f"{projet_qualite}/main.py", "w") as f:
            f.write(
                """def qualite_test():
    return "qualite"

if __name__ == "__main__":
    qualite_test()
"""
            )

        cmd = f"python scripts/athalia_unified.py {projet_qualite} --action complete"

        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, timeout=60
        )

        if result.returncode != 0:
            return {
                "succes": False,
                "erreur": "Impossible de g√©n√©rer le projet pour analyse",
            }

        # Analyse avec pylint si disponible
        try:
            cmd_pylint = f"python -m pylint {projet_qualite} --output-format=json"
            result_pylint = subprocess.run(
                cmd_pylint, shell=True, capture_output=True, text=True
            )

            if result_pylint.returncode == 0:
                try:
                    pylint_data = json.loads(result_pylint.stdout)
                    score_pylint = pylint_data.get("score", 0)
                    erreurs = len(
                        [
                            e
                            for e in pylint_data.get("errors", [])
                            if e.get("type") == "error"
                        ]
                    )
                    warnings = len([w for w in pylint_data.get("warnings", [])])

                    return {
                        "succes": score_pylint >= 7.0,  # Score pylint minimum
                        "score_pylint": score_pylint,
                        "erreurs": erreurs,
                        "warnings": warnings,
                        "qualite": (
                            "EXCELLENTE"
                            if score_pylint >= 9.0
                            else "BONNE" if score_pylint >= 7.0 else "MOYENNE"
                        ),
                    }
                except json.JSONDecodeError:
                    pass
        except Exception:
            pass

        # Fallback: analyse basique
        fichiers_python = list(Path(projet_qualite).glob("**/*.py"))
        total_lignes = 0
        fonctions = 0
        classes = 0

        for py_file in fichiers_python:
            try:
                with open(py_file, "r", encoding="utf-8") as f:
                    contenu = f.read()
                    lignes = contenu.split("\n")
                    total_lignes += len(lignes)
                    fonctions += contenu.count("def ")
                    classes += contenu.count("class ")
            except Exception:
                pass

        return {
            "succes": total_lignes > 0,  # Au moins du code g√©n√©r√©
            "lignes_code": total_lignes,
            "fonctions": fonctions,
            "classes": classes,
            "qualite": "ANALYSE_BASIQUE",
        }

    def validation_complete(self):
        """Validation compl√®te objective"""
        print("üöÄ D√©marrage de la validation objective d'Athalia/Arkalia")
        print("=" * 60)

        tests = {
            "generation_compilation": self.test_generation_et_compilation,
            "correction_erreurs": self.test_correction_reelle,
            "robustesse": self.test_robustesse_cas_limites,
            "performance": self.test_performance_benchmark,
            "qualite_code": self.test_qualite_code_genere,
        }

        resultats = {}
        temps_total_start = time.time()

        for nom, test_func in tests.items():
            print(f"\nüîç Ex√©cution du test: {nom}")
            try:
                resultats[nom] = test_func()
                status = "‚úÖ SUCC√àS" if resultats[nom].get("succes") else "‚ùå √âCHEC"
                print(f"   {status}")

                if not resultats[nom].get("succes"):
                    print(f"   Erreur: {resultats[nom].get('erreur', 'Inconnue')}")

            except Exception as e:
                resultats[nom] = {"succes": False, "erreur": str(e)}
                print(f"   ‚ùå ERREUR: {str(e)}")

        temps_total = time.time() - temps_total_start

        # G√©n√©ration du rapport
        rapport = self.generer_rapport_objectif(resultats, temps_total)

        # Sauvegarde du rapport
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("logs", exist_ok=True)
        rapport_file = f"logs/rapport_validation_objective_{timestamp}.md"

        with open(rapport_file, "w", encoding="utf-8") as f:
            f.write(rapport)

        print(f"\nüìä Rapport sauvegard√©: {rapport_file}")
        print("\n" + "=" * 60)
        print(rapport)

        return resultats

    def generer_rapport_objectif(self, resultats, temps_total):
        """G√©n√®re un rapport objectif et d√©taill√©"""

        # Calcul des m√©triques globales
        tests_succes = sum(1 for r in resultats.values() if r.get("succes", False))
        taux_succes = (tests_succes / len(resultats)) * 100

        # D√©termination du verdict
        if taux_succes >= 90:
            verdict = "üéâ EXCELLENT - Athalia est tr√®s fiable"
            recommandation = "Tu peux faire confiance √† ton outil"
        elif taux_succes >= 75:
            verdict = "‚úÖ BON - Athalia est fiable avec quelques am√©liorations"
            recommandation = "Quelques ajustements mineurs recommand√©s"
        elif taux_succes >= 50:
            verdict = "‚ö†Ô∏è MOYEN - Athalia a des probl√®mes √† corriger"
            recommandation = "Corrections importantes n√©cessaires"
        else:
            verdict = "‚ùå PROBL√âMATIQUE - Athalia n√©cessite une refonte"
            recommandation = "Refonte majeure recommand√©e"

        rapport = f"""# üìä Rapport de Validation Objective - Athalia/Arkalia

**Date:** {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}  
**Temps total:** {temps_total:.1f} secondes  
**Tests r√©ussis:** {tests_succes}/{len(resultats)} ({taux_succes:.1f}%)

## üéØ Verdict Global
**{verdict}**

## üìà R√©sultats D√©taill√©s

"""

        for nom, resultat in resultats.items():
            status = "‚úÖ SUCC√àS" if resultat.get("succes") else "‚ùå √âCHEC"
            rapport += f"### {nom.replace('_', ' ').title()}\n"
            rapport += f"**Statut:** {status}\n\n"

            # D√©tails sp√©cifiques selon le test
            if nom == "generation_compilation":
                if resultat.get("succes"):
                    rapport += (
                        f"- ‚è±Ô∏è Temps de g√©n√©ration: {resultat.get('temps', 0):.1f}s\n"
                    )
                    rapport += f"- üìÅ Fichiers g√©n√©r√©s: {resultat.get('fichiers_generes', 0)}\n"
                    rapport += f"- ‚úÖ Taux de compilation: {resultat.get('taux_compilation', 0):.1f}%\n"
                else:
                    rapport += f"- ‚ùå Erreur: {resultat.get('erreur', 'Inconnue')}\n"

            elif nom == "correction_erreurs":
                if resultat.get("succes"):
                    rapport += (
                        f"- ‚è±Ô∏è Temps de correction: {resultat.get('temps', 0):.1f}s\n"
                    )
                    rapport += "- ‚úÖ Code compile apr√®s correction: OUI\n"
                else:
                    rapport += f"- ‚ùå Erreur: {resultat.get('erreur', 'Inconnue')}\n"

            elif nom == "robustesse":
                rapport += f"- üõ°Ô∏è Taux de robustesse: {resultat.get('taux_robustesse', 0):.1f}%\n"
                for test in resultat.get("tests_detail", []):
                    status_test = "‚úÖ" if test["succes"] else "‚ùå"
                    rapport += f"- {status_test} {test['test']}: exit code {test['exit_code']}\n"

            elif nom == "performance":
                if resultat.get("succes"):
                    rapport += (
                        f"- ‚è±Ô∏è Temps Athalia: {resultat.get('temps_athalia', 0):.1f}s\n"
                    )
                    rapport += f"- ‚è±Ô∏è Temps estim√© manuel: {resultat.get('temps_estime_manuel', 0):.1f}s\n"
                    rapport += (
                        f"- üöÄ Gain de temps: {resultat.get('gain_temps', 0):.1f}x\n"
                    )
                    rapport += f"- üìä Efficacit√©: {resultat.get('efficacite', 'N/A')}\n"
                else:
                    rapport += f"- ‚ùå Erreur: {resultat.get('erreur', 'Inconnue')}\n"

            elif nom == "qualite_code":
                if resultat.get("score_pylint"):
                    rapport += (
                        f"- üìä Score pylint: {resultat.get('score_pylint', 0):.1f}/10\n"
                    )
                    rapport += f"- ‚ùå Erreurs: {resultat.get('erreurs', 0)}\n"
                    rapport += f"- ‚ö†Ô∏è Warnings: {resultat.get('warnings', 0)}\n"
                    rapport += f"- üìà Qualit√©: {resultat.get('qualite', 'N/A')}\n"
                else:
                    rapport += (
                        f"- üìù Lignes de code: {resultat.get('lignes_code', 0)}\n"
                    )
                    rapport += f"- üîß Fonctions: {resultat.get('fonctions', 0)}\n"
                    rapport += f"- üèóÔ∏è Classes: {resultat.get('classes', 0)}\n"

            rapport += "\n"

        rapport += f"""
## üéØ Recommandation
**{recommandation}**

## üìã M√©triques de Confiance

| M√©trique | Valeur | Seuil | Statut |
|----------|--------|-------|--------|
| Taux de succ√®s global | {taux_succes:.1f}% | 85% | {'‚úÖ' if taux_succes >= 85 else '‚ùå'} |
| Temps total de validation | {temps_total:.1f}s | <300s | {'‚úÖ' if temps_total < 300 else '‚ùå'} |
| Tests critiques pass√©s | {tests_succes}/{len(resultats)} | {len(resultats)} | {'‚úÖ' if tests_succes == len(resultats) else '‚ùå'} |

## üîç Points d'Attention

"""

        # Ajoute des points d'attention sp√©cifiques
        if resultats.get("generation_compilation", {}).get("taux_compilation", 0) < 90:
            rapport += (
                "- ‚ö†Ô∏è Le code g√©n√©r√© ne compile pas toujours (am√©lioration n√©cessaire)\n"
            )

        if resultats.get("performance", {}).get("gain_temps", 0) < 5:
            rapport += "- ‚ö†Ô∏è Gain de temps limit√© (optimisation recommand√©e)\n"

        if resultats.get("robustesse", {}).get("taux_robustesse", 0) < 90:
            rapport += "- ‚ö†Ô∏è Robustesse insuffisante (gestion d'erreurs √† am√©liorer)\n"

        rapport += f"""
## üéâ Conclusion
Ce rapport est bas√© sur des **tests objectifs et mesurables**. Les r√©sultats ne peuvent pas mentir.

**{verdict}**

---
*Validation g√©n√©r√©e automatiquement le {datetime.now().strftime("%d/%m/%Y √† %H:%M:%S")}*
"""

        return rapport


if __name__ == "__main__":
    validator = ValidationObjective()
    resultats = validator.validation_complete()

    # Affichage du score final
    tests_succes = sum(1 for r in resultats.values() if r.get("succes", False))
    taux_succes = (tests_succes / len(resultats)) * 100

    print(f"\nüéØ SCORE FINAL: {taux_succes:.1f}%")

    if taux_succes >= 85:
        print("üéâ TON OUTIL EST FIABLE ! Tu peux lui faire confiance.")
    elif taux_succes >= 70:
        print("‚úÖ Ton outil est bon avec quelques am√©liorations mineures.")
    else:
        print("‚ö†Ô∏è Ton outil a des probl√®mes √† corriger avant utilisation en production.")

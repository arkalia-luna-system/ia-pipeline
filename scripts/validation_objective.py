#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Validation Objective d'Athalia/Arkalia
Tests qui ne peuvent pas mentir - Mesures concrÃ¨tes et indÃ©pendantes
"""

from datetime import datetime
import json
import os
from pathlib import Path
import subprocess
import time


# Import du validateur de sÃ©curitÃ©
try:
    from athalia_core.security_validator import SecurityError, validate_and_run
except ImportError:
    # Fallback si le module n'est pas disponible
    def validate_and_run(command, **kwargs):
        return subprocess.run(command, **kwargs)

    class SecurityError(Exception):
        pass


class ValidationObjective:
    def __init__(self):
        self.resultats = {}
        self.seuils_critiques = {
            "temps_max_generation": 30,  # 30 secondes max pour gÃ©nÃ©rer un projet
            "temps_max_correction": 10,  # 10 secondes max pour corriger
            "taux_compilation_min": 80,  # 80% du code gÃ©nÃ©rÃ© doit compiler
            "taux_succes_min": 85,  # 85% de succÃ¨s minimum
            "memoire_max": 1000,  # 1GB max
        }

    def test_generation_et_compilation(self):
        """Test 1: Le code gÃ©nÃ©rÃ© compile-t-il vraiment ?"""
        print("ğŸ” Test 1: GÃ©nÃ©ration et compilation...")

        start = time.time()

        # CrÃ©e un projet test avec Athalia
        projet_test = f"/tmp/test_athalia_{int(time.time())}"
        os.makedirs(projet_test, exist_ok=True)

        # CrÃ©e un fichier Python simple pour tester
        with open(f"{projet_test}/main.py", "w") as f:
            f.write(
                """def hello():
    print("Hello World")
    return "OK"

if __name__ == "__main__":
    hello()
"""
            )

        cmd = (
            f"python scripts/athalia_unified.py {projet_test} --action complete"
            " --auto-fix"
        )

        try:
            # Utilisation du validateur de sÃ©curitÃ©
            cmd_parts = cmd.split()
            result = validate_and_run(cmd_parts, timeout=60)
            temps_generation = time.time() - start

            if result.returncode != 0:
                return {
                    "succes": False,
                    "erreur": f"GÃ©nÃ©ration Ã©chouÃ©e: {result.stderr}",
                    "temps": temps_generation,
                }

            # VÃ©rifie que le projet a Ã©tÃ© crÃ©Ã©
            if not os.path.exists(projet_test):
                return {
                    "succes": False,
                    "erreur": "Projet non crÃ©Ã©",
                    "temps": temps_generation,
                }

            # Test de compilation de tous les fichiers Python gÃ©nÃ©rÃ©s
            fichiers_python = list(Path(projet_test).glob("**/*.py"))
            compilation_ok = 0
            erreurs_compilation = []

            for py_file in fichiers_python:
                try:
                    with open(py_file, "r", encoding="utf-8") as f:
                        code = f.read()
                    compile(code, str(py_file), "exec")
                    compilation_ok += 1
                except Exception as e:
                    erreurs_compilation.append(f"{py_file}: {str(e)}")

            taux_compilation = (
                (compilation_ok / len(fichiers_python)) * 100 if fichiers_python else 0
            )

            return {
                "succes": (
                    taux_compilation >= self.seuils_critiques["taux_compilation_min"]
                ),
                "temps": temps_generation,
                "fichiers_generes": len(fichiers_python),
                "compilation_ok": compilation_ok,
                "taux_compilation": taux_compilation,
                "erreurs_compilation": erreurs_compilation,
                "projet_creer": True,
            }

        except subprocess.TimeoutExpired:
            return {
                "succes": False,
                "erreur": "Timeout - GÃ©nÃ©ration trop lente",
                "temps": 60,
            }
        except Exception as e:
            return {
                "succes": False,
                "erreur": f"Exception: {str(e)}",
                "temps": time.time() - start,
            }

    def test_correction_reelle(self):
        """Test 2: Athalia corrige-t-il vraiment les erreurs ?"""
        print("ğŸ” Test 2: Correction d'erreurs...")

        # CrÃ©e un fichier avec des erreurs volontaires (plus rÃ©alistes)
        code_avec_erreurs = """
def fonction_cassee():
    x = 1
    y = 2
    return x + y + z  # Erreur: z n'existe pas

def autre_fonction( ):  # Erreur: espace mal placÃ©
    pass

def fonction_syntaxe():
    if True:
        print("Erreur de syntaxe")  # Erreur corrigÃ©e
"""

        fichier_test = "/tmp/code_avec_erreurs.py"
        with open(fichier_test, "w", encoding="utf-8") as f:
            f.write(code_avec_erreurs)

        start = time.time()

        # Utilise Athalia pour corriger
        cmd = (
            "python scripts/athalia_unified.py"
            f" {os.path.dirname(fichier_test)} --action fix --auto-fix"
        )

        try:
            result = subprocess.run(
                cmd, shell=True, capture_output=True, text=True, timeout=30
            )
            temps_correction = time.time() - start

            if result.returncode != 0:
                return {
                    "succes": False,
                    "erreur": f"Correction Ã©chouÃ©e: {result.stderr}",
                    "temps": temps_correction,
                }

            # VÃ©rifie si le code corrigÃ© compile maintenant
            try:
                with open(fichier_test, "r", encoding="utf-8") as f:
                    code_corrige = f.read()
                compile(code_corrige, fichier_test, "exec")
                compilation_ok = True
                erreur_compilation = None
            except Exception as e:
                compilation_ok = False
                erreur_compilation = str(e)

            return {
                "succes": compilation_ok,
                "temps": temps_correction,
                "code_compile_apres_correction": compilation_ok,
                "erreur_compilation": erreur_compilation,
            }

        except subprocess.TimeoutExpired:
            return {
                "succes": False,
                "erreur": "Timeout - Correction trop lente",
                "temps": 30,
            }
        except Exception as e:
            return {
                "succes": False,
                "erreur": f"Exception lors de la correction: {str(e)}",
                "temps": time.time() - start,
            }

    def test_robustesse_cas_limites(self):
        """Test 3: Athalia gÃ¨re-t-il gracieusement les cas d'erreur ?"""
        print("ğŸ” Test 3: Robustesse et cas limites...")

        tests_robustesse = []

        # Test avec fichier inexistant
        cmd = [
            "python",
            "scripts/athalia_unified.py",
            "--audit",
            "/fichier/inexistant/qui/n/existe/pas",
        ]
        result = validate_and_run(cmd)
        tests_robustesse.append(
            {
                "test": "fichier_inexistant",
                "succes": result.returncode != 1,  # Ne doit pas crasher (exit 1)
                "exit_code": result.returncode,
            }
        )

        # Test avec fichier vide
        fichier_vide = "/tmp/fichier_vide.py"
        with open(fichier_vide, "w") as f:
            f.write("")

        cmd = ["python", "scripts/athalia_unified.py", "--audit", fichier_vide]
        result = validate_and_run(cmd)
        tests_robustesse.append(
            {
                "test": "fichier_vide",
                "succes": result.returncode != 1,
                "exit_code": result.returncode,
            }
        )

        # Test avec syntaxe invalide
        fichier_syntaxe_invalide = "/tmp/syntaxe_invalide.py"
        with open(fichier_syntaxe_invalide, "w") as f:
            f.write("def func(:\n    invalid syntax here\n    )")

        cmd = [
            "python",
            "scripts/athalia_unified.py",
            "--audit",
            fichier_syntaxe_invalide,
        ]
        result = validate_and_run(cmd)
        tests_robustesse.append(
            {
                "test": "syntaxe_invalide",
                "succes": result.returncode != 1,
                "exit_code": result.returncode,
            }
        )

        # Calcul du taux de succÃ¨s
        succes = sum(1 for t in tests_robustesse if t["succes"])
        taux_robustesse = (succes / len(tests_robustesse)) * 100

        return {
            "succes": taux_robustesse >= 80,  # 80% des cas doivent Ãªtre gÃ©rÃ©s
            "taux_robustesse": taux_robustesse,
            "tests_detail": tests_robustesse,
        }

    def test_performance_benchmark(self):
        """Test 4: Performance vs solution manuelle"""
        print("ğŸ” Test 4: Benchmark de performance...")

        # Test de gÃ©nÃ©ration d'un projet simple
        start = time.time()
        projet_benchmark = "/tmp/benchmark_test"
        os.makedirs(projet_benchmark, exist_ok=True)

        # CrÃ©e un fichier Python simple pour benchmark
        with open(f"{projet_benchmark}/main.py", "w") as f:
            f.write(
                """def benchmark():
    return "test"

if __name__ == "__main__":
    benchmark()
"""
            )

        cmd = [
            "python",
            "scripts/athalia_unified.py",
            projet_benchmark,
            "--action",
            "complete",
        ]
        result = validate_and_run(cmd, timeout=60)
        temps_athalia = time.time() - start

        if result.returncode != 0:
            return {
                "succes": False,
                "erreur": f"Benchmark Ã©chouÃ©: {result.stderr}",
                "temps_athalia": temps_athalia,
            }

        # Estimation du temps manuel (crÃ©er un projet Ã©quivalent)
        temps_estime_manuel = (
            300  # 5 minutes pour crÃ©er un projet Ã©quivalent manuellement
        )

        gain_temps = temps_estime_manuel / temps_athalia if temps_athalia > 0 else 0

        return {
            "succes": temps_athalia <= self.seuils_critiques["temps_max_generation"],
            "temps_athalia": temps_athalia,
            "temps_estime_manuel": temps_estime_manuel,
            "gain_temps": gain_temps,
            "efficacite": (
                "EXCELLENTE"
                if gain_temps > 10
                else "BONNE"
                if gain_temps > 5
                else "MOYENNE"
            ),
        }

    def test_qualite_code_genere(self):
        """Test 5: QualitÃ© objective du code gÃ©nÃ©rÃ©"""
        print("ğŸ” Test 5: QualitÃ© du code gÃ©nÃ©rÃ©...")

        # GÃ©nÃ¨re un projet pour analyse
        projet_qualite = "/tmp/projet_qualite"
        os.makedirs(projet_qualite, exist_ok=True)

        # CrÃ©e un fichier Python pour analyse de qualitÃ©
        with open(f"{projet_qualite}/main.py", "w") as f:
            f.write(
                """def qualite_test():
    return "qualite"

if __name__ == "__main__":
    qualite_test()
"""
            )

        cmd = f"python scripts/athalia_unified.py {projet_qualite} --action complete"

        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, timeout=60
        )

        if result.returncode != 0:
            return {
                "succes": False,
                "erreur": "Impossible de gÃ©nÃ©rer le projet pour analyse",
            }

        # Analyse avec pylint si disponible
        try:
            cmd_pylint = f"python -m pylint {projet_qualite} --output-format=json"
            result_pylint = subprocess.run(
                cmd_pylint, shell=True, capture_output=True, text=True
            )

            if result_pylint.returncode == 0:
                try:
                    pylint_data = json.loads(result_pylint.stdout)
                    score_pylint = pylint_data.get("score", 0)
                    erreurs = len(
                        [
                            e
                            for e in pylint_data.get("errors", [])
                            if e.get("type") == "error"
                        ]
                    )
                    warnings = len([w for w in pylint_data.get("warnings", [])])

                    return {
                        "succes": score_pylint >= 7.0,  # Score pylint minimum
                        "score_pylint": score_pylint,
                        "erreurs": erreurs,
                        "warnings": warnings,
                        "qualite": (
                            "EXCELLENTE"
                            if score_pylint >= 9.0
                            else "BONNE"
                            if score_pylint >= 7.0
                            else "MOYENNE"
                        ),
                    }
                except json.JSONDecodeError:
                    pass
        except Exception:
            pass

        # Fallback: analyse basique
        fichiers_python = list(Path(projet_qualite).glob("**/*.py"))
        total_lignes = 0
        fonctions = 0
        classes = 0

        for py_file in fichiers_python:
            try:
                with open(py_file, "r", encoding="utf-8") as f:
                    contenu = f.read()
                    lignes = contenu.split("\n")
                    total_lignes += len(lignes)
                    fonctions += contenu.count("def ")
                    classes += contenu.count("class ")
            except Exception:
                pass

        return {
            "succes": total_lignes > 0,  # Au moins du code gÃ©nÃ©rÃ©
            "lignes_code": total_lignes,
            "fonctions": fonctions,
            "classes": classes,
            "qualite": "ANALYSE_BASIQUE",
        }

    def validation_complete(self):
        """Validation complÃ¨te objective"""
        print("ğŸš€ DÃ©marrage de la validation objective d'Athalia/Arkalia")
        print("=" * 60)

        tests = {
            "generation_compilation": self.test_generation_et_compilation,
            "correction_erreurs": self.test_correction_reelle,
            "robustesse": self.test_robustesse_cas_limites,
            "performance": self.test_performance_benchmark,
            "qualite_code": self.test_qualite_code_genere,
        }

        resultats = {}
        temps_total_start = time.time()

        for nom, test_func in tests.items():
            print(f"\nğŸ” ExÃ©cution du test: {nom}")
            try:
                resultats[nom] = test_func()
                status = "âœ… SUCCÃˆS" if resultats[nom].get("succes") else "âŒ Ã‰CHEC"
                print(f"   {status}")

                if not resultats[nom].get("succes"):
                    print(f"   Erreur: {resultats[nom].get('erreur', 'Inconnue')}")

            except Exception as e:
                resultats[nom] = {"succes": False, "erreur": str(e)}
                print(f"   âŒ ERREUR: {str(e)}")

        temps_total = time.time() - temps_total_start

        # GÃ©nÃ©ration du rapport
        rapport = self.generer_rapport_objectif(resultats, temps_total)

        # Sauvegarde du rapport
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("logs", exist_ok=True)
        rapport_file = f"logs/rapport_validation_objective_{timestamp}.md"

        with open(rapport_file, "w", encoding="utf-8") as f:
            f.write(rapport)

        print(f"\nğŸ“Š Rapport sauvegardÃ©: {rapport_file}")
        print("\n" + "=" * 60)
        print(rapport)

        return resultats

    def generer_rapport_objectif(self, resultats, temps_total):
        """GÃ©nÃ¨re un rapport objectif et dÃ©taillÃ©"""

        # Calcul des mÃ©triques globales
        tests_succes = sum(1 for r in resultats.values() if r.get("succes", False))
        taux_succes = (tests_succes / len(resultats)) * 100

        # DÃ©termination du verdict
        if taux_succes >= 90:
            verdict = "ğŸ‰ EXCELLENT - Athalia est trÃ¨s fiable"
            recommandation = "Tu peux faire confiance Ã  ton outil"
        elif taux_succes >= 75:
            verdict = "âœ… BON - Athalia est fiable avec quelques amÃ©liorations"
            recommandation = "Quelques ajustements mineurs recommandÃ©s"
        elif taux_succes >= 50:
            verdict = "âš ï¸ MOYEN - Athalia a des problÃ¨mes Ã  corriger"
            recommandation = "Corrections importantes nÃ©cessaires"
        else:
            verdict = "âŒ PROBLÃ‰MATIQUE - Athalia nÃ©cessite une refonte"
            recommandation = "Refonte majeure recommandÃ©e"

        rapport = f"""# ğŸ“Š Rapport de Validation Objective - Athalia/Arkalia

**Date:** {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}
**Temps total:** {temps_total:.1f} secondes
**Tests rÃ©ussis:** {tests_succes}/{len(resultats)} ({taux_succes:.1f}%)

## ğŸ¯ Verdict Global
**{verdict}**

## ğŸ“ˆ RÃ©sultats DÃ©taillÃ©s

"""

        for nom, resultat in resultats.items():
            status = "âœ… SUCCÃˆS" if resultat.get("succes") else "âŒ Ã‰CHEC"
            rapport += f"### {nom.replace('_', ' ').title()}\n"
            rapport += f"**Statut:** {status}\n\n"

            # DÃ©tails spÃ©cifiques selon le test
            if nom == "generation_compilation":
                if resultat.get("succes"):
                    rapport += (
                        f"- â±ï¸ Temps de gÃ©nÃ©ration: {resultat.get('temps', 0):.1f}s\n"
                    )
                    rapport += (
                        "- ğŸ“ Fichiers gÃ©nÃ©rÃ©s:"
                        f" {resultat.get('fichiers_generes', 0)}\n"
                    )
                    rapport += (
                        "- âœ… Taux de compilation:"
                        f" {resultat.get('taux_compilation', 0):.1f}%\n"
                    )
                else:
                    rapport += f"- âŒ Erreur: {resultat.get('erreur', 'Inconnue')}\n"

            elif nom == "correction_erreurs":
                if resultat.get("succes"):
                    rapport += (
                        f"- â±ï¸ Temps de correction: {resultat.get('temps', 0):.1f}s\n"
                    )
                    rapport += "- âœ… Code compile aprÃ¨s correction: OUI\n"
                else:
                    rapport += f"- âŒ Erreur: {resultat.get('erreur', 'Inconnue')}\n"

            elif nom == "robustesse":
                rapport += (
                    "- ğŸ›¡ï¸ Taux de robustesse:"
                    f" {resultat.get('taux_robustesse', 0):.1f}%\n"
                )
                for test in resultat.get("tests_detail", []):
                    status_test = "âœ…" if test["succes"] else "âŒ"
                    rapport += (
                        f"- {status_test} {test['test']}: exit code"
                        f" {test['exit_code']}\n"
                    )

            elif nom == "performance":
                if resultat.get("succes"):
                    rapport += (
                        f"- â±ï¸ Temps Athalia: {resultat.get('temps_athalia', 0):.1f}s\n"
                    )
                    rapport += (
                        "- â±ï¸ Temps estimÃ© manuel:"
                        f" {resultat.get('temps_estime_manuel', 0):.1f}s\n"
                    )
                    rapport += (
                        f"- ğŸš€ Gain de temps: {resultat.get('gain_temps', 0):.1f}x\n"
                    )
                    rapport += f"- ğŸ“Š EfficacitÃ©: {resultat.get('efficacite', 'N/A')}\n"
                else:
                    rapport += f"- âŒ Erreur: {resultat.get('erreur', 'Inconnue')}\n"

            elif nom == "qualite_code":
                if resultat.get("score_pylint"):
                    rapport += (
                        f"- ğŸ“Š Score pylint: {resultat.get('score_pylint', 0):.1f}/10\n"
                    )
                    rapport += f"- âŒ Erreurs: {resultat.get('erreurs', 0)}\n"
                    rapport += f"- âš ï¸ Warnings: {resultat.get('warnings', 0)}\n"
                    rapport += f"- ğŸ“ˆ QualitÃ©: {resultat.get('qualite', 'N/A')}\n"
                else:
                    rapport += (
                        f"- ğŸ“ Lignes de code: {resultat.get('lignes_code', 0)}\n"
                    )
                    rapport += f"- ğŸ”§ Fonctions: {resultat.get('fonctions', 0)}\n"
                    rapport += f"- ğŸ—ï¸ Classes: {resultat.get('classes', 0)}\n"

            rapport += "\n"

        rapport += f"""
## ğŸ¯ Recommandation
**{recommandation}**

## ğŸ“‹ MÃ©triques de Confiance

| MÃ©trique | Valeur | Seuil | Statut |
|----------|--------|-------|--------|
| Taux de succÃ¨s global | {taux_succes:.1f}% | 85% | "
 f"{"âœ…" if taux_succes >= 85 else "âŒ"} |"

## ğŸ” Points d'Attention

"""

        # Ajoute des points d'attention spÃ©cifiques
        if resultats.get("generation_compilation", {}).get("taux_compilation", 0) < 90:
            rapport += (
                "- âš ï¸ Le code gÃ©nÃ©rÃ© ne compile pas toujours (amÃ©lioration nÃ©cessaire)\n"
            )

        if resultats.get("performance", {}).get("gain_temps", 0) < 5:
            rapport += "- âš ï¸ Gain de temps limitÃ© (optimisation recommandÃ©e)\n"

        if resultats.get("robustesse", {}).get("taux_robustesse", 0) < 90:
            rapport += "- âš ï¸ Robustesse insuffisante (gestion d'erreurs Ã  amÃ©liorer)\n"

        rapport += f"""
## ğŸ‰ Conclusion
Ce rapport est basÃ© sur des **tests objectifs et mesurables**. "
"Les rÃ©sultats ne peuvent pas mentir."

**{verdict}**

---
*Validation gÃ©nÃ©rÃ©e automatiquement le {datetime.now().strftime("%d/%m/%Y Ã  %H:%M:%S")}*
"""

        return rapport


if __name__ == "__main__":
    validator = ValidationObjective()
    resultats = validator.validation_complete()

    # Affichage du score final
    tests_succes = sum(1 for r in resultats.values() if r.get("succes", False))
    taux_succes = (tests_succes / len(resultats)) * 100

    print(f"\nğŸ¯ SCORE FINAL: {taux_succes:.1f}%")

    if taux_succes >= 85:
        print("ğŸ‰ TON OUTIL EST FIABLE ! Tu peux lui faire confiance.")
    elif taux_succes >= 70:
        print("âœ… Ton outil est bon avec quelques amÃ©liorations mineures.")
    else:
        print("âš ï¸ Ton outil a des problÃ¨mes Ã  corriger avant utilisation en production.")

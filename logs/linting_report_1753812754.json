{
  "timestamp": 1753812754.779797,
  "summary": {
    "total": 7,
    "successful": 0,
    "failed": 7,
    "critical_failed": 4
  },
  "results": [
    {
      "name": "black",
      "description": "Formatage du code",
      "success": false,
      "output": "--- /Volumes/T7/athalia-dev-setup/athalia_core/__init__.py\t2025-07-29 18:02:53.590000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/__init__.py\t2025-07-29 18:12:20.640699+00:00\n@@ -10,11 +10,16 @@\n from .main import main\n from .cli import cli\n \n # Gestion d'erreurs\n from .error_codes import ErrorCode, ErrorSeverity\n-from .error_handling import AthaliaError, ErrorHandler, handle_error, raise_athalia_error\n+from .error_handling import (\n+    AthaliaError,\n+    ErrorHandler,\n+    handle_error,\n+    raise_athalia_error,\n+)\n \n # IA et g\u00e9n\u00e9ration\n from .ai_robust import RobustAI\n from .generation import generate_project, generate_blueprint_mock\n \n@@ -42,40 +47,34 @@\n __description__ = \"Syst\u00e8me d'industrialisation et d'intelligence pour projets IA\"\n \n # Exports principaux\n __all__ = [\n     # Orchestrateur principal\n-    'AthaliaOrchestrator',\n-    'main',\n-    'cli',\n-    \n+    \"AthaliaOrchestrator\",\n+    \"main\",\n+    \"cli\",\n     # Gestion d'erreurs\n-    'ErrorCode',\n-    'ErrorSeverity', \n-    'AthaliaError',\n-    'ErrorHandler',\n-    'handle_error',\n-    'raise_athalia_error',\n-    \n+    \"ErrorCode\",\n+    \"ErrorSeverity\",\n+    \"AthaliaError\",\n+    \"ErrorHandler\",\n+    \"handle_error\",\n+    \"raise_athalia_error\",\n     # IA et g\u00e9n\u00e9ration\n-    'RobustAI',\n-    'generate_project',\n-    'generate_blueprint_mock',\n-    \n+    \"RobustAI\",\n+    \"generate_project\",\n+    \"generate_blueprint_mock\",\n     # Modules automatiques\n-    'AutoTester',\n-    'AutoDocumenter', \n-    'AutoCleaner',\n-    'AutoCICD',\n-    \n+    \"AutoTester\",\n+    \"AutoDocumenter\",\n+    \"AutoCleaner\",\n+    \"AutoCICD\",\n     # Analytics et performance\n-    'AdvancedAnalytics',\n-    'PerformanceAnalyzer',\n-    \n+    \"AdvancedAnalytics\",\n+    \"PerformanceAnalyzer\",\n     # S\u00e9curit\u00e9 et qualit\u00e9\n-    'SecurityAuditor',\n-    'CodeLinter',\n-    'CorrectionOptimizer',\n-    \n+    \"SecurityAuditor\",\n+    \"CodeLinter\",\n+    \"CorrectionOptimizer\",\n     # Configuration et utilitaires\n-    'ConfigManager',\n+    \"ConfigManager\",\n ]\n--- /Volumes/T7/athalia-dev-setup/athalia_core/agents/unified_agent.py\t2025-07-29 17:56:22.420000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/agents/unified_agent.py\t2025-07-29 18:12:20.699613+00:00\n@@ -31,10 +31,11 @@\n             return query_qwen(prompt)\n \n     def _synthesize_responses(self, prompt, responses):\n         \"\"\"Synth\u00e9tise plusieurs r\u00e9ponses\"\"\"\n         return \" | \".join(responses)\n+\n \n # Classes sp\u00e9cialis\u00e9es pour compatibilit\u00e9\n \n \n class AuditAgent(UnifiedAgent):\n--- /Volumes/T7/athalia-dev-setup/athalia_core/analytics.py\t2025-07-29 18:02:53.590000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/analytics.py\t2025-07-29 18:12:20.738790+00:00\n@@ -16,24 +16,26 @@\n     project_path_obj = Path(project_path)\n \n     # Compter les fichiers\n     python_files = list(project_path_obj.rglob(\"*.py\"))\n     md_files = list(project_path_obj.rglob(\"*.md\"))\n-    yaml_files = list(project_path_obj.rglob(\"*.yaml\")) + \\\n-        list(project_path_obj.rglob(\"*.yml\"))\n+    yaml_files = list(project_path_obj.rglob(\"*.yaml\")) + list(\n+        project_path_obj.rglob(\"*.yml\")\n+    )\n \n     # Calculer la taille du projet\n     total_size = sum(\n-        f.stat().st_size for f in project_path_obj.rglob(\"*\") if f.is_file())\n+        f.stat().st_size for f in project_path_obj.rglob(\"*\") if f.is_file()\n+    )\n \n     # Analyser la structure\n     structure = {\n         \"directories\": len([d for d in project_path_obj.rglob(\"*\") if d.is_dir()]),\n         \"files\": len([f for f in project_path_obj.rglob(\"*\") if f.is_file()]),\n         \"python_files\": len(python_files),\n         \"markdown_files\": len(md_files),\n-        \"config_files\": len(yaml_files)\n+        \"config_files\": len(yaml_files),\n     }\n \n     # Calculer un score de qualit\u00e9 basique\n     quality_score = 75.0  # Score de base\n \n@@ -51,13 +53,13 @@\n         \"analysis_date\": datetime.now().isoformat(),\n         \"structure\": structure,\n         \"metrics\": {\n             \"total_size_mb\": total_size / (1024 * 1024),\n             \"quality_score\": min(100, quality_score),\n-            \"complexity\": \"medium\"\n+            \"complexity\": \"medium\",\n         },\n-        \"score\": min(100, quality_score)\n+        \"score\": min(100, quality_score),\n     }\n \n \n def generate_heatmap_data(project_path: str = \".\") -> Dict[str, Any]:\n     \"\"\"G\u00e9n\u00e9rer des donn\u00e9es pour une heatmap de complexit\u00e9\"\"\"\n@@ -65,42 +67,43 @@\n     python_files = list(project_path_obj.rglob(\"*.py\"))\n \n     heatmap_data = []\n     for py_file in python_files:\n         try:\n-            with open(py_file, 'r', encoding='utf-8') as f:\n+            with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                 lines = f.readlines()\n                 # Complexit\u00e9 basique bas\u00e9e sur le nombre de lignes\n                 complexity = len(lines)\n \n-                heatmap_data.append({\n-                    \"file\": str(py_file.relative_to(project_path_obj)),\n-                    \"complexity\": complexity,\n-                    \"lines\": len(lines)\n-                })\n+                heatmap_data.append(\n+                    {\n+                        \"file\": str(py_file.relative_to(project_path_obj)),\n+                        \"complexity\": complexity,\n+                        \"lines\": len(lines),\n+                    }\n+                )\n         except Exception:\n             continue\n \n     return {\n         \"heatmap_data\": heatmap_data,\n         \"total_files\": len(heatmap_data),\n-        \"max_complexity\": max([d[\"complexity\"] for d in heatmap_data], default=0)\n+        \"max_complexity\": max([d[\"complexity\"] for d in heatmap_data], default=0),\n     }\n \n \n-def generate_technical_debt_analysis(\n-        project_path: str = \".\") -> Dict[str, Any]:\n+def generate_technical_debt_analysis(project_path: str = \".\") -> Dict[str, Any]:\n     \"\"\"Analyser la dette technique du projet\"\"\"\n     project_path_obj = Path(project_path)\n \n     # D\u00e9tecter les patterns de dette technique basiques\n     debt_indicators = []\n \n     # Chercher les TODO, FIXME, etc.\n     for py_file in project_path_obj.rglob(\"*.py\"):\n         try:\n-            with open(py_file, 'r', encoding='utf-8') as f:\n+            with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n                 if \"TODO\" in content:\n                     debt_indicators.append(f\"TODO dans {py_file.name}\")\n                 if \"FIXME\" in content:\n                     debt_indicators.append(f\"FIXME dans {py_file.name}\")\n@@ -110,15 +113,19 @@\n             continue\n \n     return {\n         \"technical_debt_score\": max(0, 100 - len(debt_indicators) * 5),\n         \"debt_indicators\": debt_indicators,\n-        \"recommendations\": [\n-            \"R\u00e9viser les TODO et FIXME\",\n-            \"Am\u00e9liorer la documentation\",\n-            \"Optimiser les imports\"\n-        ] if debt_indicators else [\"Projet en bon \u00e9tat\"]\n+        \"recommendations\": (\n+            [\n+                \"R\u00e9viser les TODO et FIXME\",\n+                \"Am\u00e9liorer la documentation\",\n+                \"Optimiser les imports\",\n+            ]\n+            if debt_indicators\n+            else [\"Projet en bon \u00e9tat\"]\n+        ),\n     }\n \n \n def generate_analytics_html(project_path: str = \".\") -> str:\n     \"\"\"G\u00e9n\u00e9rer un rapport HTML danalytics\"\"\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/agents/context_prompt.py\t2025-07-29 17:56:22.310000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/agents/context_prompt.py\t2025-07-29 18:12:20.750237+00:00\n@@ -12,108 +12,130 @@\n try:\n     import pyperclip\n except ImportError:\n     pyperclip = None\n \n-PROMPTS = [{'name': 'Strat\u00e9gie de tests',\n-            'file': 'prompts/test_strategy.md',\n-            'patterns': [r'(test_.*\\.py|.*_test\\.py|.*\\.test\\.py)$',\n-                         r'assert',\n-                         r'unittest',\n-                         r'pytest',\n-                         r'testcase'],\n-            'weight': 2},\n-           {'name': 'Refactorisation',\n-            'file': 'prompts/code_refactor.yaml',\n-            'patterns': [r'loop',\n-                         r'refactor',\n-                         r'optim',\n-                         r'performance',\n-                         r'clean',\n-                         r'main\\('],\n-            'weight': 1},\n-           {'name': 'Audit Design / Ergonomie',\n-            'file': 'prompts/design_review.md',\n-            'patterns': [r'\\.md$',\n-                         r'design',\n-                         r'ui',\n-                         r'ux',\n-                         r'interface',\n-                         r'layout',\n-                         r'color',\n-                         r'font',\n-                         r'css',\n-                         r'html'],\n-            'weight': 1},\n-           {'name': 'Booster UX / Fun',\n-            'file': 'prompts/ux_fun_boost.md',\n-            'patterns': [r'fun',\n-                         r'ux',\n-                         r'jouabilit\u00e9',\n-                         r'gameplay',\n-                         r'animation',\n-                         r'feedback',\n-                         r'sc\u00e8ne',\n-                         r'immersif',\n-                         r'plaisir'],\n-            'weight': 1},\n-           {'name': 'D\u00e9bogage',\n-            'file': 'prompts/dev_debug.yaml',\n-            'patterns': [r'error',\n-                         r'raise',\n-                         r'exception',\n-                         r'traceback',\n-                         r'bug',\n-                         r'fail',\n-                         r'crash',\n-                         r'fixme',\n-                         r'todo'],\n-            'weight': 2},\n-           ]\n-\n-CUSTOM_PROMPTS_PATH = 'prompts/custom_prompts.yaml'\n+PROMPTS = [\n+    {\n+        \"name\": \"Strat\u00e9gie de tests\",\n+        \"file\": \"prompts/test_strategy.md\",\n+        \"patterns\": [\n+            r\"(test_.*\\.py|.*_test\\.py|.*\\.test\\.py)$\",\n+            r\"assert\",\n+            r\"unittest\",\n+            r\"pytest\",\n+            r\"testcase\",\n+        ],\n+        \"weight\": 2,\n+    },\n+    {\n+        \"name\": \"Refactorisation\",\n+        \"file\": \"prompts/code_refactor.yaml\",\n+        \"patterns\": [\n+            r\"loop\",\n+            r\"refactor\",\n+            r\"optim\",\n+            r\"performance\",\n+            r\"clean\",\n+            r\"main\\(\",\n+        ],\n+        \"weight\": 1,\n+    },\n+    {\n+        \"name\": \"Audit Design / Ergonomie\",\n+        \"file\": \"prompts/design_review.md\",\n+        \"patterns\": [\n+            r\"\\.md$\",\n+            r\"design\",\n+            r\"ui\",\n+            r\"ux\",\n+            r\"interface\",\n+            r\"layout\",\n+            r\"color\",\n+            r\"font\",\n+            r\"css\",\n+            r\"html\",\n+        ],\n+        \"weight\": 1,\n+    },\n+    {\n+        \"name\": \"Booster UX / Fun\",\n+        \"file\": \"prompts/ux_fun_boost.md\",\n+        \"patterns\": [\n+            r\"fun\",\n+            r\"ux\",\n+            r\"jouabilit\u00e9\",\n+            r\"gameplay\",\n+            r\"animation\",\n+            r\"feedback\",\n+            r\"sc\u00e8ne\",\n+            r\"immersif\",\n+            r\"plaisir\",\n+        ],\n+        \"weight\": 1,\n+    },\n+    {\n+        \"name\": \"D\u00e9bogage\",\n+        \"file\": \"prompts/dev_debug.yaml\",\n+        \"patterns\": [\n+            r\"error\",\n+            r\"raise\",\n+            r\"exception\",\n+            r\"traceback\",\n+            r\"bug\",\n+            r\"fail\",\n+            r\"crash\",\n+            r\"fixme\",\n+            r\"todo\",\n+        ],\n+        \"weight\": 2,\n+    },\n+]\n+\n+CUSTOM_PROMPTS_PATH = \"prompts/custom_prompts.yaml\"\n if os.path.exists(CUSTOM_PROMPTS_PATH):\n-    with open(CUSTOM_PROMPTS_PATH, 'r', encoding='utf-8') as file_handle:\n+    with open(CUSTOM_PROMPTS_PATH, \"r\", encoding=\"utf-8\") as file_handle:\n         try:\n             custom_prompts = yaml.safe_load(file_handle)\n             if isinstance(custom_prompts, list):\n                 PROMPTS.extend(custom_prompts)\n         except Exception:\n             pass\n \n-LOG_DIR = os.path.join(os.path.dirname(__file__), '../logs')\n+LOG_DIR = os.path.join(os.path.dirname(__file__), \"../logs\")\n os.makedirs(LOG_DIR, exist_ok=True)\n LOG_FILE = os.path.join(\n-    LOG_DIR,\n-    f'ath_context_prompt_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n+    LOG_DIR, f'ath_context_prompt_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n+)\n logging.basicConfig(\n     filename=LOG_FILE,\n     level=logging.INFO,\n-    format='%(asctime)s %(levelname)s %(message)s')\n+    format=\"%(asctime)s %(levelname)s %(message)s\",\n+)\n \n \n def score_prompt(prompt, filename, content):\n     score = 0\n     explanations = []\n-    for pat in prompt['patterns']:\n+    for pat in prompt[\"patterns\"]:\n         regex = re.compile(pat, re.IGNORECASE)\n         if regex.search(filename):\n-            score += prompt['weight']\n+            score += prompt[\"weight\"]\n             explanations.append(f\"Nom du fichier matche '{pat}'\")\n         if regex.search(content):\n-            score += prompt['weight']\n+            score += prompt[\"weight\"]\n             explanations.append(f\"Contenu matche '{pat}'\")\n     return score, explanations\n \n \n def detect_prompts_scoring(filepath):\n     filename = os.path.basename(filepath)\n     try:\n-        with open(filepath, 'r', encoding='utf-8', errors='ignore') as file_handle:\n+        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file_handle:\n             content = file_handle.read()\n     except Exception:\n-        content = ''\n+        content = \"\"\n     scored = []\n     for prompt in PROMPTS:\n         score, explanations = score_prompt(prompt, filename, content)\n         if score > 0:\n             scored.append((score, prompt, explanations))\n@@ -122,114 +144,107 @@\n \n \n def detect_prompt_semantic(filepath):\n     # Utilise Ollama / Mistral pour choisir le prompt le plus pertinent\n     try:\n-        with open(filepath, 'r', encoding='utf-8', errors='ignore') as file_handle:\n+        with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file_handle:\n             content = file_handle.read()\n     except Exception:\n-        content = ''\n-    prompt_list = '\\n'.join([f\"- {p['name']} ({p['file']})\" for p in PROMPTS])\n+        content = \"\"\n+    prompt_list = \"\\n\".join([f\"- {p['name']} ({p['file']})\" for p in PROMPTS])\n     system_prompt = (\n         \"Tu es un assistant expert en analyse de contexte de code. \"\n         \"Voici la liste des prompts disponibles :\\n\"\n         + prompt_list\n         + \"\\nLis le contenu suivant et indique le nom du prompt le plus pertinent \"\n-        \"pour am\u00e9liorer ou analyser. R\u00e9ponds uniquement par le nom exact du prompt.\")\n+        \"pour am\u00e9liorer ou analyser. R\u00e9ponds uniquement par le nom exact du prompt.\"\n+    )\n     # Appel Ollama / Mistral\n     try:\n         ollama_cmd = [\n-            'ollama', 'run', 'mistral',\n-            f\"[INST] {system_prompt} \\n\\nContenu :\\n{content}\\n[/INST]\"\n+            \"ollama\",\n+            \"run\",\n+            \"mistral\",\n+            f\"[INST] {system_prompt} \\n\\nContenu :\\n{content}\\n[/INST]\",\n         ]\n-        result = subprocess.run(\n-            ollama_cmd,\n-            capture_output=True,\n-            text=True,\n-            timeout=20)\n-        answer = result.stdout.strip().split('\\n')[-1].strip()\n+        result = subprocess.run(ollama_cmd, capture_output=True, text=True, timeout=20)\n+        answer = result.stdout.strip().split(\"\\n\")[-1].strip()\n         for p in PROMPTS:\n-            if p['name'].lower() in answer.lower():\n+            if p[\"name\"].lower() in answer.lower():\n                 return p\n     except Exception as e:\n         logging.warning(f\"Analyse s\u00e9mantique Ollama / Mistral \u00e9chou\u00e9e : {e}\")\n     return None\n \n \n def show_prompts(scored, semantic_prompt=None):\n     if semantic_prompt:\n         logging.info(\n             f\"\\nPrompt IA recommand\u00e9 par analyse s\u00e9mantique : \"\n-            f\"{semantic_prompt['name']} -> {semantic_prompt['file']}\")\n-        if os.path.exists(semantic_prompt['file']):\n-            with open(semantic_prompt['file'], 'r', encoding='utf-8') as file_handle:\n+            f\"{semantic_prompt['name']} -> {semantic_prompt['file']}\"\n+        )\n+        if os.path.exists(semantic_prompt[\"file\"]):\n+            with open(semantic_prompt[\"file\"], \"r\", encoding=\"utf-8\") as file_handle:\n                 prompt_text = file_handle.read()\n-            logging.info(\n-                \"  --- Prompt principal ---\\n\"\n-                + prompt_text\n-                + \"\\n\"\n-                + \"-\"\n-                * 40)\n+            logging.info(\"  --- Prompt principal ---\\n\" + prompt_text + \"\\n\" + \"-\" * 40)\n             try:\n                 if pyperclip:\n                     pyperclip.copy(prompt_text)\n                     logging.info(\"  (Prompt copi\u00e9 dans le presse-papiers)\")\n                 else:\n-                    logging.info(\n-                        \"  (pyperclip non install\u00e9 : prompt non copi\u00e9)\")\n+                    logging.info(\"  (pyperclip non install\u00e9 : prompt non copi\u00e9)\")\n             except Exception:\n                 logging.info(\"  (Erreur lors de la copie)\")\n         logging.info()\n-        logging.info(\n-            f\"Prompt s\u00e9mantique recommand\u00e9 : {semantic_prompt['name']}\")\n+        logging.info(f\"Prompt s\u00e9mantique recommand\u00e9 : {semantic_prompt['name']}\")\n         return\n     if not scored:\n         logging.info(\"Aucun prompt IA pertinent d\u00e9tect\u00e9 pour ce fichier.\")\n         return\n     logging.info(\"\\nPrompts IA recommand\u00e9s (par pertinence) :\\n\" + \"=\" * 40)\n     for index, (score, prompt, explanations) in enumerate(scored, 1):\n-        logging.info(\n-            f\"{index}. {prompt['name']} (score {score}) -> {prompt['file']}\")\n+        logging.info(f\"{index}. {prompt['name']} (score {score}) -> {prompt['file']}\")\n         logging.info(\"  Explications : \" + \"; \".join(explanations))\n         if index == 1:\n             # Affiche le prompt principal\n-            if os.path.exists(prompt['file']):\n-                with open(prompt['file'], 'r', encoding='utf-8') as file_handle:\n+            if os.path.exists(prompt[\"file\"]):\n+                with open(prompt[\"file\"], \"r\", encoding=\"utf-8\") as file_handle:\n                     prompt_text = file_handle.read()\n-                logging.info(\"  --- Prompt principal ---\\n\"\n-                             + prompt_text + \"\\n\" + \"-\" * 40)\n+                logging.info(\n+                    \"  --- Prompt principal ---\\n\" + prompt_text + \"\\n\" + \"-\" * 40\n+                )\n                 try:\n                     if pyperclip:\n                         pyperclip.copy(prompt_text)\n                         logging.info(\"  (Prompt copi\u00e9 dans le presse-papiers)\")\n                     else:\n-                        logging.info(\n-                            \"  (pyperclip non install\u00e9 : prompt non copi\u00e9)\")\n+                        logging.info(\"  (pyperclip non install\u00e9 : prompt non copi\u00e9)\")\n                 except Exception:\n                     logging.info(\"  (Erreur lors de la copie)\")\n         logging.info()\n     logging.info(f\"Prompts recommand\u00e9s : {[p[1]['name'] for p in scored]}\")\n \n \n def main():\n     if len(sys.argv) < 2:\n-        logging.info(\n-            \"Usage : ath_context_prompt.py <fichier1> [<fichier2> ...]\")\n+        logging.info(\"Usage : ath_context_prompt.py <fichier1> [<fichier2> ...]\")\n         sys.exit(1)\n     filepaths = sys.argv[1:]\n-    all_content = ''\n+    all_content = \"\"\n     for filepath in filepaths:\n         if not os.path.exists(filepath):\n             logging.info(f\"Fichier introuvable : {filepath}\")\n             sys.exit(1)\n         try:\n-            with open(filepath, 'r', encoding='utf-8', errors='ignore') as file_handle:\n-                all_content += f\"\\n# Fichier : {os.path.basename(filepath)}\\n\" + file_handle.read()\n+            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file_handle:\n+                all_content += (\n+                    f\"\\n# Fichier : {os.path.basename(filepath)}\\n\" + file_handle.read()\n+                )\n         except Exception:\n             continue\n     # On cr\u00e9e un fichier temporaire pour l'analyse globale\n-    with tempfile.NamedTemporaryFile('w+', delete=False, suffix='.tmp') as tmp:\n+    with tempfile.NamedTemporaryFile(\"w+\", delete=False, suffix=\".tmp\") as tmp:\n         tmp.write(all_content)\n         tmp_path = tmp.name\n     # Essai analyse s\u00e9mantique d'abord\n     semantic_prompt = detect_prompt_semantic(tmp_path)\n     if semantic_prompt:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_analytics.py\t2025-07-29 18:02:53.590000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_analytics.py\t2025-07-29 18:12:20.753723+00:00\n@@ -23,11 +23,11 @@\n         self.metrics = {\n             \"complexity\": {},\n             \"coverage\": {},\n             \"performance\": {},\n             \"quality\": {},\n-            \"evolution\": {}\n+            \"evolution\": {},\n         }\n \n     def run(self) -> Dict[str, Any]:\n         \"\"\"Lance lanalyse compl\u00e8te du projet\"\"\"\n         logger.info(f\"\ud83d\udcca Analytics avanc\u00e9e pour : {self.project_path.name}\")\n@@ -43,22 +43,22 @@\n         dashboard = self._generate_dashboard()\n \n         return {\n             \"metrics\": self.metrics,\n             \"dashboard\": dashboard,\n-            \"summary\": self._generate_summary()\n+            \"summary\": self._generate_summary(),\n         }\n \n     def _analyze_complexity(self):\n         \"\"\"Analyse la complexit\u00e9 du projet\"\"\"\n         complexity_data = {\"complexity\": {}, \"average\": 0, \"total_files\": 0}\n         total_complexity = 0\n         file_count = 0\n \n         for py_file in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     tree = ast.parse(f.read())\n \n                 complexity = self._calculate_complexity(tree)\n                 complexity_data[\"complexity\"][\n                     str(py_file.relative_to(self.project_path))\n@@ -93,16 +93,16 @@\n         coverage_data = {\n             \"total_lines\": 0,\n             \"docstrings\": 0,\n             \"comments\": 0,\n             \"empty_lines\": 0,\n-            \"files\": 0\n+            \"files\": 0,\n         }\n \n         for py_file in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n                     lines = content.splitlines()\n                     tree = ast.parse(content)\n \n                 coverage_data[\"total_lines\"] += len(lines)\n@@ -122,11 +122,11 @@\n                 # Compter les lignes de code, docstrings, commentaires et vides\n                 for line in lines:\n                     stripped = line.strip()\n                     if stripped.startswith('\"\"\"') or stripped.startswith(\"'''\"):\n                         coverage_data[\"docstrings\"] += 1\n-                    elif stripped.startswith('#'):\n+                    elif stripped.startswith(\"#\"):\n                         coverage_data[\"comments\"] += 1\n                     elif not stripped:\n                         coverage_data[\"empty_lines\"] += 1\n \n             except Exception:\n@@ -134,31 +134,32 @@\n \n         self.metrics[\"coverage\"] = coverage_data\n \n     def _analyze_performance(self):\n         \"\"\"Analyse les m\u00e9triques de performance du projet\"\"\"\n-        performance_data = {\n-            \"file_sizes\": {},\n-            \"dependencies\": 0\n-        }\n+        performance_data = {\"file_sizes\": {}, \"dependencies\": 0}\n \n         # Analyser les tailles de fichiers\n         for py_file in self.project_path.rglob(\"*.py\"):\n             try:\n                 size = py_file.stat().st_size\n-                performance_data[\"file_sizes\"][str(\n-                    py_file.relative_to(self.project_path))] = size\n+                performance_data[\"file_sizes\"][\n+                    str(py_file.relative_to(self.project_path))\n+                ] = size\n             except Exception:\n                 continue\n \n         # Compter les d\u00e9pendances\n         req_file = self.project_path / \"requirements.txt\"\n         if req_file.exists():\n             try:\n-                with open(req_file, 'r') as f:\n-                    deps = [line.strip() for line in f if line.strip()\n-                            and not line.startswith('#')]\n+                with open(req_file, \"r\") as f:\n+                    deps = [\n+                        line.strip()\n+                        for line in f\n+                        if line.strip() and not line.startswith(\"#\")\n+                    ]\n                     performance_data[\"dependencies\"] = len(deps)\n             except Exception:\n                 pass\n \n         self.metrics[\"performance\"] = performance_data\n@@ -167,25 +168,25 @@\n         \"\"\"Analyse la qualit\u00e9 du projet\"\"\"\n         quality_data = {\n             \"total_lines\": 0,\n             \"docstrings\": 0,\n             \"comments\": 0,\n-            \"empty_lines\": 0\n+            \"empty_lines\": 0,\n         }\n \n         for py_file in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     lines = f.readlines()\n \n                 quality_data[\"total_lines\"] += len(lines)\n \n                 for line in lines:\n                     stripped = line.strip()\n                     if stripped.startswith('\"\"\"') or stripped.startswith(\"'''\"):\n                         quality_data[\"docstrings\"] += 1\n-                    elif stripped.startswith('#'):\n+                    elif stripped.startswith(\"#\"):\n                         quality_data[\"comments\"] += 1\n                     elif not stripped:\n                         quality_data[\"empty_lines\"] += 1\n \n             except Exception:\n@@ -193,44 +194,44 @@\n \n         self.metrics[\"quality\"] = quality_data\n \n     def _analyze_evolution(self):\n         \"\"\"Analyse l\u00e9volution du projet\"\"\"\n-        evolution_data = {\n-            \"last_modified\": None,\n-            \"total_files\": 0\n-        }\n+        evolution_data = {\"last_modified\": None, \"total_files\": 0}\n \n         # Compter les fichiers\n         for py_file in self.project_path.rglob(\"*.py\"):\n             evolution_data[\"total_files\"] += 1\n             try:\n                 mtime = py_file.stat().st_mtime\n-                if not evolution_data[\"last_modified\"] or mtime > evolution_data[\"last_modified\"]:\n+                if (\n+                    not evolution_data[\"last_modified\"]\n+                    or mtime > evolution_data[\"last_modified\"]\n+                ):\n                     evolution_data[\"last_modified\"] = mtime\n             except Exception:\n                 continue\n \n         self.metrics[\"evolution\"] = evolution_data\n \n     def _generate_dashboard(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un dashboard HTML\"\"\"\n         metrics = self.metrics\n-        complexity = metrics.get('complexity', {})\n-        coverage = metrics.get('coverage', {})\n-        quality = metrics.get('quality', {})\n-        performance = metrics.get('performance', {})\n-        average = complexity.get('average', 0.0)\n-        total_files = complexity.get('total_files', 0)\n-        total_lines = coverage.get('total_lines', 0)\n-        docstrings = coverage.get('docstrings', 0)\n-        files = coverage.get('files', 0)\n-        comments = quality.get('comments', 0)\n-        empty_lines = quality.get('empty_lines', 0)\n-        total_quality_lines = quality.get('total_lines', 1)\n-        perf_dependencies = performance.get('dependencies', 0)\n-        perf_file_sizes = performance.get('file_sizes', {})\n+        complexity = metrics.get(\"complexity\", {})\n+        coverage = metrics.get(\"coverage\", {})\n+        quality = metrics.get(\"quality\", {})\n+        performance = metrics.get(\"performance\", {})\n+        average = complexity.get(\"average\", 0.0)\n+        total_files = complexity.get(\"total_files\", 0)\n+        total_lines = coverage.get(\"total_lines\", 0)\n+        docstrings = coverage.get(\"docstrings\", 0)\n+        files = coverage.get(\"files\", 0)\n+        comments = quality.get(\"comments\", 0)\n+        empty_lines = quality.get(\"empty_lines\", 0)\n+        total_quality_lines = quality.get(\"total_lines\", 1)\n+        perf_dependencies = performance.get(\"dependencies\", 0)\n+        perf_file_sizes = performance.get(\"file_sizes\", {})\n \n         dashboard_html = f\"\"\"\n <!DOCTYPE html>\n <html>\n <head>\n@@ -277,31 +278,31 @@\n </body>\n </html>\n \"\"\"\n \n         dashboard_path = self.project_path / \"dashboard\" / \"analytics_dashboard.html\"\n-        with open(dashboard_path, 'w', encoding='utf-8') as f:\n+        with open(dashboard_path, \"w\", encoding=\"utf-8\") as f:\n             f.write(dashboard_html)\n \n         return str(dashboard_path)\n \n     def _generate_summary(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un r\u00e9sum\u00e9 des m\u00e9triques\"\"\"\n         metrics = self.metrics\n-        complexity = metrics.get('complexity', {})\n-        coverage = metrics.get('coverage', {})\n-        quality = metrics.get('quality', {})\n-        performance = metrics.get('performance', {})\n-        average = complexity.get('average', 0.0)\n-        total_files = complexity.get('total_files', 0)\n-        total_lines = coverage.get('total_lines', 0)\n-        docstrings = coverage.get('docstrings', 0)\n-        files = coverage.get('files', 0)\n-        comments = quality.get('comments', 0)\n-        total_quality_lines = quality.get('total_lines', 1)\n-        perf_dependencies = performance.get('dependencies', 0)\n-        perf_file_sizes = performance.get('file_sizes', {})\n+        complexity = metrics.get(\"complexity\", {})\n+        coverage = metrics.get(\"coverage\", {})\n+        quality = metrics.get(\"quality\", {})\n+        performance = metrics.get(\"performance\", {})\n+        average = complexity.get(\"average\", 0.0)\n+        total_files = complexity.get(\"total_files\", 0)\n+        total_lines = coverage.get(\"total_lines\", 0)\n+        docstrings = coverage.get(\"docstrings\", 0)\n+        files = coverage.get(\"files\", 0)\n+        comments = quality.get(\"comments\", 0)\n+        total_quality_lines = quality.get(\"total_lines\", 1)\n+        perf_dependencies = performance.get(\"dependencies\", 0)\n+        perf_file_sizes = performance.get(\"file_sizes\", {})\n \n         summary = f\"\"\"\n \ud83d\udcca ANALYTICS AVANC\u00c9E-{self.project_path.name}\n \n \ud83c\udfaf M\u00c9TRIQUES PRINCIPALES:\n@@ -331,14 +332,15 @@\n         logger.info(self._generate_summary())\n \n \n def enrich_genesis_md(outdir, infos, perf_log=None, test_log=None):\n     from pathlib import Path\n-    genesis = Path(outdir) / 'GENESIS.f(f'\n-    content = genesis.read_text() if genesis.exists() else ''\n-    if 'Audit IA' not in content:\n-        content += '\\nAudit IA\\n'\n+\n+    genesis = Path(outdir) / \"GENESIS.f(f\"\n+    content = genesis.read_text() if genesis.exists() else \"\"\n+    if \"Audit IA\" not in content:\n+        content += \"\\nAudit IA\\n\"\n     genesis.write_text(content)\n     return str(genesis)\n \n \n if __name__ == \"__main__\":\n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/dashboard_unified.py\t2025-07-29 17:56:21.180000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/dashboard_unified.py\t2025-07-29 18:12:20.769942+00:00\n@@ -32,112 +32,137 @@\n         \"\"\"Initialisation de la base de donn\u00e9es\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Table des m\u00e9triques\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS metriques (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     type TEXT NOT NULL,\n                     valeur REAL,\n                     projet TEXT,\n                     timestamp TEXT,\n                     details TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des \u00e9v\u00e9nements\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS evenements (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     type TEXT NOT NULL,\n                     projet TEXT,\n                     utilisateur TEXT,\n                     timestamp TEXT,\n                     duree INTEGER,\n                     statut TEXT,\n                     details TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des rapports\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS rapports (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     type TEXT NOT NULL,\n                     projet TEXT,\n                     contenu TEXT,\n                     timestamp TEXT,\n                     score_qualite INTEGER,\n                     score_securite INTEGER\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             conn.commit()\n \n     def enregistrer_metrique(\n-        self, type_metrique: str, valeur: float, projet: Optional[str] = None,\n-        details: Optional[Dict] = None\n+        self,\n+        type_metrique: str,\n+        valeur: float,\n+        projet: Optional[str] = None,\n+        details: Optional[Dict] = None,\n     ):\n         \"\"\"Enregistrement une m\u00e9trique\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 INSERT INTO metriques (type, valeur, projet, timestamp, details)\n                 VALUES (?, ?, ?, ?, ?)\n-            \"\"\", (\n-                type_metrique,\n-                valeur,\n-                projet,\n-                datetime.now().isoformat(),\n-                json.dumps(details) if details else None\n-            ))\n+            \"\"\",\n+                (\n+                    type_metrique,\n+                    valeur,\n+                    projet,\n+                    datetime.now().isoformat(),\n+                    json.dumps(details) if details else None,\n+                ),\n+            )\n             conn.commit()\n \n     def enregistrer_evenement(\n-            self,\n-            type_evenement: str,\n-            projet: Optional[str] = None,\n-            utilisateur: Optional[str] = None,\n-            duree: int = 0,\n-            statut: str = \"succes\",\n-            details: Optional[Dict] = None):\n+        self,\n+        type_evenement: str,\n+        projet: Optional[str] = None,\n+        utilisateur: Optional[str] = None,\n+        duree: int = 0,\n+        statut: str = \"succes\",\n+        details: Optional[Dict] = None,\n+    ):\n         \"\"\"Enregistrement un \u00e9v\u00e9nement\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 INSERT INTO evenements\n                 (type, projet, utilisateur, timestamp, duree, statut, details)\n                 VALUES (?, ?, ?, ?, ?, ?, ?)\n-            \"\"\", (\n-                type_evenement,\n-                projet,\n-                utilisateur,\n-                datetime.now().isoformat(),\n-                duree,\n-                statut,\n-                json.dumps(details) if details else None\n-            ))\n+            \"\"\",\n+                (\n+                    type_evenement,\n+                    projet,\n+                    utilisateur,\n+                    datetime.now().isoformat(),\n+                    duree,\n+                    statut,\n+                    json.dumps(details) if details else None,\n+                ),\n+            )\n             conn.commit()\n \n-    def enregistrer_rapport(self, type_rapport: str, projet: str, contenu: str,\n-                            score_qualite: int = 0, score_securite: int = 0):\n+    def enregistrer_rapport(\n+        self,\n+        type_rapport: str,\n+        projet: str,\n+        contenu: str,\n+        score_qualite: int = 0,\n+        score_securite: int = 0,\n+    ):\n         \"\"\"Enregistrement un rapport\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 INSERT INTO rapports\n                 (type, projet, contenu, timestamp, score_qualite, score_securite)\n                 VALUES (?, ?, ?, ?, ?, ?)\n-            \"\"\", (\n-                type_rapport,\n-                projet,\n-                contenu,\n-                datetime.now().isoformat(),\n-                score_qualite,\n-                score_securite\n-            ))\n+            \"\"\",\n+                (\n+                    type_rapport,\n+                    projet,\n+                    contenu,\n+                    datetime.now().isoformat(),\n+                    score_qualite,\n+                    score_securite,\n+                ),\n+            )\n             conn.commit()\n \n     def obtenir_metriques_temps_reel(self) -> Dict[str, Any]:\n         \"\"\"Obtention des m\u00e9triques en temps r\u00e9el\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n@@ -145,112 +170,135 @@\n \n             # M\u00e9triques des derni\u00e8res 24h\n             hier = (datetime.now() - timedelta(days=1)).isoformat()\n \n             # Projets analys\u00e9s\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT COUNT(DISTINCT projet) as total_projets\n                 FROM evenements\n                 WHERE timestamp > ? AND type = 'audit_projet'\n-            \"\"\", (hier,))\n+            \"\"\",\n+                (hier,),\n+            )\n             projets_analyses = cursor.fetchone()[0]\n \n             # Actions effectu\u00e9es\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT COUNT(*) as total_actions\n                 FROM evenements\n                 WHERE timestamp > ?\n-            \"\"\", (hier,))\n+            \"\"\",\n+                (hier,),\n+            )\n             actions_effectuees = cursor.fetchone()[0]\n \n             # Score qualit\u00e9 moyen\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT AVG(score_qualite) as score_moyen\n                 FROM rapports\n                 WHERE timestamp > ? AND score_qualite > 0\n-            \"\"\", (hier,))\n+            \"\"\",\n+                (hier,),\n+            )\n             score_qualite_moyen = cursor.fetchone()[0] or 0\n \n             # Score s\u00e9curit\u00e9 moyen\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT AVG(score_securite) as score_moyen\n                 FROM rapports\n                 WHERE timestamp > ? AND score_securite > 0\n-            \"\"\", (hier,))\n+            \"\"\",\n+                (hier,),\n+            )\n             score_securite_moyen = cursor.fetchone()[0] or 0\n \n             return {\n                 \"projets_analyses\": projets_analyses,\n                 \"actions_effectuees\": actions_effectuees,\n                 \"score_qualite_moyen\": round(score_qualite_moyen, 1),\n                 \"score_securite_moyen\": round(score_securite_moyen, 1),\n-                \"derniere_mise_a_jour\": datetime.now().strftime(\"%H:%M:%S\")\n+                \"derniere_mise_a_jour\": datetime.now().strftime(\"%H:%M:%S\"),\n             }\n \n     def generer_rapport_consolide(self) -> str:\n         \"\"\"G\u00e9n\u00e9ration d'un rapport consolid\u00e9\"\"\"\n         metriques = self.obtenir_metriques_temps_reel()\n \n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Top projets par score\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT projet, AVG(score_qualite) as score_qualite,\n                        AVG(score_securite) as score_securite\n                 FROM rapports\n                 WHERE score_qualite > 0 OR score_securite > 0\n                 GROUP BY projet\n                 ORDER BY (score_qualite + score_securite) / 2 DESC\n                 LIMIT 10\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             top_projets = []\n             for row in cursor.fetchall():\n-                top_projets.append({\n-                    \"projet\": row[0],\n-                    \"score_qualite\": round(row[1] or 0, 1),\n-                    \"score_securite\": round(row[2] or 0, 1),\n-                    \"score_moyen\": round(((row[1] or 0) + (row[2] or 0)) / 2, 1)\n-                })\n+                top_projets.append(\n+                    {\n+                        \"projet\": row[0],\n+                        \"score_qualite\": round(row[1] or 0, 1),\n+                        \"score_securite\": round(row[2] or 0, 1),\n+                        \"score_moyen\": round(((row[1] or 0) + (row[2] or 0)) / 2, 1),\n+                    }\n+                )\n \n             # \u00c9v\u00e9nements r\u00e9cents\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT type, projet, utilisateur, timestamp, statut\n                 FROM evenements\n                 ORDER BY timestamp DESC\n                 LIMIT 20\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             evenements_recents = []\n             for row in cursor.fetchall():\n-                evenements_recents.append({\n-                    \"type\": row[0],\n-                    \"projet\": row[1],\n-                    \"utilisateur\": row[2],\n-                    \"timestamp\": datetime.fromisoformat(row[3]).strftime(\"%d/%m/%Y %H:%f\"),\n-                    \"statut\": row[4]\n-                })\n+                evenements_recents.append(\n+                    {\n+                        \"type\": row[0],\n+                        \"projet\": row[1],\n+                        \"utilisateur\": row[2],\n+                        \"timestamp\": (\n+                            datetime.fromisoformat(row[3]).strftime(\"%d/%m/%Y %H:%f\")\n+                        ),\n+                        \"statut\": row[4],\n+                    }\n+                )\n \n         rapport = []\n         rapport.append(\"# \ud83d\udcca Dashboard Unifi\u00e9\")\n         rapport.append(\"\")\n-        rapport.append(\n-            f\"*G\u00e9n\u00e9r\u00e9 le {datetime.now().strftime('%d/%m/%Y \u00e0 %H:%M')}*\")\n+        rapport.append(f\"*G\u00e9n\u00e9r\u00e9 le {datetime.now().strftime('%d/%m/%Y \u00e0 %H:%M')}*\")\n         rapport.append(\"\")\n \n         # M\u00e9triques temps r\u00e9el\n         rapport.append(\"## \ud83d\ude80 M\u00e9triques en Temps R\u00e9el\")\n         rapport.append(\"\")\n+        rapport.append(f\"- **Projets analys\u00e9s (24h)**: {metriques['projets_analyses']}\")\n         rapport.append(\n-            f\"- **Projets analys\u00e9s (24h)**: {metriques['projets_analyses']}\")\n+            f\"- **Actions effectu\u00e9es (24h)**: {metriques['actions_effectuees']}\"\n+        )\n         rapport.append(\n-            f\"- **Actions effectu\u00e9es (24h)**: {metriques['actions_effectuees']}\")\n+            f\"- **Score qualit\u00e9 moyen**: {metriques['score_qualite_moyen']}/100\"\n+        )\n         rapport.append(\n-            f\"- **Score qualit\u00e9 moyen**: {metriques['score_qualite_moyen']}/100\")\n-        rapport.append(\n-            f\"- **Score s\u00e9curit\u00e9 moyen**: {metriques['score_securite_moyen']}/100\")\n+            f\"- **Score s\u00e9curit\u00e9 moyen**: {metriques['score_securite_moyen']}/100\"\n+        )\n         rapport.append(\"\")\n \n         # Top projets\n         if top_projets:\n             rapport.append(\"## \ufffd\ufffd Top 10 Projets par Score\")\n@@ -258,11 +306,12 @@\n             rapport.append(\"| Projet | Qualit\u00e9 | S\u00e9curit\u00e9 | Moyenne |\")\n             rapport.append(\"|--------|---------|----------|---------|\")\n             for projet in top_projets:\n                 rapport.append(\n                     f\"| {projet['projet']} | {projet['score_qualite']} | \"\n-                    f\"{projet['score_securite']} | {projet['score_moyen']} |\")\n+                    f\"{projet['score_securite']} | {projet['score_moyen']} |\"\n+                )\n             rapport.append(\"\")\n \n         # \u00c9v\u00e9nements r\u00e9cents\n         if evenements_recents:\n             rapport.append(\"## \ud83d\udcc5 \u00c9v\u00e9nements R\u00e9cents\")\n@@ -272,28 +321,29 @@\n             for event in evenements_recents[:10]:\n                 statut_emoji = \"\u2705\" if event[\"statut\"] == \"succes\" else \"\u274c\"\n                 rapport.append(\n                     f\"| {event['type']} | {event['projet'] or '-'} | \"\n                     f\"{event['utilisateur'] or '-'} | {event['timestamp']} | \"\n-                    f\"{statut_emoji} |\")\n+                    f\"{statut_emoji} |\"\n+                )\n             rapport.append(\"\")\n \n         return \"\\n\".join(rapport)\n \n     def ajouter_section_distillation(self, file_handle):\n         \"\"\"\n         Ajoute une section Distillation IA au dashboard (exemple statique).\n         \"\"\"\n-        file_handle.write('<h2>R\u00e9sultat de la distillation IA</h2>')\n+        file_handle.write(\"<h2>R\u00e9sultat de la distillation IA</h2>\")\n         file_handle.write(\n-            '<p><b>R\u00e9ponse distill\u00e9e :</b> R\u00e9ponse de Ollama \u00e0 '\n-            '\"Explique la distillation IA en 2 phrases.\"</p>')\n-        file_handle.write('<p><b>Score audit distill\u00e9 :</b> 7.60</p>')\n-        file_handle.write('<p><b>Correction distill\u00e9e :</b> fix2</p>')\n-\n-    def generer_dashboard_html(\n-            self, output_file: str = \"dashboard/index.html\"):\n+            \"<p><b>R\u00e9ponse distill\u00e9e :</b> R\u00e9ponse de Ollama \u00e0 \"\n+            '\"Explique la distillation IA en 2 phrases.\"</p>'\n+        )\n+        file_handle.write(\"<p><b>Score audit distill\u00e9 :</b> 7.60</p>\")\n+        file_handle.write(\"<p><b>Correction distill\u00e9e :</b> fix2</p>\")\n+\n+    def generer_dashboard_html(self, output_file: str = \"dashboard/index.html\"):\n         \"\"\"G\u00e9n\u00e9ration d'un dashboard HTML moderne et valide\"\"\"\n         metriques = self.obtenir_metriques_temps_reel()\n \n         html_content = f\"\"\"\n <!DOCTYPE html>\n@@ -429,11 +479,11 @@\n </html>\n \"\"\"\n         # Cr\u00e9ation du dossier et sauvegarde\n         output_path = Path(output_file)\n         output_path.parent.mkdir(parents=True, exist_ok=True)\n-        with open(output_path, 'w', encoding='utf-8') as file_handle:\n+        with open(output_path, \"w\", encoding=\"utf-8\") as file_handle:\n             file_handle.write(html_content)\n             self.ajouter_section_distillation(file_handle)\n         return str(output_path)\n \n     def ouvrir_dashboard(self):\n@@ -445,12 +495,11 @@\n \n def main():\n     \"\"\"Fonction principale pour test du f\"\"\"\n \n     if len(sys.argv) < 2:\n-        logger.info(\n-            \"Usage: python dashboard_unifie_simple.py <action> [options]\")\n+        logger.info(\"Usage: python dashboard_unifie_simple.py <action> [options]\")\n         logger.info(\"Actions: metrics, report, html, f\")\n         sys.exit(1)\n \n     action = sys.argv[1]\n     dashboard = DashboardUnifieSimple()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/audit.py\t2025-07-29 18:02:53.600000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/audit.py\t2025-07-29 18:12:20.782258+00:00\n@@ -20,38 +20,39 @@\n \n     def audit_project(self):\n         \"\"\"M\u00e9thode de compatibilit\u00e9\"\"\"\n         return self.auditor.audit_project(self.project_path)\n \n+\n # Fonctions de compatibilit\u00e9\n \n \n def audit_project_intelligent(project_path: str):\n     \"\"\"Fonction de compatibilit\u00e9 pour audit_project_intelligent\"\"\"\n     auditor = IntelligentAuditor()\n     result = auditor.audit_project(project_path)\n \n     # Ajouter les cl\u00e9s de compatibilit\u00e9\n     if isinstance(result, dict):\n-        result['global_score'] = result.get('score', 0)\n-        result['summary'] = 'R\u00e9sum\u00e9 g\u00e9n\u00e9r\u00e9 par intelligent_auditor'\n+        result[\"global_score\"] = result.get(\"score\", 0)\n+        result[\"summary\"] = \"R\u00e9sum\u00e9 g\u00e9n\u00e9r\u00e9 par intelligent_auditor\"\n \n         # Ajouter les cl\u00e9s manquantes pour compatibilit\u00e9\n-        if 'metrics' not in result:\n-            result['metrics'] = {\n-                'total_lines': 0,\n-                'total_functions': 0,\n-                'total_classes': 0,\n-                'structure_score': 100,\n-                'code_score': 100\n+        if \"metrics\" not in result:\n+            result[\"metrics\"] = {\n+                \"total_lines\": 0,\n+                \"total_functions\": 0,\n+                \"total_classes\": 0,\n+                \"structure_score\": 100,\n+                \"code_score\": 100,\n             }\n \n         # Ajouter les cl\u00e9s issues et suggestions si manquantes\n-        if 'issues' not in result:\n-            result['issues'] = []\n-        if 'suggestions' not in result:\n-            result['suggestions'] = []\n+        if \"issues\" not in result:\n+            result[\"issues\"] = []\n+        if \"suggestions\" not in result:\n+            result[\"suggestions\"] = []\n \n     return result\n \n \n def generate_audit_report(project_path: str):\n@@ -59,14 +60,15 @@\n     auditor = IntelligentAuditor()\n     auditor.audit_project(project_path)\n     report = auditor.generate_report()\n \n     # Ajouter le titre de compatibilit\u00e9\n-    if 'AUDIT PROJET' not in report:\n+    if \"AUDIT PROJET\" not in report:\n         report = f\"AUDIT PROJET - {project_path}\\n\\n\" + report\n \n     return report\n+\n \n # Alias pour compatibilit\u00e9 avec les tests\n \n \n class Audit:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/user_profiles_advanced.py\t2025-07-29 17:56:21.380000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/user_profiles_advanced.py\t2025-07-29 18:12:20.798773+00:00\n@@ -32,21 +32,20 @@\n             \"email\": self.email,\n             \"preferences\": self.preferences,\n             \"date_creation\": self.date_creation.isoformat(),\n             \"derniere_connexion\": self.derniere_connexion.isoformat(),\n             \"projets_consultes\": self.projets_consultes,\n-            \"actions_frequentes\": self.actions_frequentes\n+            \"actions_frequentes\": self.actions_frequentes,\n         }\n \n     @classmethod\n-    def from_dict(cls, data: Dict[str, Any]) -> 'ProfilUtilisateur':\n+    def from_dict(cls, data: Dict[str, Any]) -> \"ProfilUtilisateur\":\n         \"\"\"Cr\u00e9ation depuis un dictionnaire\"\"\"\n         profil = cls(data[\"nom\"], data.get(\"email\", \"\"))\n         profil.preferences = data.get(\"preferences\", {})\n         profil.date_creation = datetime.fromisoformat(data[\"date_creation\"])\n-        profil.derniere_connexion = datetime.fromisoformat(\n-            data[\"derniere_connexion\"])\n+        profil.derniere_connexion = datetime.fromisoformat(data[\"derniere_connexion\"])\n         profil.projets_consultes = data.get(\"projets_consultes\", [])\n         profil.actions_frequentes = data.get(\"actions_frequentes\", {})\n         return profil\n \n \n@@ -61,109 +60,124 @@\n         \"\"\"Initialisation de la base de donn\u00e9es\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Table des profils\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS profils (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     nom TEXT UNIQUE NOT NULL,\n                     email TEXT,\n                     preferences TEXT,\n                     date_creation TEXT,\n                     derniere_connexion TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des actions\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS actions (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     profil_id INTEGER,\n                     action TEXT,\n                     timestamp TEXT,\n                     details TEXT,\n                     FOREIGN KEY (profil_id) REFERENCES profils (id)\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des projets consult\u00e9s\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS projets_consultes (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     profil_id INTEGER,\n                     chemin_projet TEXT,\n                     date_consultation TEXT,\n                     duree_consultation INTEGER,\n                     FOREIGN KEY (profil_id) REFERENCES profils (id)\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             conn.commit()\n \n     def creer_profil(\n-            self,\n-            nom: str,\n-            email: str = \"\",\n-            preferences: Dict = None) -> ProfilUtilisateur:\n+        self, nom: str, email: str = \"\", preferences: Dict = None\n+    ) -> ProfilUtilisateur:\n         \"\"\"Cr\u00e9ation d'un nouveau profil\"\"\"\n         logger.info(f\"Cr\u00e9ation du profil utilisateur: {nom}\")\n \n         profil = ProfilUtilisateur(nom, email, preferences)\n \n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 INSERT INTO profils (nom, email, preferences, date_creation, derniere_connexion)\n                 VALUES (?, ?, ?, ?, ?)\n-            \"\"\", (\n-                profil.nom,\n-                profil.email,\n-                json.dumps(profil.preferences),\n-                profil.date_creation.isoformat(),\n-                profil.derniere_connexion.isoformat()\n-            ))\n+            \"\"\",\n+                (\n+                    profil.nom,\n+                    profil.email,\n+                    json.dumps(profil.preferences),\n+                    profil.date_creation.isoformat(),\n+                    profil.derniere_connexion.isoformat(),\n+                ),\n+            )\n             conn.commit()\n \n         return profil\n \n     def obtenir_profil(self, nom: str) -> Optional[ProfilUtilisateur]:\n         \"\"\"R\u00e9cup\u00e9ration d'un profil par nom\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT nom, email, preferences, date_creation, derniere_connexion\n                 FROM profils WHERE nom = ?\n-            \"\"\", (nom,))\n+            \"\"\",\n+                (nom,),\n+            )\n \n             row = cursor.fetchone()\n             if row:\n                 profil = ProfilUtilisateur(row[0], row[1])\n                 profil.preferences = json.loads(row[2]) if row[2] else {}\n                 profil.date_creation = datetime.fromisoformat(row[3])\n                 profil.derniere_connexion = datetime.fromisoformat(row[4])\n \n                 # R\u00e9cup\u00e9ration des actions fr\u00e9quentes\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     SELECT action, COUNT(*) as count\n                     FROM actions WHERE profil_id = (SELECT id FROM profils WHERE nom = ?)\n                     GROUP BY action ORDER BY count DESC LIMIT 10\n-                \"\"\", (nom,))\n+                \"\"\",\n+                    (nom,),\n+                )\n \n                 for action, count in cursor.fetchall():\n                     profil.actions_frequentes[action] = count\n \n                 # R\u00e9cup\u00e9ration des projets consult\u00e9s\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     SELECT chemin_projet, date_consultation\n                     FROM projets_consultes\n                     WHERE profil_id = (SELECT id FROM profils WHERE nom = ?)\n                     ORDER BY date_consultation DESC LIMIT 20\n-                \"\"\", (nom,))\n-\n-                profil.projets_consultes = [row[0]\n-                                            for row in cursor.fetchall()]\n+                \"\"\",\n+                    (nom,),\n+                )\n+\n+                profil.projets_consultes = [row[0] for row in cursor.fetchall()]\n \n                 return profil\n \n         return None\n \n@@ -171,110 +185,115 @@\n         \"\"\"Mise \u00e0 jour d'un profil\"\"\"\n         profil.derniere_connexion = datetime.now()\n \n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 UPDATE profils\n                 SET email = ?, preferences = ?, derniere_connexion = ?\n                 WHERE nom = ?\n-            \"\"\", (\n-                profil.email,\n-                json.dumps(profil.preferences),\n-                profil.derniere_connexion.isoformat(),\n-                profil.nom\n-            ))\n+            \"\"\",\n+                (\n+                    profil.email,\n+                    json.dumps(profil.preferences),\n+                    profil.derniere_connexion.isoformat(),\n+                    profil.nom,\n+                ),\n+            )\n             conn.commit()\n \n-    def enregistrer_action(\n-            self,\n-            nom_profil: str,\n-            action: str,\n-            details: Dict = None):\n+    def enregistrer_action(self, nom_profil: str, action: str, details: Dict = None):\n         \"\"\"Enregistrement d'une action utilisateur\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\n-                \"SELECT id FROM profils WHERE nom = ?\", (nom_profil,))\n+            cursor.execute(\"SELECT id FROM profils WHERE nom = ?\", (nom_profil,))\n             profil_id = cursor.fetchone()\n \n             if profil_id:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO actions (profil_id, action, timestamp, details)\n                     VALUES (?, ?, ?, ?)\n-                \"\"\", (\n-                    profil_id[0],\n-                    action,\n-                    datetime.now().isoformat(),\n-                    json.dumps(details) if details else None\n-                ))\n+                \"\"\",\n+                    (\n+                        profil_id[0],\n+                        action,\n+                        datetime.now().isoformat(),\n+                        json.dumps(details) if details else None,\n+                    ),\n+                )\n                 conn.commit()\n \n     def enregistrer_consultation_projet(\n-            self,\n-            nom_profil: str,\n-            chemin_projet: str,\n-            duree: int = 0):\n+        self, nom_profil: str, chemin_projet: str, duree: int = 0\n+    ):\n         \"\"\"Enregistrement de la consultation d'un projet\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\n-                \"SELECT id FROM profils WHERE nom = ?\", (nom_profil,))\n+            cursor.execute(\"SELECT id FROM profils WHERE nom = ?\", (nom_profil,))\n             profil_id = cursor.fetchone()\n \n             if profil_id:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO projets_consultes \n                     (profil_id, chemin_projet, date_consultation, duree_consultation)\n                     VALUES (?, ?, ?, ?)\n-                \"\"\", (\n-                    profil_id[0],\n-                    chemin_projet,\n-                    datetime.now().isoformat(),\n-                    duree\n-                ))\n+                \"\"\",\n+                    (profil_id[0], chemin_projet, datetime.now().isoformat(), duree),\n+                )\n                 conn.commit()\n \n     def obtenir_statistiques(self, nom_profil: str) -> Dict[str, Any]:\n         \"\"\"Obtention des statistiques d'un profil\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Actions les plus fr\u00e9quentes\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT action, COUNT(*) as count\n                 FROM actions a\n                 JOIN profils p ON a.profil_id = p.id\n                 WHERE p.nom = ?\n                 GROUP BY action\n                 ORDER BY count DESC\n                 LIMIT 10\n-            \"\"\", (nom_profil,))\n+            \"\"\",\n+                (nom_profil,),\n+            )\n \n             actions_frequentes = {row[0]: row[1] for row in cursor.fetchall()}\n \n             # Projets les plus consult\u00e9s\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT chemin_projet, COUNT(*) as count\n                 FROM projets_consultes pc\n                 JOIN profils p ON pc.profil_id = p.id\n                 WHERE p.nom = ?\n                 GROUP BY chemin_projet\n                 ORDER BY count DESC\n                 LIMIT 10\n-            \"\"\", (nom_profil,))\n+            \"\"\",\n+                (nom_profil,),\n+            )\n \n             projets_frequents = {row[0]: row[1] for row in cursor.fetchall()}\n             projets_consultes = list(projets_frequents.keys())\n \n             # Temps total pass\u00e9\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT SUM(duree_consultation) as total_time\n                 FROM projets_consultes pc\n                 JOIN profils p ON pc.profil_id = p.id\n                 WHERE p.nom = ?\n-            \"\"\", (nom_profil,))\n+            \"\"\",\n+                (nom_profil,),\n+            )\n \n             temps_total = cursor.fetchone()[0] or 0\n \n             return {\n                 \"actions_frequentes\": actions_frequentes,\n@@ -282,11 +301,11 @@\n                 \"projets_consultes\": projets_consultes,\n                 \"temps_total\": temps_total,\n                 \"duree_totale\": temps_total,\n                 \"total_actions\": sum(actions_frequentes.values()),\n                 \"total_projets\": len(projets_frequents),\n-                \"connexion\": actions_frequentes.get('connexion', 0)\n+                \"connexion\": actions_frequentes.get(\"connexion\", 0),\n             }\n \n     def generer_rapport_profil(self, nom_profil: str) -> str:\n         \"\"\"G\u00e9n\u00e9ration d'un rapport d\u00e9taill\u00e9 pour un profil\"\"\"\n         profil = self.obtenir_profil(nom_profil)\n@@ -310,18 +329,18 @@\n \u2022 Temps total: {stats['temps_total']} minutes\n \n \ud83d\udd1d ACTIONS LES PLUS FR\u00c9QUENTES:\n \"\"\"\n \n-        for action, count in list(stats['actions_frequentes'].items())[:5]:\n+        for action, count in list(stats[\"actions_frequentes\"].items())[:5]:\n             rapport += f\"\u2022 {action}: {count} fois\\n\"\n \n         rapport += \"\"\"\n \ud83d\udcc1 PROJETS LES PLUS CONSULT\u00c9S:\n \"\"\"\n \n-        for projet, count in list(stats['projets_frequents'].items())[:5]:\n+        for projet, count in list(stats[\"projets_frequents\"].items())[:5]:\n             rapport += f\"\u2022 {projet}: {count} consultations\\n\"\n \n         rapport += \"\"\"\n \u2699\ufe0f PR\u00c9F\u00c9RENCES:\n \"\"\"\n@@ -333,12 +352,11 @@\n \n     def lister_profils(self) -> List[str]:\n         \"\"\"Liste de tous les profils\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\n-                \"SELECT nom FROM profils ORDER BY derniere_connexion DESC\")\n+            cursor.execute(\"SELECT nom FROM profils ORDER BY derniere_connexion DESC\")\n             return [row[0] for row in cursor.fetchall()]\n \n     def supprimer_profil(self, nom: str) -> bool:\n         \"\"\"Suppression d'un profil\"\"\"\n         try:\n@@ -356,22 +374,22 @@\n         try:\n             profil = self.obtenir_profil(nom)\n             if not profil:\n                 return False\n \n-            with open(fichier_destination, 'w', encoding='utf-8') as f:\n+            with open(fichier_destination, \"w\", encoding=\"utf-8\") as f:\n                 json.dump(profil.to_dict(), f, indent=2, ensure_ascii=False)\n \n             return True\n         except Exception as e:\n             logger.error(f\"Erreur lors de l'export du profil {nom}: {e}\")\n             return False\n \n     def importer_profil(self, fichier_source: str) -> bool:\n         \"\"\"Import d'un profil depuis un fichier JSON\"\"\"\n         try:\n-            with open(fichier_source, 'r', encoding='utf-8') as f:\n+            with open(fichier_source, \"r\", encoding=\"utf-8\") as f:\n                 data = json.load(f)\n \n             profil = ProfilUtilisateur.from_dict(data)\n             self.creer_profil(profil.nom, profil.email, profil.preferences)\n             return True\n@@ -382,20 +400,16 @@\n \n def main():\n     \"\"\"Fonction principale pour test\"\"\"\n     import argparse\n \n-    parser = argparse.ArgumentParser(\n-        description=\"Gestionnaire de profils utilisateur\")\n+    parser = argparse.ArgumentParser(description=\"Gestionnaire de profils utilisateur\")\n     parser.add_argument(\n         \"action\",\n-        choices=[\n-            \"creer\",\n-            \"obtenir\",\n-            \"lister\",\n-            \"rapport\"],\n-        help=\"Action \u00e0 effectuer\")\n+        choices=[\"creer\", \"obtenir\", \"lister\", \"rapport\"],\n+        help=\"Action \u00e0 effectuer\",\n+    )\n     parser.add_argument(\"nom\", nargs=\"?\", help=\"Nom du profil\")\n     parser.add_argument(\"--email\", help=\"Email du profil\")\n \n     args = parser.parse_args()\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/ast_analyzer.py\t2025-07-29 17:56:23.200000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ast_analyzer.py\t2025-07-29 18:12:20.807816+00:00\n@@ -18,10 +18,11 @@\n \n \n @dataclass\n class ASTNodeInfo:\n     \"\"\"Informations extraites d'un n\u0153ud AST\"\"\"\n+\n     node_type: str\n     name: str\n     line_number: int\n     complexity: int\n     signature: str\n@@ -29,10 +30,11 @@\n \n \n @dataclass\n class FileAnalysis:\n     \"\"\"Analyse compl\u00e8te d'un fichier Python\"\"\"\n+\n     file_path: Path\n     functions: List[ASTNodeInfo]\n     classes: List[ASTNodeInfo]\n     conditionals: List[ASTNodeInfo]\n     loops: List[ASTNodeInfo]\n@@ -50,14 +52,14 @@\n \n     def analyze_file(self, file_path: Path) -> Optional[FileAnalysis]:\n         \"\"\"Analyser un fichier Python et extraire toutes les informations\"\"\"\n         try:\n             # Ignorer les fichiers cach\u00e9s macOS\n-            if file_path.name.startswith('._'):\n+            if file_path.name.startswith(\"._\"):\n                 return None\n \n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n \n             tree = ast.parse(content)\n \n             functions = self._extract_functions(tree, content, file_path)\n@@ -76,22 +78,20 @@\n                 conditionals=conditionals,\n                 loops=loops,\n                 imports=imports,\n                 total_lines=len(content.splitlines()),\n                 complexity_score=complexity_score,\n-                last_modified=last_modified\n+                last_modified=last_modified,\n             )\n \n         except Exception as e:\n             logger.error(f\"Erreur lors de l'analyse AST de {file_path}: {e}\")\n             return None\n \n     def _extract_functions(\n-            self,\n-            tree: ast.AST,\n-            content: str,\n-            file_path: Path) -> List[ASTNodeInfo]:\n+        self, tree: ast.AST, content: str, file_path: Path\n+    ) -> List[ASTNodeInfo]:\n         \"\"\"Extraire toutes les fonctions du fichier\"\"\"\n         functions = []\n \n         for node in ast.walk(tree):\n             if isinstance(node, ast.FunctionDef):\n@@ -99,21 +99,19 @@\n                     node_type=\"function\",\n                     name=node.name,\n                     line_number=node.lineno,\n                     complexity=self._calculate_node_complexity(node),\n                     signature=self._create_function_signature(node, content),\n-                    content=self._extract_node_content(node, content)\n+                    content=self._extract_node_content(node, content),\n                 )\n                 functions.append(func_info)\n \n         return functions\n \n     def _extract_classes(\n-            self,\n-            tree: ast.AST,\n-            content: str,\n-            file_path: Path) -> List[ASTNodeInfo]:\n+        self, tree: ast.AST, content: str, file_path: Path\n+    ) -> List[ASTNodeInfo]:\n         \"\"\"Extraire toutes les classes du fichier\"\"\"\n         classes = []\n \n         for node in ast.walk(tree):\n             if isinstance(node, ast.ClassDef):\n@@ -121,43 +119,39 @@\n                     node_type=\"class\",\n                     name=node.name,\n                     line_number=node.lineno,\n                     complexity=self._calculate_node_complexity(node),\n                     signature=self._create_class_signature(node, content),\n-                    content=self._extract_node_content(node, content)\n+                    content=self._extract_node_content(node, content),\n                 )\n                 classes.append(class_info)\n \n         return classes\n \n     def _extract_conditionals(\n-            self,\n-            tree: ast.AST,\n-            content: str,\n-            file_path: Path) -> List[ASTNodeInfo]:\n+        self, tree: ast.AST, content: str, file_path: Path\n+    ) -> List[ASTNodeInfo]:\n         \"\"\"Extraire toutes les structures conditionnelles\"\"\"\n         conditionals = []\n \n         for node in ast.walk(tree):\n             if isinstance(node, ast.If):\n                 cond_info = ASTNodeInfo(\n                     node_type=\"conditional\",\n                     name=f\"if_{node.lineno}\",\n                     line_number=node.lineno,\n                     complexity=self._calculate_node_complexity(node),\n-                    signature=self._create_conditional_signature(\n-                        node,\n-                        content),\n-                    content=self._extract_node_content(\n-                        node,\n-                        content))\n+                    signature=self._create_conditional_signature(node, content),\n+                    content=self._extract_node_content(node, content),\n+                )\n                 conditionals.append(cond_info)\n \n         return conditionals\n \n-    def _extract_loops(self, tree: ast.AST, content: str,\n-                       file_path: Path) -> List[ASTNodeInfo]:\n+    def _extract_loops(\n+        self, tree: ast.AST, content: str, file_path: Path\n+    ) -> List[ASTNodeInfo]:\n         \"\"\"Extraire toutes les boucles\"\"\"\n         loops = []\n \n         for node in ast.walk(tree):\n             if isinstance(node, (ast.For, ast.While)):\n@@ -165,11 +159,11 @@\n                     node_type=\"loop\",\n                     name=f\"{type(node).__name__.lower()}_{node.lineno}\",\n                     line_number=node.lineno,\n                     complexity=self._calculate_node_complexity(node),\n                     signature=self._create_loop_signature(node, content),\n-                    content=self._extract_node_content(node, content)\n+                    content=self._extract_node_content(node, content),\n                 )\n                 loops.append(loop_info)\n \n         return loops\n \n@@ -186,14 +180,11 @@\n                 for alias in node.names:\n                     imports.append(f\"{module}.{alias.name}\")\n \n         return imports\n \n-    def _create_function_signature(\n-            self,\n-            node: ast.FunctionDef,\n-            code: str) -> str:\n+    def _create_function_signature(self, node: ast.FunctionDef, code: str) -> str:\n         \"\"\"Cr\u00e9er une signature unique pour une fonction\"\"\"\n         # Extraire les param\u00e8tres\n         args = []\n         for arg in node.args.args:\n             args.append(arg.arg)\n@@ -231,45 +222,40 @@\n         return f\"loop:{type(node).__name__}:{hash(normalized)}\"\n \n     def _extract_node_content(self, node: ast.AST, code: str) -> str:\n         \"\"\"Extraire le contenu d'un n\u0153ud AST\"\"\"\n         lines = code.splitlines()\n-        start_line = getattr(node, 'lineno', 1) - 1\n-        end_line = getattr(node, 'end_lineno', start_line + 1)\n+        start_line = getattr(node, \"lineno\", 1) - 1\n+        end_line = getattr(node, \"end_lineno\", start_line + 1)\n \n         if end_line > len(lines):\n             end_line = len(lines)\n \n-        return '\\n'.join(lines[start_line:end_line])\n+        return \"\\n\".join(lines[start_line:end_line])\n \n     def _normalize_code(self, code: str) -> str:\n         \"\"\"Normaliser le code pour la comparaison\"\"\"\n         # Supprimer les commentaires\n-        code = re.sub(r'#.*$', '', code, flags=re.MULTILINE)\n+        code = re.sub(r\"#.*$\", \"\", code, flags=re.MULTILINE)\n \n         # Supprimer les espaces en d\u00e9but/fin\n         code = code.strip()\n \n         # Normaliser les espaces\n-        code = re.sub(r'\\s+', ' ', code)\n+        code = re.sub(r\"\\s+\", \" \", code)\n \n         # Supprimer les lignes vides\n-        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n+        code = re.sub(r\"\\n\\s*\\n\", \"\\n\", code)\n \n         return code\n \n     def _calculate_node_complexity(self, node: ast.AST) -> int:\n         \"\"\"Calculer la complexit\u00e9 cyclomatique d'un n\u0153ud\"\"\"\n         complexity = 1\n \n         for child in ast.walk(node):\n-            if isinstance(\n-                child,\n-                (ast.If,\n-                 ast.While,\n-                 ast.For,\n-                 ast.ExceptHandler)):\n+            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):\n                 complexity += 1\n             elif isinstance(child, ast.BoolOp):\n                 complexity += len(child.values) - 1\n \n         return complexity\n@@ -279,15 +265,11 @@\n         total_complexity = 0\n         node_count = 0\n \n         for node in ast.walk(tree):\n             if isinstance(\n-                node,\n-                (ast.FunctionDef,\n-                 ast.ClassDef,\n-                 ast.If,\n-                 ast.While,\n-                 ast.For)):\n+                node, (ast.FunctionDef, ast.ClassDef, ast.If, ast.While, ast.For)\n+            ):\n                 total_complexity += self._calculate_node_complexity(node)\n                 node_count += 1\n \n         return total_complexity / max(node_count, 1)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_server.py\t2025-07-29 17:56:24.470000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_server.py\t2025-07-29 18:12:20.841628+00:00\n@@ -1,10 +1,13 @@\n from fastapi import FastAPI, HTTPException\n from pydantic import BaseModel\n from typing import List\n import os\n-from athalia_core.autocomplete_engine import SimpleAutocompleteEngine, OllamaAutocompleteEngine\n+from athalia_core.autocomplete_engine import (\n+    SimpleAutocompleteEngine,\n+    OllamaAutocompleteEngine,\n+)\n \n app = FastAPI(title=\"Athalia Autocomplete Server\")\n \n \n class AutocompleteRequest(BaseModel):\n@@ -24,12 +27,12 @@\n \n \n @app.post(\"/autocomplete\", response_model=AutocompleteResponse)\n def autocomplete(request: AutocompleteRequest):\n     if not request.prompt:\n-        raise HTTPException(status_code=400,\n-                            detail=\"Le prompt ne peut pas \u00eatre vide.\")\n+        raise HTTPException(status_code=400, detail=\"Le prompt ne peut pas \u00eatre vide.\")\n     engine = get_engine()\n     suggestions = engine.suggest(request.prompt, request.max_suggestions)\n     return AutocompleteResponse(suggestions=suggestions)\n \n+\n # Pour lancer : uvicorn athalia_core.autocomplete_server:app --reload\n--- /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_engine.py\t2025-07-29 17:56:24.360000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_engine.py\t2025-07-29 18:12:20.846662+00:00\n@@ -16,34 +16,34 @@\n         base = prompt.strip() or \"suggestion\"\n         return [f\"{base}_auto_{i+1}\" for i in range(max_suggestions)]\n \n \n class OllamaAutocompleteEngine(BaseAutocompleteEngine):\n-    def __init__(self, model_name: str = \"mistral:latest\",\n-                 host: str = \"http://localhost:11434\"):\n+    def __init__(\n+        self, model_name: str = \"mistral:latest\", host: str = \"http://localhost:11434\"\n+    ):\n         self.model_name = model_name\n         self.host = host.rstrip(\"/\")\n \n     def suggest(self, prompt: str, max_suggestions: int = 5) -> List[str]:\n         url = f\"{self.host}/api/generate\"\n         payload = {\n             \"model\": self.model_name,\n             \"prompt\": f\"Compl\u00e8te ce code ou cette phrase : {prompt}\",\n             \"stream\": False,\n-            \"options\": {\"num_predict\": max_suggestions}\n+            \"options\": {\"num_predict\": max_suggestions},\n         }\n         try:\n             resp = requests.post(url, json=payload, timeout=10)\n             resp.raise_for_status()\n             data = resp.json()\n             # On d\u00e9coupe la r\u00e9ponse en suggestions (simple split pour la V1)\n             text = data.get(\"response\", \"\")\n-            suggestions = [s.strip() for s in text.split(\n-                \"\\n\") if s.strip()][:max_suggestions]\n+            suggestions = [s.strip() for s in text.split(\"\\n\") if s.strip()][\n+                :max_suggestions\n+            ]\n             if not suggestions:\n-                suggestions = [\n-                    f\"{prompt}_ollama_{i+1}\" for i in range(max_suggestions)]\n+                suggestions = [f\"{prompt}_ollama_{i+1}\" for i in range(max_suggestions)]\n             return suggestions\n         except Exception:\n             # Fallback simple en cas d'erreur\n-            return [\n-                f\"{prompt}_ollama_error_{i+1}\" for i in range(max_suggestions)]\n+            return [f\"{prompt}_ollama_error_{i+1}\" for i in range(max_suggestions)]\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust_broken.py\t2025-07-29 18:02:53.590000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust_broken.py\t2025-07-29 18:12:20.846401+00:00\n@@ -12,20 +12,22 @@\n import requests\n \n \n class AIModel(Enum):\n     \"\"\"Mod\u00e8les IA disponibles.\"\"\"\n+\n     OLLAMA_MISTRAL = \"ollama_mistral\"\n     OLLAMA_LLAMA = \"ollama_llama\"\n     OLLAMA_CODEGEN = \"ollama_codegen\"\n     OLLAMA_QWEN = \"ollama_qwen\"\n     OLLAMA_LLAVA = \"ollama_llava\"\n     MOCK = \"mock\"\n \n \n class PromptContext(Enum):\n     \"\"\"Contextes de prompts.\"\"\"\n+\n     BLUEPRINT = \"blueprint\"\n     CODE_REVIEW = \"code_review\"\n     DOCUMENTATION = \"documentation\"\n     TESTING = \"testing\"\n     SECURITY = \"security\"\n@@ -44,117 +46,114 @@\n         \"\"\"G\u00e9n\u00e8re un blueprint de projet \u00e0 partir dune id\u00e9e.\"\"\"\n         # Analyse intelligente de l'id\u00e9e'\n         idea_lower = idea.lower()\n \n         # D\u00e9tection du type de projet (priorit\u00e9 aux mots cl\u00e9s sp\u00e9cifiques)\n-        project_type = 'generic'\n-        if any(\n-            word in idea_lower for word in [\n-                'fastapi',\n-                'swagger',\n-                'openapi']):\n-            project_type = 'api'\n-        elif any(word in idea_lower for word in ['api', 'rest', 'endpoint']):\n-            project_type = 'api'\n-        elif any(word in idea_lower for word in ['robot', 'reachy', 'ros', 'opencv']):\n-            project_type = 'robotics'\n-        elif any(word in idea_lower for word in [\n-                'calculatrice', 'calculator', 'desktop', 'tkinter']):\n-            project_type = 'desktop'\n-        elif any(word in idea_lower for word in [\n-                'web', 'flask', 'django', 'interface', 'react', 'vue', 'angular']):\n-            project_type = 'web'\n-        elif any(word in idea_lower for word in ['ia', 'ai', 'machine learning', 'ml']):\n-            project_type = 'ai_application'\n+        project_type = \"generic\"\n+        if any(word in idea_lower for word in [\"fastapi\", \"swagger\", \"openapi\"]):\n+            project_type = \"api\"\n+        elif any(word in idea_lower for word in [\"api\", \"rest\", \"endpoint\"]):\n+            project_type = \"api\"\n+        elif any(word in idea_lower for word in [\"robot\", \"reachy\", \"ros\", \"opencv\"]):\n+            project_type = \"robotics\"\n+        elif any(\n+            word in idea_lower\n+            for word in [\"calculatrice\", \"calculator\", \"desktop\", \"tkinter\"]\n+        ):\n+            project_type = \"desktop\"\n+        elif any(\n+            word in idea_lower\n+            for word in [\n+                \"web\",\n+                \"flask\",\n+                \"django\",\n+                \"interface\",\n+                \"react\",\n+                \"vue\",\n+                \"angular\",\n+            ]\n+        ):\n+            project_type = \"web\"\n+        elif any(word in idea_lower for word in [\"ia\", \"ai\", \"machine learning\", \"ml\"]):\n+            project_type = \"ai_application\"\n \n         # Extraction du nom de projet\n         project_name = self._extract_project_name(idea)\n \n         # D\u00e9pendances selon le type\n-        dependencies = ['numpy', 'pandas']\n-        if project_type == 'api':\n-            dependencies.extend(['fastapi',\n-                                 'uvicorn',\n-                                 'pydantic',\n-                                 'sqlalchemy',\n-                                 'python-jose[cryptography]',\n-                                 'passlib[bcrypt]',\n-                                 'httpx'])\n-        elif project_type == 'web':\n-            dependencies.extend(['flask', 'requests', 'jinja2', 'flask-cors'])\n-        elif project_type == 'robotics':\n+        dependencies = [\"numpy\", \"pandas\"]\n+        if project_type == \"api\":\n             dependencies.extend(\n-                ['opencv-python', 'numpy', 'matplotlib', 'rospy'])\n-        elif project_type == 'desktop':\n-            dependencies.extend(['tkinter', 'matplotlib'])\n-        elif project_type == 'ai_application':\n-            dependencies.extend(['scikit-learn', 'tensorflow', 'torch'])\n+                [\n+                    \"fastapi\",\n+                    \"uvicorn\",\n+                    \"pydantic\",\n+                    \"sqlalchemy\",\n+                    \"python-jose[cryptography]\",\n+                    \"passlib[bcrypt]\",\n+                    \"httpx\",\n+                ]\n+            )\n+        elif project_type == \"web\":\n+            dependencies.extend([\"flask\", \"requests\", \"jinja2\", \"flask-cors\"])\n+        elif project_type == \"robotics\":\n+            dependencies.extend([\"opencv-python\", \"numpy\", \"matplotlib\", \"rospy\"])\n+        elif project_type == \"desktop\":\n+            dependencies.extend([\"tkinter\", \"matplotlib\"])\n+        elif project_type == \"ai_application\":\n+            dependencies.extend([\"scikit-learn\", \"tensorflow\", \"torch\"])\n \n         # D\u00e9tection des fonctionnalit\u00e9s\n-        has_docker = any(\n-            word in idea_lower for word in [\n-                'docker', 'container'])\n+        has_docker = any(word in idea_lower for word in [\"docker\", \"container\"])\n         has_cicd = any(\n-            word in idea_lower for word in [\n-                'ci',\n-                'cd',\n-                'github actions',\n-                'pipeline'])\n-        has_tests = any(\n-            word in idea_lower for word in [\n-                'test', 'unittest', 'pytest'])\n-        has_docs = any(\n-            word in idea_lower for word in [\n-                'doc', 'swagger', 'openapi'])\n+            word in idea_lower for word in [\"ci\", \"cd\", \"github actions\", \"pipeline\"]\n+        )\n+        has_tests = any(word in idea_lower for word in [\"test\", \"unittest\", \"pytest\"])\n+        has_docs = any(word in idea_lower for word in [\"doc\", \"swagger\", \"openapi\"])\n \n         # Structure du projet\n-        structure = [\n-            'src/',\n-            'tests/',\n-            'docs/',\n-            'requirements.txt',\n-            'README.md']\n+        structure = [\"src/\", \"tests/\", \"docs/\", \"requirements.txt\", \"README.md\"]\n         if has_docker:\n-            structure.extend(['Dockerfile', 'docker-compose.yml'])\n+            structure.extend([\"Dockerfile\", \"docker-compose.yml\"])\n         if has_cicd:\n-            structure.extend(['.github/workflows/'])\n+            structure.extend([\".github/workflows/\"])\n \n         # Modules selon le type\n-        modules = ['core', 'api', 'ui', 'tests', 'docs']\n-        if project_type == 'api':\n-            modules.extend(['auth', 'database', 'models'])\n-        elif project_type == 'web':\n-            modules.extend(['templates', 'static', 'routes'])\n-        elif project_type == 'robotics':\n-            modules.extend(['vision', 'control', 'navigation'])\n+        modules = [\"core\", \"api\", \"ui\", \"tests\", \"docs\"]\n+        if project_type == \"api\":\n+            modules.extend([\"auth\", \"database\", \"models\"])\n+        elif project_type == \"web\":\n+            modules.extend([\"templates\", \"static\", \"routes\"])\n+        elif project_type == \"robotics\":\n+            modules.extend([\"vision\", \"control\", \"navigation\"])\n \n         return {\n-            'project_name': project_name,\n-            'description': idea,\n-            'project_type': project_type,\n-            'modules': modules,\n-            'structure': structure,\n-            'dependencies': dependencies,\n-            'prompts': ['prompts/main.yaml'],\n-            'booster_ia': True,\n-            'docker': has_docker,\n-            'ci_cd': has_cicd,\n-            'tests': has_tests,\n-            'documentation': has_docs\n+            \"project_name\": project_name,\n+            \"description\": idea,\n+            \"project_type\": project_type,\n+            \"modules\": modules,\n+            \"structure\": structure,\n+            \"dependencies\": dependencies,\n+            \"prompts\": [\"prompts/main.yaml\"],\n+            \"booster_ia\": True,\n+            \"docker\": has_docker,\n+            \"ci_cd\": has_cicd,\n+            \"tests\": has_tests,\n+            \"documentation\": has_docs,\n         }\n \n     def _extract_project_name(self, idea: str) -> str:\n         \"\"\"Extrait un nom de projet de lid\u00e9e\"\"\"\n         import re\n \n         # Cherche des mots cl\u00e9s sp\u00e9cifiques\n         patterns = [\n-            r'calculatrice\\s+(\\w+)',\n-            r'application\\s+(\\w+)',\n-            r'robot\\s+(\\w+)',\n-            r'api\\s+(\\w+)',\n-            r'(\\w+)\\s+avec'\n+            r\"calculatrice\\s+(\\w+)\",\n+            r\"application\\s+(\\w+)\",\n+            r\"robot\\s+(\\w+)\",\n+            r\"api\\s+(\\w+)\",\n+            r\"(\\w+)\\s+avec\",\n         ]\n \n         for pattern in patterns:\n             match = re.search(pattern, idea, re.IGNORECASE)\n             if match:\n@@ -167,84 +166,82 @@\n                 return word.lower()\n \n         return \"projet_ia\"\n \n     def review_code(\n-            self,\n-            code: str,\n-            filename: str,\n-            project_type: str,\n-            current_score: int) -> dict:\n+        self, code: str, filename: str, project_type: str, current_score: int\n+    ) -> dict:\n         \"\"\"G\u00e9n\u00e8re une revue de code mock\u00e9e.\"\"\"\n         return {\n-            'score': current_score + 5,\n-            'issues': [\"Am\u00e9liorer la gestion d'erreurs\"],\n-            'suggestions': [\"Ajouter des docstrings\"]\n+            \"score\": current_score + 5,\n+            \"issues\": [\"Am\u00e9liorer la gestion d'erreurs\"],\n+            \"suggestions\": [\"Ajouter des docstrings\"],\n         }\n \n     def generate_documentation(\n-            self,\n-            project_name: str,\n-            project_type: str,\n-            modules: list) -> str:\n+        self, project_name: str, project_type: str, modules: list\n+    ) -> str:\n         \"\"\"G\u00e9n\u00e8re une documentation technique mock\u00e9e.\"\"\"\n-        return (f\"# Documentation de {project_name}\\n\\nType: {project_type}\\n\"\n-                f\"Modules: {', '.join(modules)}\\n...\")\n+        return (\n+            f\"# Documentation de {project_name}\\n\\nType: {project_type}\\n\"\n+            f\"Modules: {', '.join(modules)}\\n...\"\n+        )\n \n     def classify_project_complexity(self, codebase_path: str) -> dict:\n         \"\"\"Classifie la complexit\u00e9 dun projet (mock).\"\"\"\n-        return {\n-            'complexity': 'moyenne',\n-            'score': 50\n-        }\n+        return {\"complexity\": \"moyenne\", \"score\": 50}\n \n     def get_dynamic_prompt(self, context: str, **kwargs) -> str:\n         \"\"\"Retourne un prompt dynamique mock\u00e9 selon le contexte.\"\"\"\n         return self.prompt_templates.get(\n-            context, \"Prompt mock\u00e9 pour le contexte : \" + context)\n+            context, \"Prompt mock\u00e9 pour le contexte : \" + context\n+        )\n \n     class _BlueprintProxy:\n         def __init__(self, parent):\n             self.parent = parent\n \n         def info(self, *args, **kwargs):\n             return self.parent.generate_blueprint(*args, **kwargs)\n+\n     # Ajout dun proxy robuste pour supporter generate_bluelogger.info partout\n \n     @property\n     def generate_bluelogger(self):\n         return self._BlueprintProxy(self)\n+\n     # Alias pour compatibilit\u00e9\n \n     def generate_blueprint_mock(self, *args, **kwargs):\n         return self.generate_blueprint(*args, **kwargs)\n \n     def save_blueprint(self, *args, **kwargs):\n         from athalia_core import generation\n+\n         return generation.save_blueprint(*args, **kwargs)\n \n     def scan_existing_project(self, *args, **kwargs):\n         from athalia_core import generation\n+\n         return generation.scan_existing_project(*args, **kwargs)\n \n     def _detect_available_models(self) -> List[AIModel]:\n         \"\"\"D\u00e9tecte les mod\u00e8les IA disponibles.\"\"\"\n         available = []\n         try:\n-            result = subprocess.run(\n-                ['ollama', 'list'], capture_output=True, text=True)\n+            result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n             if result.returncode == 0:\n                 output = result.stdout.lower()\n-                if 'qwen' in output:\n+                if \"qwen\" in output:\n                     available.append(AIModel.OLLAMA_QWEN)\n-                if 'mistral' in output:\n+                if \"mistral\" in output:\n                     available.append(AIModel.OLLAMA_MISTRAL)\n-                if 'llava' in output:\n+                if \"llava\" in output:\n                     available.append(AIModel.OLLAMA_LLAVA)\n-                if 'llama' in output:\n+                if \"llama\" in output:\n                     available.append(AIModel.OLLAMA_LLAMA)\n-                if 'codegen' in output:\n+                if \"codegen\" in output:\n                     available.append(AIModel.OLLAMA_CODEGEN)\n         except Exception as e:\n             logging.warning(f\"Ollama non d\u00e9tect\u00e9: {e}\")\n         available.append(AIModel.MOCK)\n         logging.info(f\"Mod\u00e8les IA disponibles: {[m.value for m in available]}\")\n@@ -258,11 +255,11 @@\n             AIModel.OLLAMA_QWEN,\n             AIModel.OLLAMA_MISTRAL,\n             AIModel.OLLAMA_LLAVA,\n             AIModel.OLLAMA_LLAMA,\n             AIModel.OLLAMA_CODEGEN,\n-            AIModel.MOCK\n+            AIModel.MOCK,\n         ]\n         for model in priority_models:\n             if model in self.available_models:\n                 chain.append(model)\n         return chain\n@@ -357,78 +354,78 @@\n \"\"\"\n             ),\n         }\n \n     def generate_response(\n-            self,\n-            context: PromptContext,\n-            distillation: bool = False,\n-            **kwargs) -> dict:\n+        self, context: PromptContext, distillation: bool = False, **kwargs\n+    ) -> dict:\n         \"\"\"G\u00e9n\u00e8re une r\u00e9ponse IA robuste avec fallback.\"\"\"\n         prompt = self._get_dynamic_prompt(context, **kwargs)\n \n         # Essayer chaque mod\u00e8le dans la cha\u00eene de fallback\n         for model in self.fallback_chain:\n             try:\n                 response = self._call_model(model, prompt)\n                 if response:\n                     return {\n-                        'model': model.value,\n-                        'response': response,\n-                        'success': True,\n-                        'context': context.value\n+                        \"model\": model.value,\n+                        \"response\": response,\n+                        \"success\": True,\n+                        \"context\": context.value,\n                     }\n             except Exception as e:\n                 logging.warning(f\"Mod\u00e8le {model.value} \u00e9chou\u00e9: {e}\")\n                 continue\n \n         # Fallback final\n         return {\n-            'model': 'mock',\n-            'response': self._mock_response(prompt),\n-            'success': False,\n-            'context': context.value,\n-            'error': 'Tous les mod\u00e8les ont \u00e9chou\u00e9'\n+            \"model\": \"mock\",\n+            \"response\": self._mock_response(prompt),\n+            \"success\": False,\n+            \"context\": context.value,\n+            \"error\": \"Tous les mod\u00e8les ont \u00e9chou\u00e9\",\n         }\n \n     def _call_model(self, model: AIModel, prompt: str) -> Optional[str]:\n         \"\"\"Appelle un mod\u00e8le IA sp\u00e9cifique.\"\"\"\n         if model == AIModel.MOCK:\n             return self._mock_response(prompt)\n-        elif model.value.startswith('ollama_'):\n-            model_name = model.value.replace('ollama_', '')\n+        elif model.value.startswith(\"ollama_\"):\n+            model_name = model.value.replace(\"ollama_\", \"\")\n             return self._call_ollama(model_name, prompt)\n         else:\n             logging.warning(f\"Mod\u00e8le non support\u00e9: {model.value}\")\n             return None\n \n     def _classify_project_complexity(self, codebase_path: str) -> dict:\n         \"\"\"Alias priv\u00e9 pour compatibilit\u00e9 avec les tests.\"\"\"\n-        if 'f' in codebase_path:\n-            return {'complexity': 'f'}\n+        if \"f\" in codebase_path:\n+            return {\"complexity\": \"f\"}\n         return self.classify_project_complexity(codebase_path)\n \n     def _get_dynamic_prompt(self, context, **kwargs) -> str:\n         \"\"\"Alias priv\u00e9 pour compatibilit\u00e9 avec les tests.\n         Accepte PromptContext ou str et fait un .format sur le template.\"\"\"\n-        ctx = context.value if hasattr(context, 'value') else str(context)\n+        ctx = context.value if hasattr(context, \"value\") else str(context)\n         template = self.prompt_templates.get(\n-            ctx, f\"Prompt mock\u00e9 pour le contexte : {ctx}\")\n+            ctx, f\"Prompt mock\u00e9 pour le contexte : {ctx}\"\n+        )\n         try:\n             return template.format(**kwargs)\n         except Exception:\n             return template\n \n-    def _call_ollama(self, model_name: str, prompt: str,\n-                     timeout: int = 30) -> Optional[str]:\n+    def _call_ollama(\n+        self, model_name: str, prompt: str, timeout: int = 30\n+    ) -> Optional[str]:\n         \"\"\"Appelle Ollama avec un mod\u00e8le sp\u00e9cifique et timeout param\u00e9trable.\"\"\"\n         try:\n             result = subprocess.run(\n-                ['ollama', 'run', model_name, prompt],\n+                [\"ollama\", \"run\", model_name, prompt],\n                 capture_output=True,\n                 text=True,\n-                timeout=timeout\n+                timeout=timeout,\n             )\n             if result.returncode == 0:\n                 return result.stdout.strip()\n             else:\n                 logging.error(f\"Ollama erreur: {result.stderr}\")\n@@ -502,16 +499,12 @@\n def query_qwen(prompt: str) -> str:\n     \"\"\"Appel local \u00e0 Qwen 7B via Ollama.\"\"\"\n     try:\n         response = requests.post(\n             \"http://localhost:11434/api/generate\",\n-            json={\n-                \"model\": \"qwen:7b\",\n-                \"prompt\": prompt,\n-                \"stream\": False\n-            },\n-            timeout=30\n+            json={\"model\": \"qwen:7b\", \"prompt\": prompt, \"stream\": False},\n+            timeout=30,\n         )\n         if response.status_code == 200:\n             return response.json().get(\"response\", \"\")\n         else:\n             logging.error(f\"Erreur Qwen: {response.status_code}\")\n@@ -524,16 +517,12 @@\n def query_mistral(prompt: str) -> str:\n     \"\"\"Appel local \u00e0 Mistral 7B via Ollama.\"\"\"\n     try:\n         response = requests.post(\n             \"http://localhost:11434/api/generate\",\n-            json={\n-                \"model\": \"mistral:7b\",\n-                \"prompt\": prompt,\n-                \"stream\": False\n-            },\n-            timeout=30\n+            json={\"model\": \"mistral:7b\", \"prompt\": prompt, \"stream\": False},\n+            timeout=30,\n         )\n         if response.status_code == 200:\n             return response.json().get(\"response\", \"\")\n         else:\n             logging.error(f\"Erreur Mistral: {response.status_code}\")\n@@ -551,8 +540,8 @@\n     # Test de g\u00e9n\u00e9ration\n     response = ai.generate_response(\n         PromptContext.BLUEPRINT,\n         idea=\"Assistant IA pour la gestion de projets\",\n         project_type=\"ai_assistant\",\n-        complexity=\"medium\"\n+        complexity=\"medium\",\n     )\n     print(f\"R\u00e9ponse g\u00e9n\u00e9r\u00e9e: {response[:100]}...\")\n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_cicd.py\t2025-07-29 18:02:53.600000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_cicd.py\t2025-07-29 18:12:20.848920+00:00\n@@ -29,19 +29,16 @@\n         # G\u00e9n\u00e9ration des configurations\n         github_actions = self._generate_github_actions()\n         docker_config = self._generate_docker_config()\n         deployment_config = self._generate_deployment_config()\n         # Sauvegarde des configurations\n-        self._save_cicd_configs(\n-            github_actions,\n-            docker_config,\n-            deployment_config)\n+        self._save_cicd_configs(github_actions, docker_config, deployment_config)\n         return {\n             \"github_actions\": github_actions,\n             \"docker_config\": docker_config,\n             \"deployment_config\": deployment_config,\n-            \"created_files\": self._get_created_files()\n+            \"created_files\": self._get_created_files(),\n         }\n \n     def _analyze_project(self):\n         \"\"\"Analyse du projet pour la CI/CD\"\"\"\n         self.project_info = {\n@@ -49,11 +46,11 @@\n             \"type\": self._detect_project_type(),\n             \"languages\": self._detect_languages(),\n             \"dependencies\": self._extract_dependencies(),\n             \"entry_points\": self._find_entry_points(),\n             \"has_tests\": self._has_tests(),\n-            \"has_documentation\": self._has_documentation()\n+            \"has_documentation\": self._has_documentation(),\n         }\n \n     def _detect_project_type(self) -> str:\n         \"\"\"D\u00e9tection du type de projet\"\"\"\n         if (self.project_path / \"package.json\").exists():\n@@ -73,44 +70,47 @@\n         \"\"\"D\u00e9tection des langages du projet\"\"\"\n         languages = set()\n         for file_path in self.project_path.rglob(\"*\"):\n             if file_path.is_file():\n                 ext = file_path.suffix.lower()\n-                if ext == '.py':\n-                    languages.add('python')\n-                elif ext in ['.js', '.jsx', '.ts', '.tsx']:\n-                    languages.add('javascript')\n-                elif ext == '.java':\n-                    languages.add('java')\n-                elif ext == '.rs':\n-                    languages.add('rust')\n-                elif ext == '.go':\n-                    languages.add('go')\n+                if ext == \".py\":\n+                    languages.add(\"python\")\n+                elif ext in [\".js\", \".jsx\", \".ts\", \".tsx\"]:\n+                    languages.add(\"javascript\")\n+                elif ext == \".java\":\n+                    languages.add(\"java\")\n+                elif ext == \".rs\":\n+                    languages.add(\"rust\")\n+                elif ext == \".go\":\n+                    languages.add(\"go\")\n         return list(languages)\n \n     def _extract_dependencies(self) -> Dict[str, List[str]]:\n         \"\"\"Extraction des d\u00e9pendances du projet\"\"\"\n         dependencies = {}\n         # Python\n         req_file = self.project_path / \"requirements.txt\"\n         if req_file.exists():\n             try:\n-                with open(req_file, 'r') as file_handle:\n-                    deps = [line.strip() for line in file_handle if line.strip()\n-                            and not line.startswith('#')]\n-                    dependencies['python'] = deps\n+                with open(req_file, \"r\") as file_handle:\n+                    deps = [\n+                        line.strip()\n+                        for line in file_handle\n+                        if line.strip() and not line.startswith(\"#\")\n+                    ]\n+                    dependencies[\"python\"] = deps\n             except Exception:\n                 pass\n         # Node.js\n         package_file = self.project_path / \"package.json\"\n         if package_file.exists():\n             try:\n-                with open(package_file, 'r') as file_handle:\n+                with open(package_file, \"r\") as file_handle:\n                     data = json.load(file_handle)\n-                    deps = list(data.get('dependencies', {}).keys())\n-                    dev_deps = list(data.get('devDependencies', {}).keys())\n-                    dependencies['nodejs'] = deps + dev_deps\n+                    deps = list(data.get(\"dependencies\", {}).keys())\n+                    dev_deps = list(data.get(\"devDependencies\", {}).keys())\n+                    dependencies[\"nodejs\"] = deps + dev_deps\n             except Exception:\n                 pass\n         return dependencies\n \n     def _find_entry_points(self) -> List[str]:\n@@ -161,33 +161,31 @@\n         configs = {}\n         configs[\"k8s-deployment.yaml\"] = \"# k8s deployment content\"\n         configs[\"k8s-service.yaml\"] = \"# k8s service content\"\n         return configs\n \n-    def _save_cicd_configs(\n-            self,\n-            github_actions,\n-            docker_config,\n-            deployment_config):\n+    def _save_cicd_configs(self, github_actions, docker_config, deployment_config):\n         from pathlib import Path\n-        ci_dir = Path(self.project_path) / '.f' / 'f'\n+\n+        ci_dir = Path(self.project_path) / \".f\" / \"f\"\n         ci_dir.mkdir(parents=True, exist_ok=True)\n-        (ci_dir / 'ci.f(f').write_text('# CI/CD config')\n+        (ci_dir / \"ci.f(f\").write_text(\"# CI/CD config\")\n \n     def _get_created_files(self) -> List[str]:\n         # Retourne le chemin du fichier ci.f(f) pour les tests\n-        ci_file = self.project_path / '.f' / 'f' / 'ci.f(f'\n+        ci_file = self.project_path / \".f\" / \"f\" / \"ci.f(f\"\n         return [str(ci_file)] if ci_file.exists() else []\n \n \n def generate_github_ci_yaml(outdir):\n     from pathlib import Path\n+\n     outdir = Path(str(outdir))  # Force la conversion\n-    ci_dir = outdir / '.f' / 'f'\n+    ci_dir = outdir / \".f\" / \"f\"\n     ci_dir.mkdir(parents=True, exist_ok=True)\n-    (ci_dir / 'ci.f(f').write_text('# CI/CD config')\n+    (ci_dir / \"ci.f(f\").write_text(\"# CI/CD config\")\n     print(f'[DEBUG] Fichier g\u00e9n\u00e9r\u00e9 : {ci_dir / \"ci.f(f\"}')\n \n \n-__all__ = ['AutoCICD', 'generate_github_ci_yaml']\n-globals()['generate_github_ci_yaml'] = generate_github_ci_yaml\n+__all__ = [\"AutoCICD\", \"generate_github_ci_yaml\"]\n+globals()[\"generate_github_ci_yaml\"] = generate_github_ci_yaml\n builtins.generate_github_ci_yaml = generate_github_ci_yaml\n--- /Volumes/T7/athalia-dev-setup/athalia_core/architecture_analyzer.py\t2025-07-29 17:56:23.090000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/architecture_analyzer.py\t2025-07-29 18:12:20.850616+00:00\n@@ -21,10 +21,11 @@\n \n \n @dataclass\n class ModuleAnalysis:\n     \"\"\"Analyse d'un module\"\"\"\n+\n     name: str\n     path: str\n     type: str\n     size: int\n     functions: List[str]\n@@ -38,20 +39,22 @@\n \n \n @dataclass\n class PerformanceIssue:\n     \"\"\"Probl\u00e8me de performance\"\"\"\n+\n     type: str\n     location: str\n     description: str\n     impact: str\n     suggestion: str\n \n \n @dataclass\n class ArchitectureMapping:\n     \"\"\"Mapping de l'architecture compl\u00e8te\"\"\"\n+\n     modules: Dict[str, ModuleAnalysis]\n     dependencies: Dict[str, List[str]]\n     duplicates: List[Any]  # DuplicateAnalysis\n     performance_issues: List[PerformanceIssue]\n     recommendations: List[str]\n@@ -75,20 +78,20 @@\n         self.ast_analyzer = ASTAnalyzer()\n \n         # Charger la configuration\n         self.config = self._load_config()\n \n-        logger.info(\n-            f\"\ud83c\udfd7\ufe0f Architecture Analyzer initialis\u00e9 dans {self.root_path}\")\n+        logger.info(f\"\ud83c\udfd7\ufe0f Architecture Analyzer initialis\u00e9 dans {self.root_path}\")\n \n     def _init_database(self):\n         \"\"\"Initialiser la base de donn\u00e9es d'architecture\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Table des modules\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS modules (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     name TEXT UNIQUE NOT NULL,\n                     path TEXT NOT NULL,\n                     type TEXT NOT NULL,\n@@ -101,44 +104,49 @@\n                     issues TEXT NOT NULL,\n                     performance_score REAL NOT NULL,\n                     last_modified TEXT NOT NULL,\n                     analyzed_at TEXT NOT NULL\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des d\u00e9pendances\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS dependencies (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     module_name TEXT NOT NULL,\n                     dependency_name TEXT NOT NULL,\n                     dependency_type TEXT NOT NULL,\n                     strength REAL DEFAULT 1.0,\n                     created_at TEXT NOT NULL\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des probl\u00e8mes de performance\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS performance_issues (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     issue_type TEXT NOT NULL,\n                     location TEXT NOT NULL,\n                     description TEXT NOT NULL,\n                     impact TEXT NOT NULL,\n                     suggestion TEXT NOT NULL,\n                     detected_at TEXT NOT NULL,\n                     resolved_at TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             conn.commit()\n \n     def _load_config(self) -> Dict[str, Any]:\n         \"\"\"Charger la configuration\"\"\"\n         if self.config_path.exists():\n-            with open(self.config_path, 'r', encoding='utf-8') as f:\n+            with open(self.config_path, \"r\", encoding=\"utf-8\") as f:\n                 return yaml.safe_load(f)\n         return {}\n \n     def analyze_entire_architecture(self) -> ArchitectureMapping:\n         \"\"\"Analyser l'architecture compl\u00e8te du projet\"\"\"\n@@ -156,19 +164,20 @@\n         # Construire le graphe de d\u00e9pendances\n         dependencies = self._build_dependency_graph(modules)\n \n         # G\u00e9n\u00e9rer les recommandations\n         recommendations = self._generate_recommendations(\n-            modules, duplicates, performance_issues)\n+            modules, duplicates, performance_issues\n+        )\n \n         # Cr\u00e9er le mapping d'architecture\n         architecture = ArchitectureMapping(\n             modules=modules,\n             dependencies=dependencies,\n             duplicates=duplicates,\n             performance_issues=performance_issues,\n-            recommendations=recommendations\n+            recommendations=recommendations,\n         )\n \n         # Sauvegarder l'analyse\n         self._save_architecture_analysis(architecture)\n \n@@ -181,44 +190,39 @@\n         # Analyser les fichiers Python dans athalia_core (ignorer les fichiers\n         # cach\u00e9s)\n         core_path = self.root_path / \"athalia_core\"\n         if core_path.exists():\n             for py_file in core_path.rglob(\"*.py\"):\n-                if py_file.name != \"__init__.py\" and not py_file.name.startswith(\n-                        '._'):\n-                    module_analysis = self._analyze_single_module(\n-                        py_file, \"core\")\n+                if py_file.name != \"__init__.py\" and not py_file.name.startswith(\"._\"):\n+                    module_analysis = self._analyze_single_module(py_file, \"core\")\n                     if module_analysis:\n                         modules[module_analysis.name] = module_analysis\n \n         # Analyser les fichiers Python dans tests (ignorer les fichiers cach\u00e9s)\n         tests_path = self.root_path / \"tests\"\n         if tests_path.exists():\n             for py_file in tests_path.rglob(\"*.py\"):\n-                if not py_file.name.startswith('._'):\n-                    module_analysis = self._analyze_single_module(\n-                        py_file, \"test\")\n+                if not py_file.name.startswith(\"._\"):\n+                    module_analysis = self._analyze_single_module(py_file, \"test\")\n                     if module_analysis:\n                         modules[module_analysis.name] = module_analysis\n \n         # Analyser les fichiers Python dans setup (ignorer les fichiers cach\u00e9s)\n         setup_path = self.root_path / \"setup\"\n         if setup_path.exists():\n             for py_file in setup_path.rglob(\"*.py\"):\n-                if not py_file.name.startswith('._'):\n-                    module_analysis = self._analyze_single_module(\n-                        py_file, \"setup\")\n+                if not py_file.name.startswith(\"._\"):\n+                    module_analysis = self._analyze_single_module(py_file, \"setup\")\n                     if module_analysis:\n                         modules[module_analysis.name] = module_analysis\n \n         logger.info(f\"\ud83d\udcca {len(modules)} modules analys\u00e9s\")\n         return modules\n \n     def _analyze_single_module(\n-            self,\n-            file_path: Path,\n-            module_type: str) -> Optional[ModuleAnalysis]:\n+        self, file_path: Path, module_type: str\n+    ) -> Optional[ModuleAnalysis]:\n         \"\"\"Analyser un module individuel\"\"\"\n         try:\n             # Analyser le fichier avec l'analyseur AST\n             file_analysis = self.ast_analyzer.analyze_file(file_path)\n             if not file_analysis:\n@@ -235,12 +239,11 @@\n \n             # D\u00e9tecter les probl\u00e8mes\n             issues = self._detect_module_issues(file_analysis)\n \n             # Calculer le score de performance\n-            performance_score = self._calculate_performance_score(\n-                file_analysis)\n+            performance_score = self._calculate_performance_score(file_analysis)\n \n             return ModuleAnalysis(\n                 name=module_name,\n                 path=str(file_path),\n                 type=module_type,\n@@ -250,22 +253,18 @@\n                 imports=imports,\n                 dependencies=dependencies,\n                 complexity=file_analysis.complexity_score,\n                 issues=issues,\n                 performance_score=performance_score,\n-                last_modified=file_analysis.last_modified\n+                last_modified=file_analysis.last_modified,\n             )\n \n         except Exception as e:\n-            logger.error(\n-                f\"Erreur lors de l'analyse du module {file_path}: {e}\")\n+            logger.error(f\"Erreur lors de l'analyse du module {file_path}: {e}\")\n             return None\n \n-    def _extract_dependencies(\n-            self,\n-            imports: List[str],\n-            module_type: str) -> List[str]:\n+    def _extract_dependencies(self, imports: List[str], module_type: str) -> List[str]:\n         \"\"\"Extraire les d\u00e9pendances d'un module\"\"\"\n         dependencies = []\n \n         for imp in imports:\n             # Filtrer les imports internes au projet\n@@ -285,17 +284,15 @@\n         \"\"\"D\u00e9tecter les probl\u00e8mes dans un module\"\"\"\n         issues = []\n \n         # V\u00e9rifier la complexit\u00e9\n         if file_analysis.complexity_score > 10:\n-            issues.append(\n-                f\"Complexit\u00e9 \u00e9lev\u00e9e: {file_analysis.complexity_score:.1f}\")\n+            issues.append(f\"Complexit\u00e9 \u00e9lev\u00e9e: {file_analysis.complexity_score:.1f}\")\n \n         # V\u00e9rifier la taille\n         if file_analysis.total_lines > 500:\n-            issues.append(\n-                f\"Fichier tr\u00e8s long: {file_analysis.total_lines} lignes\")\n+            issues.append(f\"Fichier tr\u00e8s long: {file_analysis.total_lines} lignes\")\n \n         # V\u00e9rifier le nombre de fonctions\n         if len(file_analysis.functions) > 20:\n             issues.append(f\"Trop de fonctions: {len(file_analysis.functions)}\")\n \n@@ -307,12 +304,11 @@\n         if len(file_analysis.imports) > 30:\n             issues.append(f\"Trop d'imports: {len(file_analysis.imports)}\")\n \n         return issues\n \n-    def _calculate_performance_score(\n-            self, file_analysis: FileAnalysis) -> float:\n+    def _calculate_performance_score(self, file_analysis: FileAnalysis) -> float:\n         \"\"\"Calculer un score de performance pour un module\"\"\"\n         score = 100.0\n \n         # P\u00e9naliser la complexit\u00e9\n         if file_analysis.complexity_score > 5:\n@@ -330,19 +326,19 @@\n         if len(file_analysis.classes) > 5:\n             score -= (len(file_analysis.classes) - 5) * 1\n \n         return max(0.0, score)\n \n-    def _detect_duplicates(\n-            self, modules: Dict[str, ModuleAnalysis]) -> List[Any]:\n+    def _detect_duplicates(self, modules: Dict[str, ModuleAnalysis]) -> List[Any]:\n         \"\"\"D\u00e9tecter les doublons entre modules\"\"\"\n         # Cette fonction sera impl\u00e9ment\u00e9e en utilisant le PatternDetector\n         # Pour l'instant, retourner une liste vide\n         return []\n \n     def _analyze_performance(\n-            self, modules: Dict[str, ModuleAnalysis]) -> List[PerformanceIssue]:\n+        self, modules: Dict[str, ModuleAnalysis]\n+    ) -> List[PerformanceIssue]:\n         \"\"\"Analyser les probl\u00e8mes de performance\"\"\"\n         issues = []\n \n         for module_name, module in modules.items():\n             # V\u00e9rifier les scores de performance faibles\n@@ -353,108 +349,122 @@\n                     description=(\n                         f\"Module {module_name} a un score de performance faible: \"\n                         f\"{module.performance_score:.1f}\"\n                     ),\n                     impact=\"medium\",\n-                    suggestion=\"Refactoriser pour r\u00e9duire la complexit\u00e9 et la taille\")\n+                    suggestion=\"Refactoriser pour r\u00e9duire la complexit\u00e9 et la taille\",\n+                )\n                 issues.append(issue)\n \n             # V\u00e9rifier les modules tr\u00e8s complexes\n             if module.complexity > 15:\n                 issue = PerformanceIssue(\n                     type=\"high_complexity\",\n                     location=module.path,\n                     description=f\"Module {module_name} tr\u00e8s complexe: {module.complexity:.1f}\",\n                     impact=\"high\",\n-                    suggestion=\"Diviser en modules plus petits\")\n+                    suggestion=\"Diviser en modules plus petits\",\n+                )\n                 issues.append(issue)\n \n         return issues\n \n     def _build_dependency_graph(\n-            self, modules: Dict[str, ModuleAnalysis]) -> Dict[str, List[str]]:\n+        self, modules: Dict[str, ModuleAnalysis]\n+    ) -> Dict[str, List[str]]:\n         \"\"\"Construire le graphe de d\u00e9pendances\"\"\"\n         dependencies = {}\n \n         for module_name, module in modules.items():\n             dependencies[module_name] = module.dependencies\n \n         return dependencies\n \n-    def _generate_recommendations(self,\n-                                  modules: Dict[str,\n-                                                ModuleAnalysis],\n-                                  duplicates: List[Any],\n-                                  performance_issues: List[PerformanceIssue]) -> List[str]:\n+    def _generate_recommendations(\n+        self,\n+        modules: Dict[str, ModuleAnalysis],\n+        duplicates: List[Any],\n+        performance_issues: List[PerformanceIssue],\n+    ) -> List[str]:\n         \"\"\"G\u00e9n\u00e9rer des recommandations d'architecture\"\"\"\n         recommendations = []\n \n         # Recommandations bas\u00e9es sur les modules\n         large_modules = [m for m in modules.values() if m.size > 300]\n         if large_modules:\n             recommendations.append(\n-                f\"\ud83d\udce6 {len(large_modules)} modules tr\u00e8s grands d\u00e9tect\u00e9s - consid\u00e9rer la division\")\n+                f\"\ud83d\udce6 {len(large_modules)} modules tr\u00e8s grands d\u00e9tect\u00e9s - consid\u00e9rer la division\"\n+            )\n \n         complex_modules = [m for m in modules.values() if m.complexity > 10]\n         if complex_modules:\n             recommendations.append(\n-                f\"\ud83e\udde0 {len(complex_modules)} modules complexes - refactoring recommand\u00e9\")\n+                f\"\ud83e\udde0 {len(complex_modules)} modules complexes - refactoring recommand\u00e9\"\n+            )\n \n         # Recommandations bas\u00e9es sur les performances\n         if performance_issues:\n             recommendations.append(\n-                f\"\u26a1 {len(performance_issues)} probl\u00e8mes de performance - optimisation n\u00e9cessaire\")\n+                f\"\u26a1 {len(performance_issues)} probl\u00e8mes de performance - optimisation n\u00e9cessaire\"\n+            )\n \n         # Recommandations g\u00e9n\u00e9rales\n         if len(modules) > 20:\n             recommendations.append(\n-                \"\ud83c\udfd7\ufe0f Architecture complexe - documenter les d\u00e9pendances\")\n+                \"\ud83c\udfd7\ufe0f Architecture complexe - documenter les d\u00e9pendances\"\n+            )\n \n         return recommendations\n \n     def _save_architecture_analysis(self, architecture: ArchitectureMapping):\n         \"\"\"Sauvegarder l'analyse d'architecture\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Sauvegarder les modules\n             for module_name, module in architecture.modules.items():\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT OR REPLACE INTO modules\n                     (name, path, type, size, functions, classes, imports, dependencies,\n                      complexity, issues, performance_score, last_modified, analyzed_at)\n                     VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    module.name,\n-                    module.path,\n-                    module.type,\n-                    module.size,\n-                    json.dumps(module.functions),\n-                    json.dumps(module.classes),\n-                    json.dumps(module.imports),\n-                    json.dumps(module.dependencies),\n-                    module.complexity,\n-                    json.dumps(module.issues),\n-                    module.performance_score,\n-                    module.last_modified.isoformat(),\n-                    datetime.now().isoformat()\n-                ))\n+                \"\"\",\n+                    (\n+                        module.name,\n+                        module.path,\n+                        module.type,\n+                        module.size,\n+                        json.dumps(module.functions),\n+                        json.dumps(module.classes),\n+                        json.dumps(module.imports),\n+                        json.dumps(module.dependencies),\n+                        module.complexity,\n+                        json.dumps(module.issues),\n+                        module.performance_score,\n+                        module.last_modified.isoformat(),\n+                        datetime.now().isoformat(),\n+                    ),\n+                )\n \n             # Sauvegarder les probl\u00e8mes de performance\n             for issue in architecture.performance_issues:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO performance_issues\n                     (issue_type, location, description, impact, suggestion, detected_at)\n                     VALUES (?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    issue.type,\n-                    issue.location,\n-                    issue.description,\n-                    issue.impact,\n-                    issue.suggestion,\n-                    datetime.now().isoformat()\n-                ))\n+                \"\"\",\n+                    (\n+                        issue.type,\n+                        issue.location,\n+                        issue.description,\n+                        issue.impact,\n+                        issue.suggestion,\n+                        datetime.now().isoformat(),\n+                    ),\n+                )\n \n             conn.commit()\n \n     def get_optimization_plan(self) -> Dict[str, Any]:\n         \"\"\"Obtenir un plan d'optimisation bas\u00e9 sur l'analyse\"\"\"\n@@ -473,47 +483,54 @@\n             avg_performance = cursor.fetchone()[0] or 0\n \n             # Modules probl\u00e9matiques\n             cursor.execute(\n                 \"SELECT name, complexity, performance_score FROM modules \"\n-                \"WHERE complexity > 10 OR performance_score < 50\")\n+                \"WHERE complexity > 10 OR performance_score < 50\"\n+            )\n             problematic_modules = cursor.fetchall()\n \n-        return {\"total_modules\": total_modules,\n-                \"average_complexity\": avg_complexity,\n-                \"average_performance\": avg_performance,\n-                \"problematic_modules\": problematic_modules,\n-                \"optimization_score\": max(\n-                    0, 100 - (avg_complexity * 2 + (100 - avg_performance) * 0.5)\n-                )}\n+        return {\n+            \"total_modules\": total_modules,\n+            \"average_complexity\": avg_complexity,\n+            \"average_performance\": avg_performance,\n+            \"problematic_modules\": problematic_modules,\n+            \"optimization_score\": max(\n+                0, 100 - (avg_complexity * 2 + (100 - avg_performance) * 0.5)\n+            ),\n+        }\n \n     def generate_intelligent_coordination(self) -> Dict[str, Any]:\n         \"\"\"G\u00e9n\u00e9rer des recommandations de coordination intelligente\"\"\"\n         optimization_plan = self.get_optimization_plan()\n \n         coordination_plan = {\n             \"priority_tasks\": [],\n             \"parallel_tasks\": [],\n             \"dependencies\": [],\n-            \"estimated_time\": 0\n+            \"estimated_time\": 0,\n         }\n \n         # T\u00e2ches prioritaires bas\u00e9es sur l'analyse\n         if optimization_plan[\"average_complexity\"] > 8:\n-            coordination_plan[\"priority_tasks\"].append({\n-                \"task\": \"refactor_high_complexity_modules\",\n-                \"description\": \"Refactoriser les modules tr\u00e8s complexes\",\n-                \"effort\": \"high\",\n-                \"impact\": \"high\"\n-            })\n+            coordination_plan[\"priority_tasks\"].append(\n+                {\n+                    \"task\": \"refactor_high_complexity_modules\",\n+                    \"description\": \"Refactoriser les modules tr\u00e8s complexes\",\n+                    \"effort\": \"high\",\n+                    \"impact\": \"high\",\n+                }\n+            )\n             coordination_plan[\"estimated_time\"] += 4  # heures\n \n         if optimization_plan[\"average_performance\"] < 70:\n-            coordination_plan[\"priority_tasks\"].append({\n-                \"task\": \"optimize_performance\",\n-                \"description\": \"Optimiser les performances des modules\",\n-                \"effort\": \"medium\",\n-                \"impact\": \"medium\"\n-            })\n+            coordination_plan[\"priority_tasks\"].append(\n+                {\n+                    \"task\": \"optimize_performance\",\n+                    \"description\": \"Optimiser les performances des modules\",\n+                    \"effort\": \"medium\",\n+                    \"impact\": \"medium\",\n+                }\n+            )\n             coordination_plan[\"estimated_time\"] += 2  # heures\n \n         return coordination_plan\n--- /Volumes/T7/athalia-dev-setup/athalia_core/classification/__init__.py\t2025-07-29 17:56:26.650000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/classification/__init__.py\t2025-07-29 18:12:20.853555+00:00\n@@ -12,6 +12,6 @@\n Module de classification intelligente des projets.\n Analyse le contexte et d\u00e9termine le type de projet appropri\u00e9.\n \"\"\"\n \n \n-__all__ = ['classify_project', 'ProjectType', 'get_project_config']\n+__all__ = [\"classify_project\", \"ProjectType\", \"get_project_config\"]\n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/auto_correction_advanced.py\t2025-07-29 17:56:21.080000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/auto_correction_advanced.py\t2025-07-29 18:12:20.852638+00:00\n@@ -27,11 +27,11 @@\n \n         resultats = {\n             \"corrections_appliquees\": [],\n             \"suggestions\": [],\n             \"fichiers_traites\": 0,\n-            \"erreurs_corrigees\": 0\n+            \"erreurs_corrigees\": 0,\n         }\n \n         # 1. Correction syntaxique avanc\u00e9e\n         resultats.update(self._corriger_syntaxe_avancee(dry_run))\n \n@@ -45,20 +45,20 @@\n         resultats.update(self._corriger_anti_patterns(dry_run))\n \n         # 5. Am\u00e9lioration de la lisibilit\u00e9\n         resultats.update(self._ameliorer_lisibilite(dry_run))\n \n-        corrections_appliquees = resultats.get('corrections_appliquees', [])\n+        corrections_appliquees = resultats.get(\"corrections_appliquees\", [])\n         if not isinstance(corrections_appliquees, list):\n             corrections_appliquees = []\n         corrections_count = len(corrections_appliquees)\n         logger.info(\n             f\"\u2705 Auto-correction termin\u00e9e: \"\n             f\"{corrections_count} corrections appliqu\u00e9es\"\n         )\n         retour = resultats.copy()\n-        retour['resultats'] = resultats\n+        retour[\"resultats\"] = resultats\n         return retour\n \n     def _corriger_syntaxe_avancee(self, dry_run: bool) -> Dict[str, Any]:\n         \"\"\"Correction syntaxique avanc\u00e9e avec analyse AST\"\"\"\n         corrections = []\n@@ -72,22 +72,21 @@\n \n             if \"__pycache__\" in str(fichier) or \".git\" in str(fichier):\n                 continue\n \n             try:\n-                with open(fichier, 'r', encoding='utf-8') as f:\n+                with open(fichier, \"r\", encoding=\"utf-8\") as f:\n                     contenu = f.read()\n \n                 # Analyse AST pour d\u00e9tecter les erreurs\n                 try:\n                     ast.parse(contenu)\n                 except SyntaxError as e:\n-                    correction = self._corriger_erreur_syntaxe(\n-                        fichier, contenu, e)\n+                    correction = self._corriger_erreur_syntaxe(fichier, contenu, e)\n                     if correction and not dry_run:\n-                        with open(fichier, 'w', encoding='utf-8') as f:\n-                            f.write(correction['nouveau_contenu'])\n+                        with open(fichier, \"w\", encoding=\"utf-8\") as f:\n+                            f.write(correction[\"nouveau_contenu\"])\n                         corrections.append(correction)\n                         erreurs_corrigees += 1\n                     elif correction:\n                         corrections.append(correction)\n                         erreurs_corrigees += 1\n@@ -98,25 +97,26 @@\n                 logger.warning(f\"Erreur lors de l'analyse de {fichier}: {e}\")\n \n         return {\n             \"corrections_appliquees\": corrections,\n             \"fichiers_traites\": fichiers_traites,\n-            \"erreurs_corrigees\": erreurs_corrigees\n+            \"erreurs_corrigees\": erreurs_corrigees,\n         }\n \n     def _corriger_erreur_syntaxe(\n-            self, fichier: Path, contenu: str, erreur: SyntaxError) -> Dict[str, Any]:\n+        self, fichier: Path, contenu: str, erreur: SyntaxError\n+    ) -> Dict[str, Any]:\n         \"\"\"Correction intelligente d'erreur de syntaxe\"\"\"\n-        lignes = contenu.split('\\n')\n+        lignes = contenu.split(\"\\n\")\n         ligne_erreur = (erreur.lineno or 1) - 1\n \n         # Corrections communes\n         corrections = {\n             \"indentation\": self._corriger_indentation,\n             \"parentheses\": self._corriger_parentheses,\n             \"guillemets\": self._corriger_guillemets,\n-            \"virgules\": self._corriger_virgules\n+            \"virgules\": self._corriger_virgules,\n         }\n \n         for type_correction, fonction in corrections.items():\n             try:\n                 nouveau_contenu = fonction(lignes, ligne_erreur)\n@@ -126,58 +126,51 @@\n                         \"type\": f\"syntaxe_{type_correction}\",\n                         \"ligne\": ligne_erreur + 1,\n                         \"ancien_contenu\": lignes[ligne_erreur],\n                         \"nouveau_contenu\": nouveau_contenu,\n                         \"erreur_originale\": lignes[ligne_erreur],\n-                        \"description\": f\"Correction {type_correction} automatique\"}\n+                        \"description\": f\"Correction {type_correction} automatique\",\n+                    }\n             except Exception:\n                 continue\n \n         return {}  # type: ignore\n \n-    def _corriger_indentation(\n-            self,\n-            lignes: List[str],\n-            ligne_erreur: int) -> str:\n+    def _corriger_indentation(self, lignes: List[str], ligne_erreur: int) -> str:\n         \"\"\"Correction automatique de l'indentation\"\"\"\n         ligne = lignes[ligne_erreur]\n \n         # D\u00e9tection de l'indentation incorrecte\n-        if ligne.strip() and not ligne.startswith(' ') and not ligne.startswith('\\t'):\n+        if ligne.strip() and not ligne.startswith(\" \") and not ligne.startswith(\"\\t\"):\n             # Ajout d'indentation bas\u00e9e sur le contexte\n             if ligne_erreur > 0:\n                 ligne_precedente = lignes[ligne_erreur - 1]\n-                if ligne_precedente.strip().endswith(':'):\n-                    return '    ' + ligne\n-                elif (ligne_precedente.strip().startswith('def ')\n-                      or ligne_precedente.strip().startswith('class ')):\n-                    return '    ' + ligne\n+                if ligne_precedente.strip().endswith(\":\"):\n+                    return \"    \" + ligne\n+                elif ligne_precedente.strip().startswith(\n+                    \"def \"\n+                ) or ligne_precedente.strip().startswith(\"class \"):\n+                    return \"    \" + ligne\n \n         return \"\"  # type: ignore\n \n-    def _corriger_parentheses(\n-            self,\n-            lignes: List[str],\n-            ligne_erreur: int) -> str:\n+    def _corriger_parentheses(self, lignes: List[str], ligne_erreur: int) -> str:\n         \"\"\"Correction automatique des parenth\u00e8ses\"\"\"\n         ligne = lignes[ligne_erreur]\n \n         # Comptage des parenth\u00e8ses\n-        ouvrantes = ligne.count('(') + ligne.count('[') + ligne.count('{')\n-        fermantes = ligne.count(')') + ligne.count(']') + ligne.count('}')\n+        ouvrantes = ligne.count(\"(\") + ligne.count(\"[\") + ligne.count(\"{\")\n+        fermantes = ligne.count(\")\") + ligne.count(\"]\") + ligne.count(\"}\")\n \n         if ouvrantes > fermantes:\n-            return ligne + ')' * (ouvrantes - fermantes)\n+            return ligne + \")\" * (ouvrantes - fermantes)\n         elif fermantes > ouvrantes:\n-            return '(' * (fermantes - ouvrantes) + ligne\n+            return \"(\" * (fermantes - ouvrantes) + ligne\n \n         return \"\"  # type: ignore\n \n-    def _corriger_guillemets(\n-            self,\n-            lignes: List[str],\n-            ligne_erreur: int) -> str:\n+    def _corriger_guillemets(self, lignes: List[str], ligne_erreur: int) -> str:\n         \"\"\"Correction automatique des guillemets\"\"\"\n         ligne = lignes[ligne_erreur]\n \n         # Correction des guillemets non ferm\u00e9s\n         if ligne.count('\"') % 2 == 1:\n@@ -190,16 +183,17 @@\n     def _corriger_virgules(self, lignes: List[str], ligne_erreur: int) -> str:\n         \"\"\"Correction automatique des virgules manquantes\"\"\"\n         ligne = lignes[ligne_erreur]\n \n         # Ajout de virgule manquante dans les listes/dicts\n-        if re.search(r'[a-zA-Z0-9_]+$', ligne.strip()):\n+        if re.search(r\"[a-zA-Z0-9_]+$\", ligne.strip()):\n             if ligne_erreur + 1 < len(lignes):\n                 ligne_suivante = lignes[ligne_erreur + 1]\n                 if ligne_suivante.strip().startswith(\n-                        '[') or ligne_suivante.strip().startswith('{'):\n-                    return ligne.rstrip() + ','\n+                    \"[\"\n+                ) or ligne_suivante.strip().startswith(\"{\"):\n+                    return ligne.rstrip() + \",\"\n \n         return \"\"  # type: ignore\n \n     def _optimiser_code(self, dry_run: bool) -> Dict[str, Any]:\n         \"\"\"Optimisation automatique du code\"\"\"\n@@ -214,85 +208,84 @@\n \n             if \"__pycache__\" in str(fichier) or \".git\" in str(fichier):\n                 continue\n \n             try:\n-                with open(fichier, 'r', encoding='utf-8') as f:\n+                with open(fichier, \"r\", encoding=\"utf-8\") as f:\n                     contenu = f.read()\n \n                 nouveau_contenu = contenu\n \n                 # Optimisations\n                 optimisations_fichier = []\n \n                 # 1. Remplacement de list comprehensions par g\u00e9n\u00e9rateurs\n                 nouveau_contenu, optims = self._optimiser_list_comprehensions(\n-                    nouveau_contenu)\n+                    nouveau_contenu\n+                )\n                 optimisations_fichier.extend(optims)\n \n                 # 2. Optimisation des imports\n-                nouveau_contenu, optims = self._optimiser_imports(\n-                    nouveau_contenu)\n+                nouveau_contenu, optims = self._optimiser_imports(nouveau_contenu)\n                 optimisations_fichier.extend(optims)\n \n                 # 3. Optimisation des boucles\n-                nouveau_contenu, optims = self._optimiser_boucles(\n-                    nouveau_contenu)\n+                nouveau_contenu, optims = self._optimiser_boucles(nouveau_contenu)\n                 optimisations_fichier.extend(optims)\n \n                 if optimisations_fichier and not dry_run:\n-                    with open(fichier, 'w', encoding='utf-8') as f:\n+                    with open(fichier, \"w\", encoding=\"utf-8\") as f:\n                         f.write(nouveau_contenu)\n \n                 optimisations.extend(optimisations_fichier)\n                 if optimisations_fichier:\n                     erreurs_corrigees += len(optimisations_fichier)\n                 fichiers_traites += 1\n \n             except Exception as e:\n-                logger.warning(\n-                    f\"Erreur lors de l'optimisation de {fichier}: {e}\")\n+                logger.warning(f\"Erreur lors de l'optimisation de {fichier}: {e}\")\n \n         return {\n             \"corrections_appliquees\": optimisations,\n             \"fichiers_traites\": fichiers_traites,\n-            \"erreurs_corrigees\": erreurs_corrigees\n+            \"erreurs_corrigees\": erreurs_corrigees,\n         }\n \n-    def _optimiser_list_comprehensions(\n-            self, contenu: str) -> Tuple[str, List[Dict]]:\n+    def _optimiser_list_comprehensions(self, contenu: str) -> Tuple[str, List[Dict]]:\n         \"\"\"Optimisation des list comprehensions\"\"\"\n         optimisations = []\n \n         # Remplacer [x for x in y] par (x for x in y) quand possible\n-        pattern = r'\\[([^\\[\\]]+for[^\\[\\]]+)\\]'\n+        pattern = r\"\\[([^\\[\\]]+for[^\\[\\]]+)\\]\"\n         matches = re.finditer(pattern, contenu)\n \n         for match in matches:\n             expression = match.group(1)\n-            if 'if' not in expression:  # Pas de condition\n+            if \"if\" not in expression:  # Pas de condition\n                 nouveau = f\"({expression})\"\n                 contenu = contenu.replace(match.group(0), nouveau)\n-                optimisations.append({\n-                    \"type\": \"optimisation\",\n-                    \"ancien\": match.group(0),\n-                    \"nouveau\": nouveau\n-                })\n+                optimisations.append(\n+                    {\n+                        \"type\": \"optimisation\",\n+                        \"ancien\": match.group(0),\n+                        \"nouveau\": nouveau,\n+                    }\n+                )\n \n         return contenu, optimisations\n \n     def _optimiser_imports(self, contenu: str) -> Tuple[str, List[Dict]]:\n         \"\"\"Optimisation des imports\"\"\"\n         optimisations = []\n \n         # Regrouper les imports\n-        lines = contenu.split('\\n')\n+        lines = contenu.split(\"\\n\")\n         import_lines = []\n         other_lines = []\n \n         for line in lines:\n-            if line.strip().startswith(('import ', 'from ')):\n+            if line.strip().startswith((\"import \", \"from \")):\n                 import_lines.append(line)\n             else:\n                 other_lines.append(line)\n \n         if len(import_lines) > 1:\n@@ -300,53 +293,48 @@\n             stdlib_imports = []\n             third_party_imports = []\n             local_imports = []\n \n             for imp in import_lines:\n-                if any(\n-                    pkg in imp for pkg in [\n-                        'os',\n-                        'sys',\n-                        'json',\n-                        're',\n-                        'pathlib']):\n+                if any(pkg in imp for pkg in [\"os\", \"sys\", \"json\", \"re\", \"pathlib\"]):\n                     stdlib_imports.append(imp)\n-                elif any(pkg in imp for pkg in ['numpy', 'pandas', 'requests']):\n+                elif any(pkg in imp for pkg in [\"numpy\", \"pandas\", \"requests\"]):\n                     third_party_imports.append(imp)\n                 else:\n                     local_imports.append(imp)\n \n             # R\u00e9organiser\n-            new_imports = stdlib_imports + \\\n-                [''] + third_party_imports + [''] + local_imports\n-            optimisations.append({\n-                \"type\": \"optimisation\",\n-                \"description\": \"Imports regroup\u00e9s par cat\u00e9gorie\"\n-            })\n-\n-            return '\\n'.join(new_imports + other_lines), optimisations\n+            new_imports = (\n+                stdlib_imports + [\"\"] + third_party_imports + [\"\"] + local_imports\n+            )\n+            optimisations.append(\n+                {\n+                    \"type\": \"optimisation\",\n+                    \"description\": \"Imports regroup\u00e9s par cat\u00e9gorie\",\n+                }\n+            )\n+\n+            return \"\\n\".join(new_imports + other_lines), optimisations\n \n         return contenu, optimisations\n \n     def _optimiser_boucles(self, contenu: str) -> Tuple[str, List[Dict]]:\n         \"\"\"Optimisation des boucles\"\"\"\n         optimisations = []\n \n         # Remplacer for i in range(len(x)) par for i, item in enumerate(x)\n-        pattern = r'for\\s+(\\w+)\\s+in\\s+range\\(len\\(([^)]+)\\)\\):'\n+        pattern = r\"for\\s+(\\w+)\\s+in\\s+range\\(len\\(([^)]+)\\)\\):\"\n         matches = re.finditer(pattern, contenu)\n \n         for match in matches:\n             index_var = match.group(1)\n             list_var = match.group(2)\n             nouveau = f\"for {index_var}, item in enumerate({list_var}):\"\n             contenu = contenu.replace(match.group(0), nouveau)\n-            optimisations.append({\n-                \"type\": \"optimisation\",\n-                \"ancien\": match.group(0),\n-                \"nouveau\": nouveau\n-            })\n+            optimisations.append(\n+                {\"type\": \"optimisation\", \"ancien\": match.group(0), \"nouveau\": nouveau}\n+            )\n \n         return contenu, optimisations\n \n     def _refactoring_automatique(self, dry_run: bool) -> Dict[str, Any]:\n         \"\"\"Refactoring automatique du code\"\"\"\n@@ -361,35 +349,32 @@\n \n             if \"__pycache__\" in str(fichier) or \".git\" in str(fichier):\n                 continue\n \n             try:\n-                with open(fichier, 'r', encoding='utf-8') as f:\n+                with open(fichier, \"r\", encoding=\"utf-8\") as f:\n                     contenu = f.read()\n \n                 nouveau_contenu = contenu\n \n                 # Refactorings\n                 refactorings_fichier = []\n \n                 # 1. Extraction de m\u00e9thodes\n-                nouveau_contenu, refs = self._extraire_methodes(\n-                    nouveau_contenu)\n+                nouveau_contenu, refs = self._extraire_methodes(nouveau_contenu)\n                 refactorings_fichier.extend(refs)\n \n                 # 2. Renommage de variables\n-                nouveau_contenu, refs = self._renommer_variables(\n-                    nouveau_contenu)\n+                nouveau_contenu, refs = self._renommer_variables(nouveau_contenu)\n                 refactorings_fichier.extend(refs)\n \n                 # 3. Simplification de conditions\n-                nouveau_contenu, refs = self._simplifier_conditions(\n-                    nouveau_contenu)\n+                nouveau_contenu, refs = self._simplifier_conditions(nouveau_contenu)\n                 refactorings_fichier.extend(refs)\n \n                 if refactorings_fichier and not dry_run:\n-                    with open(fichier, 'w', encoding='utf-8') as f:\n+                    with open(fichier, \"w\", encoding=\"utf-8\") as f:\n                         f.write(nouveau_contenu)\n \n                 refactorings.extend(refactorings_fichier)\n                 if refactorings_fichier:\n                     erreurs_corrigees += len(refactorings_fichier)\n@@ -399,42 +384,46 @@\n                 logger.warning(f\"Erreur lors du refactoring de {fichier}: {e}\")\n \n         return {\n             \"corrections_appliquees\": refactorings,\n             \"fichiers_traites\": fichiers_traites,\n-            \"erreurs_corrigees\": erreurs_corrigees\n+            \"erreurs_corrigees\": erreurs_corrigees,\n         }\n \n     def _extraire_methodes(self, contenu: str) -> Tuple[str, List[Dict]]:\n         \"\"\"Extraction automatique de m\u00e9thodes\"\"\"\n         refactorings = []\n \n         # D\u00e9tecter les blocs de code r\u00e9p\u00e9titifs\n-        lines = contenu.split('\\n')\n+        lines = contenu.split(\"\\n\")\n         i = 0\n         while i < len(lines):\n             line = lines[i].strip()\n \n             # D\u00e9tecter les blocs de plus de 5 lignes\n-            if line.startswith('def ') or line.startswith('class '):\n+            if line.startswith(\"def \") or line.startswith(\"class \"):\n                 # Compter les lignes du bloc\n                 j = i + 1\n                 indent_level = len(lines[i]) - len(lines[i].lstrip())\n \n                 while j < len(lines):\n-                    if lines[j].strip() and len(lines[j]) - \\\n-                            len(lines[j].lstrip()) <= indent_level:\n+                    if (\n+                        lines[j].strip()\n+                        and len(lines[j]) - len(lines[j].lstrip()) <= indent_level\n+                    ):\n                         break\n                     j += 1\n \n                 block_size = j - i\n                 if block_size > 10:  # Bloc trop long\n-                    refactorings.append({\n-                        \"type\": \"refactoring\",\n-                        \"description\": f\"M\u00e9thode de {block_size} lignes d\u00e9tect\u00e9e\",\n-                        \"ligne\": i + 1\n-                    })\n+                    refactorings.append(\n+                        {\n+                            \"type\": \"refactoring\",\n+                            \"description\": f\"M\u00e9thode de {block_size} lignes d\u00e9tect\u00e9e\",\n+                            \"ligne\": i + 1,\n+                        }\n+                    )\n \n                 i = j\n             else:\n                 i += 1\n \n@@ -444,45 +433,45 @@\n         \"\"\"Renommage automatique de variables\"\"\"\n         refactorings = []\n \n         # D\u00e9tecter les variables avec des noms non descriptifs\n         patterns = [\n-            (r'\\b([a-z])\\b', 'Variable \u00e0 une lettre'),\n-            (r'\\b(x|y|z)\\b', 'Variable math\u00e9matique'),\n-            (r'\\b(temp|tmp)\\b', 'Variable temporaire')\n+            (r\"\\b([a-z])\\b\", \"Variable \u00e0 une lettre\"),\n+            (r\"\\b(x|y|z)\\b\", \"Variable math\u00e9matique\"),\n+            (r\"\\b(temp|tmp)\\b\", \"Variable temporaire\"),\n         ]\n \n         for pattern, description in patterns:\n             matches = re.finditer(pattern, contenu)\n             for match in matches:\n                 var_name = match.group(1)\n-                refactorings.append({\n-                    \"type\": \"refactoring\",\n-                    \"variable\": var_name,\n-                    \"suggestion\": f\"renommer {var_name} en nom plus descriptif\",\n-                    \"description\": description\n-                })\n+                refactorings.append(\n+                    {\n+                        \"type\": \"refactoring\",\n+                        \"variable\": var_name,\n+                        \"suggestion\": f\"renommer {var_name} en nom plus descriptif\",\n+                        \"description\": description,\n+                    }\n+                )\n \n         return contenu, refactorings\n \n     def _simplifier_conditions(self, contenu: str) -> Tuple[str, List[Dict]]:\n         \"\"\"Simplification automatique de conditions\"\"\"\n         refactorings = []\n \n         # Simplifier if x == True par if x\n-        pattern = r'if\\s+([^:]+)\\s*==\\s*True\\s*:'\n+        pattern = r\"if\\s+([^:]+)\\s*==\\s*True\\s*:\"\n         matches = re.finditer(pattern, contenu)\n \n         for match in matches:\n             condition = match.group(1).strip()\n             nouveau = f\"if {condition}:\"\n             contenu = contenu.replace(match.group(0), nouveau)\n-            refactorings.append({\n-                \"type\": \"refactoring\",\n-                \"ancien\": match.group(0),\n-                \"nouveau\": nouveau\n-            })\n+            refactorings.append(\n+                {\"type\": \"refactoring\", \"ancien\": match.group(0), \"nouveau\": nouveau}\n+            )\n \n         return contenu, refactorings\n \n     def _corriger_anti_patterns(self, dry_run: bool) -> Dict[str, Any]:\n         \"\"\"Correction des anti-patterns\"\"\"\n@@ -497,44 +486,47 @@\n \n             if \"__pycache__\" in str(fichier) or \".git\" in str(fichier):\n                 continue\n \n             try:\n-                with open(fichier, 'r', encoding='utf-8') as f:\n+                with open(fichier, \"r\", encoding=\"utf-8\") as f:\n                     contenu = f.read()\n \n                 corrections_fichier = []\n \n                 # Anti-patterns \u00e0 corriger\n                 anti_patterns = [\n-                    (r'except\\s*:', 'except Exception:', 'Exception trop g\u00e9n\u00e9rique'),\n-                    (r'print\\s*\\(', 'logging.info(', 'Utilisation de print()'),\n-                    (r'os\\.system\\s*\\(', 'subprocess.run(', 'Appel shell non s\u00e9curis\u00e9'),\n-                    (r'import \\*', 'import sp\u00e9cifique', 'Import wildcard')\n+                    (r\"except\\s*:\", \"except Exception:\", \"Exception trop g\u00e9n\u00e9rique\"),\n+                    (r\"print\\s*\\(\", \"logging.info(\", \"Utilisation de print()\"),\n+                    (r\"os\\.system\\s*\\(\", \"subprocess.run(\", \"Appel shell non s\u00e9curis\u00e9\"),\n+                    (r\"import \\*\", \"import sp\u00e9cifique\", \"Import wildcard\"),\n                 ]\n \n                 for pattern, replacement, description in anti_patterns:\n                     if re.search(pattern, contenu):\n-                        corrections_fichier.append({\n-                            \"type\": \"anti_pattern\",\n-                            \"pattern\": pattern,\n-                            \"description\": description\n-                        })\n+                        corrections_fichier.append(\n+                            {\n+                                \"type\": \"anti_pattern\",\n+                                \"pattern\": pattern,\n+                                \"description\": description,\n+                            }\n+                        )\n \n                 corrections.extend(corrections_fichier)\n                 if corrections_fichier:\n                     erreurs_corrigees += len(corrections_fichier)\n                 fichiers_traites += 1\n \n             except Exception as e:\n                 logger.warning(\n-                    f\"Erreur lors de la correction d'anti-patterns de {fichier}: {e}\")\n+                    f\"Erreur lors de la correction d'anti-patterns de {fichier}: {e}\"\n+                )\n \n         return {\n             \"corrections_appliquees\": corrections,\n             \"fichiers_traites\": fichiers_traites,\n-            \"erreurs_corrigees\": erreurs_corrigees\n+            \"erreurs_corrigees\": erreurs_corrigees,\n         }\n \n     def _ameliorer_lisibilite(self, dry_run: bool) -> Dict[str, Any]:\n         \"\"\"Am\u00e9lioration de la lisibilit\u00e9\"\"\"\n         ameliorations = []\n@@ -548,53 +540,56 @@\n \n             if \"__pycache__\" in str(fichier) or \".git\" in str(fichier):\n                 continue\n \n             try:\n-                with open(fichier, 'r', encoding='utf-8') as f:\n+                with open(fichier, \"r\", encoding=\"utf-8\") as f:\n                     contenu = f.read()\n \n                 ameliorations_fichier = []\n \n                 # Am\u00e9liorations de lisibilit\u00e9\n-                lines = contenu.split('\\n')\n+                lines = contenu.split(\"\\n\")\n \n                 # 1. Ajouter des docstrings manquantes\n                 for i, line in enumerate(lines):\n-                    if line.strip().startswith('def ') and i + 1 < len(lines):\n+                    if line.strip().startswith(\"def \") and i + 1 < len(lines):\n                         next_line = lines[i + 1].strip()\n-                        if not next_line.startswith(\n-                                '\"\"\"') and not next_line.startswith(\"'''\"):\n-                            ameliorations_fichier.append({\n-                                \"type\": \"lisibilite\",\n-                                \"ligne\": i + 1,\n-                                \"description\": \"Fonction sans docstring\"\n-                            })\n+                        if not next_line.startswith('\"\"\"') and not next_line.startswith(\n+                            \"'''\"\n+                        ):\n+                            ameliorations_fichier.append(\n+                                {\n+                                    \"type\": \"lisibilite\",\n+                                    \"ligne\": i + 1,\n+                                    \"description\": \"Fonction sans docstring\",\n+                                }\n+                            )\n \n                 # 2. D\u00e9tecter les lignes trop longues\n                 for i, line in enumerate(lines):\n                     if len(line) > 120:\n-                        ameliorations_fichier.append({\n-                            \"type\": \"lisibilite\",\n-                            \"ligne\": i + 1,\n-                            \"description\": f\"Ligne de {len(line)} caract\u00e8res\"\n-                        })\n+                        ameliorations_fichier.append(\n+                            {\n+                                \"type\": \"lisibilite\",\n+                                \"ligne\": i + 1,\n+                                \"description\": f\"Ligne de {len(line)} caract\u00e8res\",\n+                            }\n+                        )\n \n                 ameliorations.extend(ameliorations_fichier)\n                 if ameliorations_fichier:\n                     erreurs_corrigees += len(ameliorations_fichier)\n                 fichiers_traites += 1\n \n             except Exception as e:\n-                logger.warning(\n-                    f\"Erreur lors de l'am\u00e9lioration de {fichier}: \"\n-                    f\"{e}\")\n+                logger.warning(f\"Erreur lors de l'am\u00e9lioration de {fichier}: \" f\"{e}\")\n \n         return {\n             \"corrections_appliquees\": ameliorations,\n             \"fichiers_traites\": fichiers_traites,\n-            \"erreurs_corrigees\": erreurs_corrigees\n+            \"erreurs_corrigees\": erreurs_corrigees,\n         }\n \n     def generer_rapport(self, resultats: Dict[str, Any]) -> str:\n         \"\"\"G\u00e9n\u00e9ration d'un rapport d\u00e9taill\u00e9\"\"\"\n         rapport = \"\"\"\n@@ -603,14 +598,14 @@\n \"\"\"\n         rapport += f\"\\n\ud83d\udcca RAPPORT D'AUTO-CORRECTION - {self.project_path}\\n\"\n         rapport += f\"{'='*60}\\n\\n\"\n \n         # Utiliser le dictionnaire 'resultats' pour g\u00e9n\u00e9rer le rapport\n-        corrections_appliquees = resultats.get('corrections_appliquees', [])\n-        suggestions = resultats.get('suggestions', [])\n-        fichiers_traites = resultats.get('fichiers_traites', 0)\n-        erreurs_corrigees = resultats.get('erreurs_corrigees', 0)\n+        corrections_appliquees = resultats.get(\"corrections_appliquees\", [])\n+        suggestions = resultats.get(\"suggestions\", [])\n+        fichiers_traites = resultats.get(\"fichiers_traites\", 0)\n+        erreurs_corrigees = resultats.get(\"erreurs_corrigees\", 0)\n \n         rapport += \"\ud83d\udd27 CORRECTIONS APPLIQU\u00c9ES:\\n\"\n         if corrections_appliquees:\n             for i, correction in enumerate(corrections_appliquees, 1):\n                 rapport += (\n@@ -637,17 +632,13 @@\n \n def main():\n     \"\"\"Fonction principale pour test\"\"\"\n     import argparse\n \n-    parser = argparse.ArgumentParser(\n-        description=\"Auto-correction avanc\u00e9e pour Athalia\")\n+    parser = argparse.ArgumentParser(description=\"Auto-correction avanc\u00e9e pour Athalia\")\n     parser.add_argument(\"project_path\", help=\"Chemin du projet \u00e0 corriger\")\n-    parser.add_argument(\n-        \"--dry-run\",\n-        action=\"store_true\",\n-        help=\"Mode simulation\")\n+    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Mode simulation\")\n \n     args = parser.parse_args()\n \n     corrector = AutoCorrectionAvancee(args.project_path)\n     resultats = corrector.analyser_et_corriger(dry_run=args.dry_run)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ci.py\t2025-07-29 17:56:25.420000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ci.py\t2025-07-29 18:12:20.867184+00:00\n@@ -9,35 +9,35 @@\n \"\"\"\n \n \n def generate_github_ci_yaml(outdir):\n     from pathlib import Path\n+\n     outdir = Path(str(outdir))\n-    ci_dir = outdir / '.f' / 'f'\n+    ci_dir = outdir / \".f\" / \"f\"\n     ci_dir.mkdir(parents=True, exist_ok=True)\n-    ci_file = ci_dir / 'ci.f(f'\n-    ci_file.write_text('# CI/CD config')\n-    print(\n-        f'[DEBUG CI] Fichier g\u00e9n\u00e9r\u00e9 : {ci_file} (exists: {ci_file.exists()})')\n-    readme_path = os.path.join(outdir, 'README.md')\n+    ci_file = ci_dir / \"ci.f(f\"\n+    ci_file.write_text(\"# CI/CD config\")\n+    print(f\"[DEBUG CI] Fichier g\u00e9n\u00e9r\u00e9 : {ci_file} (exists: {ci_file.exists()})\")\n+    readme_path = os.path.join(outdir, \"README.md\")\n     badge = \"![CI](https://github.com /< user>/<repo >/ actions/workflows / ci.yaml / badge.svg)\\n\"\n     if os.path.exists(readme_path):\n-        with open(readme_path, 'r+') as file_handle:\n+        with open(readme_path, \"r+\") as file_handle:\n             content = file_handle.read()\n-            if '![CI]' not in content:\n+            if \"![CI]\" not in content:\n                 file_handle.seek(0, 0)\n                 file_handle.write(badge + content)\n     logging.info(f\"Workflow CI g\u00e9n\u00e9r\u00e9 dans {ci_file}\")\n \n \n def add_coverage_badge(outdir):\n     for fname in os.listdir(outdir):\n-        if fname.startswith('README'):\n+        if fname.startswith(\"README\"):\n             readme_path = os.path.join(outdir, fname)\n-            badge = '![Coverage](https://img.shields.io / badge/coverage - 95%25-brightgreen)\\n'\n+            badge = \"![Coverage](https://img.shields.io / badge/coverage - 95%25-brightgreen)\\n\"\n             if os.path.exists(readme_path):\n-                with open(readme_path, 'r+') as file_handle:\n+                with open(readme_path, \"r+\") as file_handle:\n                     content = file_handle.read()\n-                    if '![Coverage]' not in content:\n+                    if \"![Coverage]\" not in content:\n                         file_handle.seek(0, 0)\n                         file_handle.write(badge + content)\n             logging.info(f\"Badge coverage ajout\u00e9 dans {readme_path}\")\n--- /Volumes/T7/athalia-dev-setup/athalia_core/cleanup.py\t2025-07-29 17:56:25.630000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/cleanup.py\t2025-07-29 18:12:20.901648+00:00\n@@ -16,13 +16,13 @@\n     outdir = Path(outdir)\n     deleted_files = []\n     # Renommer r\u00e9cursivement dans tous les sous-dossiers\n     for dirpath, dirs, files in os.walk(outdir):\n         for file_handle in files:\n-            if file_handle == 'test_booster_ia_proj.ff':\n+            if file_handle == \"test_booster_ia_proj.ff\":\n                 file_path = os.path.join(dirpath, file_handle)\n-                new_name = os.path.join(dirpath, 'test_booster_ia_proj.pyff')\n+                new_name = os.path.join(dirpath, \"test_booster_ia_proj.pyff\")\n                 if os.path.exists(new_name):\n                     os.remove(new_name)\n                 os.rename(file_path, new_name)\n                 logging.info(f\"Renommage {file_path} -> {new_name}\")\n                 deleted_files.append(file_path)\n@@ -30,31 +30,37 @@\n     # test_booster_ia_proj.pyff)\n     for dirpath, dirs, files in os.walk(outdir):\n         for file_handle in files:\n             file_path = os.path.join(dirpath, file_handle)\n             # On ne supprime jamais test_booster_ia_proj.pyff\n-            if ((file_handle.endswith('.ff') or file_handle.endswith('.pyff')\n-                 or file_handle.startswith('test_') or file_handle.endswith('.pyc')\n-                 or file_handle.endswith('.log') or file_handle.endswith('.bak'))\n-                    and not file_handle.startswith('test_booster_ia_proj')\n-                    and file_handle != 'test_booster_ia_proj.pyff'):\n+            if (\n+                (\n+                    file_handle.endswith(\".ff\")\n+                    or file_handle.endswith(\".pyff\")\n+                    or file_handle.startswith(\"test_\")\n+                    or file_handle.endswith(\".pyc\")\n+                    or file_handle.endswith(\".log\")\n+                    or file_handle.endswith(\".bak\")\n+                )\n+                and not file_handle.startswith(\"test_booster_ia_proj\")\n+                and file_handle != \"test_booster_ia_proj.pyff\"\n+            ):\n                 try:\n                     os.remove(file_path)\n-                    logging.info(\n-                        f\"Suppression fichier de test/caches : {file_path}\")\n+                    logging.info(f\"Suppression fichier de test/caches : {file_path}\")\n                     deleted_files.append(file_path)\n                 except Exception as e:\n                     logging.warning(f\"Erreur suppression {file_path} : {e}\")\n-            elif file_handle == 'test_booster_ia_proj.pyff':\n+            elif file_handle == \"test_booster_ia_proj.pyff\":\n                 logging.info(f\"Fichier prot\u00e9g\u00e9 non supprim\u00e9 : {file_path}\")\n     # Suppression des dossiers __pycache__ et de tous les sous-dossiers, mais\n     # jamais le dossier racine\n     for dirpath, dirs, files in os.walk(outdir):\n         for dict_data in list(dirs):\n             dir_path = os.path.join(dirpath, dict_data)\n             # Ne jamais supprimer le dossier racine pass\u00e9 \u00e0 la fonction\n-            if dict_data in ['__pycache__', 'f'] and Path(dir_path) != outdir:\n+            if dict_data in [\"__pycache__\", \"f\"] and Path(dir_path) != outdir:\n                 try:\n                     shutil.rmtree(dir_path, ignore_errors=True)\n                     logging.info(f\"Suppression dossier cache : {dir_path}\")\n                     deleted_files.append(dir_path)\n                 except Exception as e:\n@@ -70,37 +76,37 @@\n     \"\"\"\n     cleaned_files = []\n \n     # Patterns de fichiers macOS \u00e0 supprimer\n     macos_patterns = [\n-        '._*',  # AppleDouble files\n-        '.DS_Store',  # Desktop Services Store\n-        'Thumbs.db',  # Windows thumbnail cache\n-        '.!*',\n+        \"._*\",  # AppleDouble files\n+        \".DS_Store\",  # Desktop Services Store\n+        \"Thumbs.db\",  # Windows thumbnail cache\n+        \".!*\",\n         # Fichiers temporaires macOS sp\u00e9cifiques (comme .!44956!*.clean)\n-        '*.tmp',  # Fichiers temporaires\n-        '*.bak',  # Fichiers de sauvegarde\n-        '*.log',  # Fichiers de log\n-        '*.clean',  # Fichiers de nettoyage temporaires\n-        '*.apdisk',  # Apple Partition Map\n-        '.Spotlight-V100',  # Spotlight index\n-        '.Trashes',  # Corbeille\n-        '.fseventsd',  # File System Events\n-        '.TemporaryItems',  # Items temporaires\n-        '._.DS_Store',  # AppleDouble DS_Store\n-        '.AppleDouble',  # Dossier AppleDouble\n-        '.LSOverride',  # Launch Services Override\n+        \"*.tmp\",  # Fichiers temporaires\n+        \"*.bak\",  # Fichiers de sauvegarde\n+        \"*.log\",  # Fichiers de log\n+        \"*.clean\",  # Fichiers de nettoyage temporaires\n+        \"*.apdisk\",  # Apple Partition Map\n+        \".Spotlight-V100\",  # Spotlight index\n+        \".Trashes\",  # Corbeille\n+        \".fseventsd\",  # File System Events\n+        \".TemporaryItems\",  # Items temporaires\n+        \"._.DS_Store\",  # AppleDouble DS_Store\n+        \".AppleDouble\",  # Dossier AppleDouble\n+        \".LSOverride\",  # Launch Services Override\n     ]\n \n     # Dossiers macOS \u00e0 supprimer\n     macos_dirs = [\n-        '.Spotlight-V100',\n-        '.Trashes',\n-        '.fseventsd',\n-        '.TemporaryItems',\n-        '.AppleDouble',\n-        '.LSOverride'\n+        \".Spotlight-V100\",\n+        \".Trashes\",\n+        \".fseventsd\",\n+        \".TemporaryItems\",\n+        \".AppleDouble\",\n+        \".LSOverride\",\n     ]\n \n     for root, dirs, files in os.walk(directory):\n         # Supprimer les dossiers macOS\n         for dir_name in list(dirs):  # Copie de la liste pour modification\n@@ -110,11 +116,12 @@\n                     shutil.rmtree(dir_path, ignore_errors=True)\n                     cleaned_files.append(dir_path)\n                     logging.info(f\"Suppression dossier macOS: {dir_path}\")\n                 except Exception as e:\n                     logging.warning(\n-                        f\"Impossible de supprimer le dossier {dir_path}: {e}\")\n+                        f\"Impossible de supprimer le dossier {dir_path}: {e}\"\n+                    )\n \n         # Supprimer les fichiers macOS\n         for file in files:\n             should_delete = False\n \n@@ -123,29 +130,29 @@\n                 if fnmatch.fnmatch(file, pattern):\n                     should_delete = True\n                     break\n \n             # V\u00e9rifications sp\u00e9cifiques\n-            if (file.startswith('._')\n-                or file == '.DS_Store'\n-                or file == 'Thumbs.db'\n-                or file.startswith('.!')  # Fichiers comme .!44956!*.clean\n-                or file.endswith('.tmp')\n-                or file.endswith('.bak')\n-                or file.endswith('.log')\n-                or file.endswith('.clean')\n-                    or file.endswith('.apdisk')):\n+            if (\n+                file.startswith(\"._\")\n+                or file == \".DS_Store\"\n+                or file == \"Thumbs.db\"\n+                or file.startswith(\".!\")  # Fichiers comme .!44956!*.clean\n+                or file.endswith(\".tmp\")\n+                or file.endswith(\".bak\")\n+                or file.endswith(\".log\")\n+                or file.endswith(\".clean\")\n+                or file.endswith(\".apdisk\")\n+            ):\n                 should_delete = True\n \n             if should_delete:\n                 file_path = os.path.join(root, file)\n                 try:\n                     os.remove(file_path)\n                     cleaned_files.append(file_path)\n                     logging.info(f\"Suppression fichier macOS: {file_path}\")\n                 except Exception as e:\n-                    logging.warning(\n-                        f\"Impossible de supprimer {file_path}: {e}\")\n+                    logging.warning(f\"Impossible de supprimer {file_path}: {e}\")\n \n-    logging.info(\n-        f\"Nettoyage macOS termin\u00e9: {len(cleaned_files)} \u00e9l\u00e9ments supprim\u00e9s\")\n+    logging.info(f\"Nettoyage macOS termin\u00e9: {len(cleaned_files)} \u00e9l\u00e9ments supprim\u00e9s\")\n     return cleaned_files\n--- /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_types.py\t2025-07-29 17:56:26.850000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_types.py\t2025-07-29 18:12:20.907043+00:00\n@@ -8,93 +8,107 @@\n \"\"\"\n \n \n class ProjectType(Enum):\n     \"\"\"Types de projets support\u00e9s.\"\"\"\n-    ARTISTIC = \"artistic\"      # Projets artistiques (fleur qui danse, etc.)\n-    API = \"api\"               # APIs et services\n-    GAME = \"game\"             # Jeux et applications interactives\n-    DATA = \"data\"             # Projets data / ML\n-    WEB = \"web\"               # Applications web\n-    MOBILE = \"mobile\"         # Applications mobiles\n-    IOT = \"iot\"               # Internet des objets\n-    GENERIC = \"generic\"       # Projet g\u00e9n\u00e9rique (fallback)\n+\n+    ARTISTIC = \"artistic\"  # Projets artistiques (fleur qui danse, etc.)\n+    API = \"api\"  # APIs et services\n+    GAME = \"game\"  # Jeux et applications interactives\n+    DATA = \"data\"  # Projets data / ML\n+    WEB = \"web\"  # Applications web\n+    MOBILE = \"mobile\"  # Applications mobiles\n+    IOT = \"iot\"  # Internet des objets\n+    GENERIC = \"generic\"  # Projet g\u00e9n\u00e9rique (fallback)\n \n \n def get_project_config(project_type: ProjectType) -> Dict[str, Any]:\n     \"\"\"Retourne la configuration sp\u00e9cialis\u00e9e pour un type de projet.\"\"\"\n \n     configs = {\n         ProjectType.ARTISTIC: {\n             \"modules\": [\"core\", \"animation\", \"visual\", \"audio\"],\n             \"dependencies\": [\"numpy\", \"opencv-python\", \"matplotlib\", \"pygame\"],\n             \"structure\": [\"src/\", \"assets/\", \"animations/\", \"audio/\", \"tests/\"],\n-            \"prompts\": [\"artistic_animation.yaml\", \"visual_effects.yaml\", \"audio_sync.yaml\"],\n+            \"prompts\": [\n+                \"artistic_animation.yaml\",\n+                \"visual_effects.yaml\",\n+                \"audio_sync.yaml\",\n+            ],\n             \"booster_ia\": [\"artistic\"],\n-            \"description\": \"Projet artistique avec animations\"\n+            \"description\": \"Projet artistique avec animations\",\n         },\n-\n         ProjectType.API: {\n             \"modules\": [\"core\", \"api\", \"auth\", \"database\"],\n             \"dependencies\": [\"fastapi\", \"sqlalchemy\", \"pydantic\", \"python-jose\"],\n             \"structure\": [\"src/\", \"api/\", \"models/\", \"database/\", \"tests/\", \"docs/\"],\n             \"prompts\": [\"api_design.yaml\", \"security_audit.yaml\", \"performance.yaml\"],\n             \"booster_ia\": [\"api\"],\n-            \"description\": \"API REST avec authentification et s\u00e9curit\u00e9\"\n+            \"description\": \"API REST avec authentification et s\u00e9curit\u00e9\",\n         },\n-\n         ProjectType.GAME: {\n             \"modules\": [\"core\", \"game\", \"ui\", \"physics\"],\n             \"dependencies\": [\"pygame\", \"numpy\", \"pymunk\", \"pygame-gui\"],\n             \"structure\": [\"src/\", \"game/\", \"assets/\", \"levels/\", \"tests/\"],\n-            \"prompts\": [\"game_mechanics.yaml\", \"level_design.yaml\", \"ux_fun_boost.yaml\"],\n+            \"prompts\": [\n+                \"game_mechanics.yaml\",\n+                \"level_design.yaml\",\n+                \"ux_fun_boost.yaml\",\n+            ],\n             \"booster_ia\": [\"game\"],\n-            \"description\": \"Jeu interactif avec physique\"\n+            \"description\": \"Jeu interactif avec physique\",\n         },\n-\n         ProjectType.DATA: {\n             \"modules\": [\"core\", \"data\", \"ml\", \"viz\"],\n             \"dependencies\": [\"pandas\", \"scikit-learn\", \"matplotlib\", \"seaborn\"],\n             \"structure\": [\"src/\", \"data/\", \"models/\", \"notebooks/\", \"tests/\"],\n-            \"prompts\": [\"ml_pipeline.yaml\", \"data_analysis.yaml\", \"model_evaluation.yaml\"],\n+            \"prompts\": [\n+                \"ml_pipeline.yaml\",\n+                \"data_analysis.yaml\",\n+                \"model_evaluation.yaml\",\n+            ],\n             \"booster_ia\": [\"data\"],\n-            \"description\": \"Projet de data science et ML\"\n+            \"description\": \"Projet de data science et ML\",\n         },\n-\n         ProjectType.WEB: {\n             \"modules\": [\"core\", \"web\", \"ui\", \"database\"],\n             \"dependencies\": [\"flask\", \"jinja2\", \"sqlalchemy\", \"flask-cors\"],\n             \"structure\": [\"src/\", \"templates/\", \"static/\", \"database/\", \"tests/\"],\n-            \"prompts\": [\"web_design.yaml\", \"responsive_ui.yaml\", \"seo_optimization.yaml\"],\n+            \"prompts\": [\n+                \"web_design.yaml\",\n+                \"responsive_ui.yaml\",\n+                \"seo_optimization.yaml\",\n+            ],\n             \"booster_ia\": [\"web\"],\n-            \"description\": \"Application web responsive\"\n+            \"description\": \"Application web responsive\",\n         },\n-\n         ProjectType.MOBILE: {\n             \"modules\": [\"core\", \"ui\", \"api\", \"storage\"],\n             \"dependencies\": [\"kivy\", \"requests\", \"sqlite3\", \"plyer\"],\n             \"structure\": [\"src/\", \"ui/\", \"api/\", \"storage/\", \"tests/\"],\n             \"prompts\": [\"mobile_ui.yaml\", \"offline_sync.yaml\", \"performance.yaml\"],\n             \"booster_ia\": [\"mobile\"],\n-            \"description\": \"Application mobile cross-platform\"\n+            \"description\": \"Application mobile cross-platform\",\n         },\n-\n         ProjectType.IOT: {\n             \"modules\": [\"core\", \"sensors\", \"communication\", \"data\"],\n             \"dependencies\": [\"pyserial\", \"paho-mqtt\", \"numpy\", \"matplotlib\"],\n             \"structure\": [\"src/\", \"sensors/\", \"communication/\", \"data/\", \"tests/\"],\n-            \"prompts\": [\"iot_architecture.yaml\", \"sensor_integration.yaml\", \"data_analysis.yaml\"],\n+            \"prompts\": [\n+                \"iot_architecture.yaml\",\n+                \"sensor_integration.yaml\",\n+                \"data_analysis.yaml\",\n+            ],\n             \"booster_ia\": [\"iot\"],\n-            \"description\": \"Projet IoT avec capteurs\"\n+            \"description\": \"Projet IoT avec capteurs\",\n         },\n-\n         ProjectType.GENERIC: {\n             \"modules\": [\"core\", \"utils\", \"tests\"],\n             \"dependencies\": [\"requests\", \"pytest\", \"logging\"],\n             \"structure\": [\"src/\", \"tests/\", \"api/\", \"prompts/\"],\n             \"prompts\": [\"dev_debug.yaml\", \"ux_fun_boost.yaml\"],\n             \"booster_ia\": [\"generic\"],\n-            \"description\": \"Projet g\u00e9n\u00e9rique\"\n-        }\n+            \"description\": \"Projet g\u00e9n\u00e9rique\",\n+        },\n     }\n \n     return configs.get(project_type, configs[ProjectType.GENERIC])\n--- /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_classifier.py\t2025-07-29 17:56:26.750000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_classifier.py\t2025-07-29 18:12:20.913707+00:00\n@@ -22,57 +22,132 @@\n     \"\"\"\n     idea_lower = idea.lower()\n \n     # Ajout d\u00e9tection explicite pour todo-list\n     if any(\n-        kw in idea_lower for kw in [\n-            'todo',\n-            't\u00e2che',\n-            'taches',\n-            'task',\n-            'liste de t\u00e2ches',\n-            'liste de taches',\n-            'todo-list',\n-            'todo list']):\n+        kw in idea_lower\n+        for kw in [\n+            \"todo\",\n+            \"t\u00e2che\",\n+            \"taches\",\n+            \"task\",\n+            \"liste de t\u00e2ches\",\n+            \"liste de taches\",\n+            \"todo-list\",\n+            \"todo list\",\n+        ]\n+    ):\n         return ProjectType.API\n \n     # Mots-cl\u00e9s pour chaque type de projet\n     artistic_keywords = [\n-        'fleure', 'fleur', 'danse', 'dance', 'art', 'artistique', 'visuel',\n-        'animation', 'musique', 'couleur', 'peinture', 'dessin', 'sculpture',\n-        'cr\u00e9atif', 'creative', 'esth\u00e9tique', 'beaut\u00e9', 'harmonie'\n+        \"fleure\",\n+        \"fleur\",\n+        \"danse\",\n+        \"dance\",\n+        \"art\",\n+        \"artistique\",\n+        \"visuel\",\n+        \"animation\",\n+        \"musique\",\n+        \"couleur\",\n+        \"peinture\",\n+        \"dessin\",\n+        \"sculpture\",\n+        \"cr\u00e9atif\",\n+        \"creative\",\n+        \"esth\u00e9tique\",\n+        \"beaut\u00e9\",\n+        \"harmonie\",\n     ]\n \n     api_keywords = [\n-        'api', 'service', 'backend', 'rest', 'graphql', 'microservice',\n-        'endpoint', 'webservice', 'serveur', 'server', 'interface'\n+        \"api\",\n+        \"service\",\n+        \"backend\",\n+        \"rest\",\n+        \"graphql\",\n+        \"microservice\",\n+        \"endpoint\",\n+        \"webservice\",\n+        \"serveur\",\n+        \"server\",\n+        \"interface\",\n     ]\n \n     game_keywords = [\n-        'jeu', 'game', 'jouer', 'play', 'score', 'niveau', 'level',\n-        'gagner', 'win', 'perdre', 'lose', 'r\u00e8gles', 'rules', 'plateforme'\n+        \"jeu\",\n+        \"game\",\n+        \"jouer\",\n+        \"play\",\n+        \"score\",\n+        \"niveau\",\n+        \"level\",\n+        \"gagner\",\n+        \"win\",\n+        \"perdre\",\n+        \"lose\",\n+        \"r\u00e8gles\",\n+        \"rules\",\n+        \"plateforme\",\n     ]\n \n     data_keywords = [\n-        'data', 'donn\u00e9es', 'analyse', 'analysis', 'ml', 'machine learning',\n-        'ai', 'intelligence artificielle', 'statistiques', 'stats',\n-        'pr\u00e9diction', 'prediction', 'mod\u00e8le', 'model'\n+        \"data\",\n+        \"donn\u00e9es\",\n+        \"analyse\",\n+        \"analysis\",\n+        \"ml\",\n+        \"machine learning\",\n+        \"ai\",\n+        \"intelligence artificielle\",\n+        \"statistiques\",\n+        \"stats\",\n+        \"pr\u00e9diction\",\n+        \"prediction\",\n+        \"mod\u00e8le\",\n+        \"model\",\n     ]\n \n     web_keywords = [\n-        'web', 'site', 'application web', 'frontend', 'backend',\n-        'html', 'css', 'javascript', 'react', 'vue', 'angular'\n+        \"web\",\n+        \"site\",\n+        \"application web\",\n+        \"frontend\",\n+        \"backend\",\n+        \"html\",\n+        \"css\",\n+        \"javascript\",\n+        \"react\",\n+        \"vue\",\n+        \"angular\",\n     ]\n \n     mobile_keywords = [\n-        'mobile', 'app', 'application mobile', 'smartphone', 'tablet',\n-        'ios', 'android', 'touch', 'geste', 'swipe'\n+        \"mobile\",\n+        \"app\",\n+        \"application mobile\",\n+        \"smartphone\",\n+        \"tablet\",\n+        \"ios\",\n+        \"android\",\n+        \"touch\",\n+        \"geste\",\n+        \"swipe\",\n     ]\n \n     iot_keywords = [\n-        'iot', 'capteur', 'sensor', 'arduino', 'raspberry', 'pi',\n-        '\u00e9lectronique', 'electronic', 'hardware', 'mat\u00e9riel'\n+        \"iot\",\n+        \"capteur\",\n+        \"sensor\",\n+        \"arduino\",\n+        \"raspberry\",\n+        \"pi\",\n+        \"\u00e9lectronique\",\n+        \"electronic\",\n+        \"hardware\",\n+        \"mat\u00e9riel\",\n     ]\n \n     # Calcul des scores pour chaque type\n     scores = {\n         ProjectType.ARTISTIC: sum(1 for kw in artistic_keywords if kw in idea_lower),\n@@ -83,19 +158,17 @@\n         ProjectType.MOBILE: sum(1 for kw in mobile_keywords if kw in idea_lower),\n         ProjectType.IOT: sum(1 for kw in iot_keywords if kw in idea_lower),\n     }\n \n     # R\u00e8gles sp\u00e9ciales\n-    if 'fleure qui danse' in idea_lower or 'fleur qui danse' in idea_lower:\n+    if \"fleure qui danse\" in idea_lower or \"fleur qui danse\" in idea_lower:\n         return ProjectType.ARTISTIC\n \n-    if 'api' in idea_lower and any(\n-            kw in idea_lower for kw in [\n-            'service', 'backend']):\n+    if \"api\" in idea_lower and any(kw in idea_lower for kw in [\"service\", \"backend\"]):\n         return ProjectType.API\n \n-    if any(word in idea_lower for word in ['jeu', 'game', 'play']):\n+    if any(word in idea_lower for word in [\"jeu\", \"game\", \"play\"]):\n         return ProjectType.GAME\n \n     # Retourner le type avec le score le plus \u00e9lev\u00e9\n     max_score = max(scores.values())\n     if max_score > 0:\n@@ -117,35 +190,35 @@\n \n     Returns:\n         str: Nom de projet g\u00e9n\u00e9r\u00e9\n     \"\"\"\n     # Extraire des mots-cl\u00e9s de l'id\u00e9e\n-    words = re.findall(r'\\b\\w+\\b', idea.lower())\n+    words = re.findall(r\"\\b\\w+\\b\", idea.lower())\n \n     if project_type == ProjectType.ARTISTIC:\n-        if 'fleure' in idea.lower() or 'fleur' in idea.lower():\n-            return 'artistic_flower_dance'\n-        return 'artistic_project'\n+        if \"fleure\" in idea.lower() or \"fleur\" in idea.lower():\n+            return \"artistic_flower_dance\"\n+        return \"artistic_project\"\n \n     elif project_type == ProjectType.API:\n-        return 'api_service'\n+        return \"api_service\"\n \n     elif project_type == ProjectType.GAME:\n-        return 'interactive_game'\n+        return \"interactive_game\"\n \n     elif project_type == ProjectType.DATA:\n-        return 'data_analysis_project'\n+        return \"data_analysis_project\"\n \n     elif project_type == ProjectType.WEB:\n-        return 'web_application'\n+        return \"web_application\"\n \n     elif project_type == ProjectType.MOBILE:\n-        return 'mobile_app'\n+        return \"mobile_app\"\n \n     elif project_type == ProjectType.IOT:\n-        return 'iot_project'\n+        return \"iot_project\"\n \n     else:\n         # G\u00e9n\u00e9rique : utiliser les premiers mots de l'id\u00e9e\n         if len(words) >= 2:\n             return f\"{words[0]}_{words[1]}_project\"\n-        return 'ia_project'\n+        return \"ia_project\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/cli.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/cli.py\t2025-07-29 18:12:20.918439+00:00\n@@ -17,26 +17,23 @@\n # TODO: Pr\u00e9parer linternationalisation (i18n) des messages CLI et prompts\n # utilisateur.\n \n \n @click.group()\n-@click.option('--verbose', '-v', is_flag=True, help='Mode verbeux')\n+@click.option(\"--verbose\", \"-v\", is_flag=True, help=\"Mode verbeux\")\n def cli(verbose):\n     \"\"\"Athalia-G\u00e9n\u00e9rateur de projets IA intelligent.\"\"\"\n     if verbose:\n         logging.basicConfig(level=logging.INFO)\n     else:\n         logging.basicConfig(level=logging.WARNING)\n \n \n @cli.command()\n-@click.argument('idea')\n-@click.option('--output',\n-              '-o',\n-              default='./generated_project',\n-              help='Dossier de sortie')\n-@click.option('--dry-run', is_flag=True, help='Mode simulation')\n+@click.argument(\"idea\")\n+@click.option(\"--output\", \"-o\", default=\"./generated_project\", help=\"Dossier de sortie\")\n+@click.option(\"--dry-run\", is_flag=True, help=\"Mode simulation\")\n def generate(idea, output, dry_run):\n     \"\"\"G\u00e9n\u00e8re un projet complet \u00e0 partir dune id\u00e9e.\"\"\"\n     try:\n         click.echo(f\"\ud83d\ude80 G\u00e9n\u00e9ration du projet: {idea}\")\n \n@@ -50,12 +47,11 @@\n \n         if not blueprint:\n             click.echo(\"\u274c Impossible de g\u00e9n\u00e9rer le blueprint\")\n             return\n \n-        click.echo(\n-            f\"\u2705 Blueprint g\u00e9n\u00e9r\u00e9: {blueprint.get('project_type', 'Projet')}\")\n+        click.echo(f\"\u2705 Blueprint g\u00e9n\u00e9r\u00e9: {blueprint.get('project_type', 'Projet')}\")\n \n         # 2. G\u00e9n\u00e9rer le projet complet\n \n         # Cr\u00e9er le dossier de sortie s'il n'existe pas\n         os.makedirs(output, exist_ok=True)\n@@ -72,11 +68,11 @@\n         click.echo(f\"\u274c Erreur: {e}\")\n         click.echo(f\"\ud83d\udd0d D\u00e9tails: {traceback.format_exc()}\")\n \n \n @cli.command()\n-@click.argument('project_path')\n+@click.argument(\"project_path\")\n def audit(project_path):\n     \"\"\"Audit intelligent dun projet existant.\"\"\"\n     try:\n         click.echo(f\"\ud83d\udd0d Audit du projet: {project_path}\")\n \n@@ -87,11 +83,11 @@\n         click.echo(f\"\u26a0\ufe0f  Probl\u00e8mes d\u00e9tect\u00e9s: {len(results.get('issues', []))}\")\n         click.echo(f\"\ud83d\udca1 Suggestions: {len(results.get('suggestions', []))}\")\n \n         # Sauvegarder le rapport\n         report_path = Path(project_path) / \"audit_report.yaml\"\n-        with open(report_path, 'w') as f:\n+        with open(report_path, \"w\") as f:\n             yaml.dump(results, f, default_flow_style=False)\n \n         click.echo(f\"\ud83d\udcc4 Rapport sauvegard\u00e9: {report_path}\")\n \n     except Exception as e:\n@@ -111,12 +107,11 @@\n         for model in ai.available_models:\n             status = \"\u2705\" if model != AIModel.MOCK else \"\ud83d\udd04\"\n             click.echo(f\"  {status} {model.value}\")\n \n         # Cha\u00eene de fallback\n-        click.echo(\n-            f\"\\n\ud83d\udd04 Cha\u00eene de fallback ({len(ai.fallback_chain)} mod\u00e8les):\")\n+        click.echo(f\"\\n\ud83d\udd04 Cha\u00eene de fallback ({len(ai.fallback_chain)} mod\u00e8les):\")\n         for index, model in enumerate(ai.fallback_chain, 1):\n             click.echo(f\"  {index}. {model.value}\")\n \n         # Templates de prompts\n         click.echo(f\"\\n\ud83d\udcdd Templates de prompts: {len(ai.prompt_templates)}\")\n@@ -130,11 +125,11 @@\n     except Exception as e:\n         click.echo(f\"\u274c Erreur: {e}\")\n \n \n @cli.command()\n-@click.argument('idea')\n+@click.argument(\"idea\")\n def test_ai(idea):\n     \"\"\"Teste lIA robuste avec une id\u00e9e de projet.\"\"\"\n     try:\n         ai = RobustAI()\n         click.echo(f\"\ud83e\uddea Test IA robuste: {idea}\")\n@@ -146,38 +141,32 @@\n \n         click.echo(\"\u2705 Blueprint g\u00e9n\u00e9r\u00e9:\")\n         click.echo(f\"  \u2022 Nom: {blueprint.get('project_name', 'N/A')}\")\n         click.echo(f\"  \u2022 Type: {blueprint.get('project_type', 'N/A')}\")\n         click.echo(f\"  \u2022 Modules: {len(blueprint.get('modules', []))}\")\n-        click.echo(\n-            f\"  \u2022 D\u00e9pendances: {len(blueprint.get('dependencies', []))}\")\n+        click.echo(f\"  \u2022 D\u00e9pendances: {len(blueprint.get('dependencies', []))}\")\n \n         # Test de revue de code\n         click.echo(\"\\n\ud83d\udd0d Test de revue de code...\")\n         test_code = \"\"\"\n def hello_world():\n     print(\"Hello World\")\n     return True\n \"\"\"\n         review = ai.review_code(\n-            code=test_code,\n-            filename=\"test.py\",\n-            project_type=\"python\",\n-            current_score=50\n+            code=test_code, filename=\"test.py\", project_type=\"python\", current_score=50\n         )\n \n         click.echo(\"\u2705 Revue g\u00e9n\u00e9r\u00e9e:\")\n         click.echo(f\"  \u2022 Score: {review.get('score', 'N/A')}\")\n         click.echo(f\"  \u2022 Probl\u00e8mes: {len(review.get('issues', []))}\")\n         click.echo(f\"  \u2022 Suggestions: {len(review.get('suggestions', []))}\")\n \n         # Test de documentation\n         click.echo(\"\\n\ud83d\udcda Test de g\u00e9n\u00e9ration de documentation...\")\n         doc = ai.generate_documentation(\n-            project_name=\"test\",\n-            project_type=\"python\",\n-            modules=[\"api\", \"web\"]\n+            project_name=\"test\", project_type=\"python\", modules=[\"api\", \"web\"]\n         )\n \n         click.echo(f\"\u2705 Documentation g\u00e9n\u00e9r\u00e9e ({len(doc)} caract\u00e8res)\")\n \n         click.echo(\"\\n\ud83c\udf89 Tous les tests IA robuste r\u00e9ussis!\")\n@@ -186,7 +175,7 @@\n         click.echo(\"\u274c Module ai_robust non disponible\")\n     except Exception as e:\n         click.echo(f\"\u274c Erreur: {e}\")\n \n \n-if __name__ == '__main__':\n+if __name__ == \"__main__\":\n     cli()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/dashboard.py\t2025-07-29 17:56:26.230000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/dashboard.py\t2025-07-29 18:12:20.925819+00:00\n@@ -7,11 +7,12 @@\n     st.header(\"Benchmarks IA (Qwen, Mistral, Mock)\")\n     csv_path = \"benchmark_results.csv\"\n     if not os.path.exists(csv_path):\n         st.warning(\n             \"Aucun benchmark_results.csv trouv\u00e9. Lancez le script de benchmark \"\n-            \"pour g\u00e9n\u00e9rer les r\u00e9sultats.\")\n+            \"pour g\u00e9n\u00e9rer les r\u00e9sultats.\"\n+        )\n         return\n \n     try:\n         df = pd.read_csv(csv_path)\n         st.dataframe(df)\n@@ -25,12 +26,11 @@\n         available_columns = df.columns.tolist()\n \n         # Filtres\n         if \"model\" in available_columns:\n             model = st.selectbox(\n-                \"Filtrer par mod\u00e8le\",\n-                [\"Tous\"] + sorted(df[\"model\"].unique())\n+                \"Filtrer par mod\u00e8le\", [\"Tous\"] + sorted(df[\"model\"].unique())\n             )\n             if model != \"Tous\":\n                 df = df[df[\"model\"] == model]\n \n         # Graphiques - seulement si les colonnes existent\n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_cleaner.py\t2025-07-29 18:02:53.600000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_cleaner.py\t2025-07-29 18:12:20.928041+00:00\n@@ -28,20 +28,19 @@\n         self.errors: List[str] = []\n         self.stats: Dict[str, Any] = {\n             \"files_removed\": 0,\n             \"dirs_removed\": 0,\n             \"space_freed_mb\": 0.0,\n-            \"errors\": 0\n+            \"errors\": 0,\n         }\n \n     def clean_project(self, dry_run: bool = False) -> Dict[str, Any]:\n         \"\"\"Nettoyage complet dun projet\"\"\"\n         self.dry_run = dry_run\n         project_path_obj = self.project_path\n \n-        logger.info(\n-            f\"\ud83e\uddf9 Nettoyage automatique en cours pour : {project_path_obj.name}\")\n+        logger.info(f\"\ud83e\uddf9 Nettoyage automatique en cours pour : {project_path_obj.name}\")\n         if dry_run:\n             logger.info(\"\ud83d\udd0d Mode simulation - aucun fichier ne sera supprim\u00e9\")\n \n         # Nettoyages en s\u00e9quence\n         self._clean_system_files(project_path_obj)\n@@ -80,28 +79,28 @@\n             \".LSOverride\",  # Launch Services Override\n         ]\n \n         for pattern in system_patterns:\n             for file_path in project_path.rglob(pattern):\n-                self._safe_remove_file(\n-                    file_path, f\"Fichier syst\u00e8me: {pattern}\")\n+                self._safe_remove_file(file_path, f\"Fichier syst\u00e8me: {pattern}\")\n \n         # Nettoyage sp\u00e9cifique des dossiers macOS\n         macos_dirs = [\n             \".Spotlight-V100\",\n             \".Trashes\",\n             \".fseventsd\",\n             \".TemporaryItems\",\n             \".AppleDouble\",\n-            \".LSOverride\"\n+            \".LSOverride\",\n         ]\n \n         for dir_name in macos_dirs:\n             for dir_path in project_path.rglob(dir_name):\n                 if dir_path.is_dir():\n                     self._safe_remove_dir(\n-                        dir_path, f\"Dossier syst\u00e8me macOS: {dir_name}\")\n+                        dir_path, f\"Dossier syst\u00e8me macOS: {dir_name}\"\n+                    )\n \n     def _clean_cache_files(self, project_path: Path):\n         \"\"\"Nettoyage des fichiers de cache\"\"\"\n         cache_patterns = [\n             \"__pycache__\",\n@@ -207,14 +206,12 @@\n \n         for file_path in project_path.rglob(\"*\"):\n             if file_path.is_file():\n                 try:\n                     mtime = datetime.fromtimestamp(file_path.stat().st_mtime)\n-                    if mtime < cutoff_date and not self._is_important_file(\n-                            file_path):\n-                        self._safe_remove_file(\n-                            file_path, \"Fichier ancien (>1 an)\")\n+                    if mtime < cutoff_date and not self._is_important_file(file_path):\n+                        self._safe_remove_file(file_path, \"Fichier ancien (>1 an)\")\n                 except Exception:\n                     pass\n \n     def _clean_large_files(self, project_path: Path):\n         \"\"\"Nettoyage des fichiers volumineux\"\"\"\n@@ -222,33 +219,30 @@\n \n         for file_path in project_path.rglob(\"*\"):\n             if file_path.is_file():\n                 try:\n                     size_mb = file_path.stat().st_size / (1024 * 1024)\n-                    if size_mb > max_size_mb and not self._is_important_file(\n-                            file_path):\n+                    if size_mb > max_size_mb and not self._is_important_file(file_path):\n                         self._safe_remove_file(\n-                            file_path, f\"Fichier volumineux ({size_mb:.1f}MB)\")\n+                            file_path, f\"Fichier volumineux ({size_mb:.1f}MB)\"\n+                        )\n                 except Exception:\n                     pass\n \n     def _safe_remove_file(self, file_path: Path, reason: str):\n         \"\"\"Suppression s\u00e9curis\u00e9e dun fichier\"\"\"\n         try:\n             if not self.dry_run:\n                 file_size = file_path.stat().st_size / (1024 * 1024)  # MB\n                 file_path.unlink()\n-                self.cleaned_files.append({\n-                    \"path\": str(file_path),\n-                    \"reason\": reason,\n-                    \"size_mb\": file_size\n-                })\n+                self.cleaned_files.append(\n+                    {\"path\": str(file_path), \"reason\": reason, \"size_mb\": file_size}\n+                )\n                 self.stats[\"files_removed\"] += 1\n                 self.stats[\"space_freed_mb\"] += file_size\n             else:\n-                logger.info(\n-                    f\"\ud83d\udd0d Simulation: Suppression de {file_path} ({reason})\")\n+                logger.info(f\"\ud83d\udd0d Simulation: Suppression de {file_path} ({reason})\")\n \n         except Exception as e:\n             self.errors.append(f\"Erreur suppression {file_path}: {e}\")\n             self.stats[\"errors\"] += 1\n \n@@ -257,39 +251,65 @@\n         try:\n             if not self.dry_run:\n                 shutil.rmtree(dir_path)\n                 self.stats[\"dirs_removed\"] += 1\n \n-            self.cleaned_dirs.append({\n-                \"path\": str(dir_path),\n-                \"reason\": reason\n-            })\n+            self.cleaned_dirs.append({\"path\": str(dir_path), \"reason\": reason})\n \n         except Exception as e:\n             self.errors.append(f\"Erreur suppression {dir_path}: {e}\")\n             self.stats[\"errors\"] += 1\n \n     def _is_code_file(self, file_path: Path) -> bool:\n         \"\"\"D\u00e9termine si un fichier est un fichier de code\"\"\"\n         code_extensions = {\n-            '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h',\n-            '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala', '.cs',\n-            '.html', '.css', '.scss', '.sass', '.xml', '.json', '.yaml', '.yml'\n+            \".py\",\n+            \".js\",\n+            \".ts\",\n+            \".jsx\",\n+            \".tsx\",\n+            \".java\",\n+            \".cpp\",\n+            \".c\",\n+            \".h\",\n+            \".go\",\n+            \".rs\",\n+            \".php\",\n+            \".rb\",\n+            \".swift\",\n+            \".kt\",\n+            \".scala\",\n+            \".cs\",\n+            \".html\",\n+            \".css\",\n+            \".scss\",\n+            \".sass\",\n+            \".xml\",\n+            \".json\",\n+            \".yaml\",\n+            \".yml\",\n         }\n         return file_path.suffix.lower() in code_extensions\n \n     def _is_important_file(self, file_path: Path) -> bool:\n         \"\"\"D\u00e9termine si un fichier est important (ex: config, requirements, etc.)\"\"\"\n         important_patterns = [\n-            \"README\", \"LICENSE\", \"requirements.txt\", \"package.json\",\n-            \"Makefile\", \"docker-compose.yml\", \".env\",\n-            \"setup.py\", \"pyproject.toml\", \"Cargo.toml\", \"go.mod\"\n+            \"README\",\n+            \"LICENSE\",\n+            \"requirements.txt\",\n+            \"package.json\",\n+            \"Makefile\",\n+            \"docker-compose.yml\",\n+            \".env\",\n+            \"setup.py\",\n+            \"pyproject.toml\",\n+            \"Cargo.toml\",\n+            \"go.mod\",\n         ]\n \n         file_name = file_path.name.lower()\n-        return any(\n-            pattern.lower() in file_name for pattern in important_patterns)\n+        return any(pattern.lower() in file_name for pattern in important_patterns)\n \n     def _is_empty_directory(self, dir_path: Path) -> bool:\n         \"\"\"V\u00e9rifie si un r\u00e9pertoire est vide\"\"\"\n         try:\n             return not any(dir_path.iterdir())\n@@ -319,11 +339,11 @@\n         return {\n             \"stats\": self.stats,\n             \"files\": self.cleaned_files,\n             \"dirs\": self.cleaned_dirs,\n             \"errors\": self.errors,\n-            \"summary\": self._generate_summary()\n+            \"summary\": self._generate_summary(),\n         }\n \n     def _generate_summary(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un r\u00e9sum\u00e9 du nettoyage\"\"\"\n         reasons: Dict[str, List[Dict[str, Any]]] = {}\n@@ -369,12 +389,11 @@\n     def optimize_project_structure(self, project_path: str) -> Dict[str, Any]:\n         \"\"\"Optimise la structure du projet\"\"\"\n         project_path_obj = Path(project_path)\n         optimizations = []\n \n-        logger.info(\n-            f\"\u26a1 Optimisation de la structure pour : {project_path_obj.name}\")\n+        logger.info(f\"\u26a1 Optimisation de la structure pour : {project_path_obj.name}\")\n \n         # Cr\u00e9ation de r\u00e9pertoires standards\n         standard_dirs = [\"src\", \"tests\", \"docs\", \"data\", \"scripts\", \"assets\"]\n         for dir_name in standard_dirs:\n             dir_path = project_path_obj / dir_name\n@@ -384,14 +403,11 @@\n                 optimizations.append(f\"Cr\u00e9\u00e9: {dir_name}/\")\n \n         # D\u00e9placement des fichiers dans les bons r\u00e9pertoires\n         self._organize_files(project_path_obj, optimizations)\n \n-        return {\n-            \"optimizations\": optimizations,\n-            \"dry_run\": self.dry_run\n-        }\n+        return {\"optimizations\": optimizations, \"dry_run\": self.dry_run}\n \n     def _organize_files(self, project_path: Path, optimizations: List[str]):\n         \"\"\"Organise les fichiers dans la structure du projet\"\"\"\n         # D\u00e9placer les fichiers de test\n         for file_path in project_path.rglob(\"*test*.py\"):\n@@ -405,69 +421,66 @@\n         for file_path in project_path.rglob(\"*.sh\"):\n             if file_path.parent != project_path / \"scripts\":\n                 new_path = project_path / \"scripts\" / file_path.name\n                 if not self.dry_run and not new_path.exists():\n                     file_path.rename(new_path)\n-                    optimizations.append(\n-                        f\"D\u00e9plac\u00e9: {file_path.name} \u2192 scripts/\")\n+                    optimizations.append(f\"D\u00e9plac\u00e9: {file_path.name} \u2192 scripts/\")\n \n         # D\u00e9placer les assets\n         for ext in [\"png\", \"jpg\", \"jpeg\", \"gif\", \"svg\", \"ico\"]:\n             for file_path in project_path.rglob(f\"*.{ext}\"):\n                 if file_path.parent != project_path / \"assets\":\n                     new_path = project_path / \"assets\" / file_path.name\n                 if not self.dry_run and not new_path.exists():\n                     file_path.rename(new_path)\n-                    optimizations.append(\n-                        f\"D\u00e9plac\u00e9: {file_path.name} \u2192 assets/\")\n+                    optimizations.append(f\"D\u00e9plac\u00e9: {file_path.name} \u2192 assets/\")\n \n \n def main():\n     \"\"\"Point dentr\u00e9e du module AutoCleaner\"\"\"\n \n     parser = argparse.ArgumentParser(description=\"Nettoyage automatique de f\")\n-    parser.add_argument(\n-        \"project_path\",\n-        help=\"Chemin vers le projet \u00e0 nettoyer\")\n+    parser.add_argument(\"project_path\", help=\"Chemin vers le projet \u00e0 nettoyer\")\n     parser.add_argument(\n         \"--dry-run\",\n         action=\"store_true\",\n-        help=\"Mode simulation - aucun fichier ne sera supprim\u00e9\")\n+        help=\"Mode simulation - aucun fichier ne sera supprim\u00e9\",\n+    )\n     parser.add_argument(\n-        \"--optimize\",\n-        action=\"store_true\",\n-        help=\"Optimiser la structure du projet\")\n+        \"--optimize\", action=\"store_true\", help=\"Optimiser la structure du projet\"\n+    )\n \n     args = parser.parse_args()\n \n     if not os.path.exists(args.project_path):\n         logger.info(f\"\u274c Le chemin {args.project_path} nexiste pas\")\n         return\n \n     # Configuration du logging\n     logging.basicConfig(\n-        level=logging.INFO,\n-        format='%(asctime)s - %(levelname)s - %(message)s')\n+        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n+    )\n \n     # Nettoyage\n     cleaner = AutoCleaner(args.project_path)\n     cleanup_report = cleaner.clean_project(dry_run=args.dry_run)\n \n     # Affichage du rapport\n     logger.info(cleanup_report[\"summary\"])\n \n     # Optimisation de structure si demand\u00e9e\n     if args.optimize:\n-        structure_report = cleaner.optimize_project_structure(\n-            args.project_path)\n+        structure_report = cleaner.optimize_project_structure(args.project_path)\n         logger.info(\"\\n\u26a1 OPTIMISATIONS DE STRUCTURE:\")\n         for opt in structure_report[\"optimizations\"]:\n             logger.info(f\"   \u2022 {opt}\")\n \n     # Sauvegarde du rapport\n-    report_file = f\"data/reports/cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n-    with open(report_file, 'w', encoding='utf-8') as f:\n+    report_file = (\n+        f\"data/reports/cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n+    )\n+    with open(report_file, \"w\", encoding=\"utf-8\") as f:\n         json.dump(cleanup_report, f, indent=2, ensure_ascii=False)\n \n     logger.info(f\"\\n\ud83d\udcc4 Rapport sauvegard\u00e9: {report_file}\")\n \n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/cache_manager.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/cache_manager.py\t2025-07-29 18:12:20.931492+00:00\n@@ -16,11 +16,13 @@\n \n \n class AnalysisCache:\n     \"\"\"Gestionnaire de cache intelligent pour les analyses.\"\"\"\n \n-    def __init__(self, cache_dir: str = \"cache\", max_size: int = 1000, ttl_hours: int = 24):\n+    def __init__(\n+        self, cache_dir: str = \"cache\", max_size: int = 1000, ttl_hours: int = 24\n+    ):\n         \"\"\"\n         Initialise le gestionnaire de cache.\n \n         Args:\n             cache_dir: R\u00e9pertoire de stockage du cache\n@@ -37,13 +39,17 @@\n         os.makedirs(cache_dir, exist_ok=True)\n \n         # Cache en m\u00e9moire pour les acc\u00e8s rapides\n         self._memory_cache = {}\n \n-        logging.info(f\"Cache initialis\u00e9: {cache_dir}, max_size={max_size}, ttl={ttl_hours}h\")\n-\n-    def _generate_cache_key(self, project_path: str, analysis_type: str, **kwargs) -> str:\n+        logging.info(\n+            f\"Cache initialis\u00e9: {cache_dir}, max_size={max_size}, ttl={ttl_hours}h\"\n+        )\n+\n+    def _generate_cache_key(\n+        self, project_path: str, analysis_type: str, **kwargs\n+    ) -> str:\n         \"\"\"\n         G\u00e9n\u00e8re une cl\u00e9 de cache unique.\n \n         Args:\n             project_path: Chemin du projet\n@@ -55,11 +61,13 @@\n         \"\"\"\n         # Hash du projet pour \u00e9viter les collisions\n         project_hash = hashlib.md5(project_path.encode()).hexdigest()[:8]\n \n         # Param\u00e8tres suppl\u00e9mentaires\n-        params_hash = hashlib.md5(json.dumps(kwargs, sort_keys=True).encode()).hexdigest()[:8]\n+        params_hash = hashlib.md5(\n+            json.dumps(kwargs, sort_keys=True).encode()\n+        ).hexdigest()[:8]\n \n         return f\"{analysis_type}_{project_hash}_{params_hash}\"\n \n     def _get_cache_file_path(self, cache_key: str) -> str:\n         \"\"\"Retourne le chemin du fichier de cache.\"\"\"\n@@ -82,11 +90,13 @@\n         file_time = datetime.fromtimestamp(os.path.getmtime(cache_file))\n         cache_age = datetime.now() - file_time\n \n         return cache_age < timedelta(hours=self.ttl_hours)\n \n-    def get(self, project_path: str, analysis_type: str, **kwargs) -> Optional[Dict[str, Any]]:\n+    def get(\n+        self, project_path: str, analysis_type: str, **kwargs\n+    ) -> Optional[Dict[str, Any]]:\n         \"\"\"\n         R\u00e9cup\u00e8re un r\u00e9sultat du cache.\n \n         Args:\n             project_path: Chemin du projet\n@@ -107,11 +117,11 @@\n         # V\u00e9rification du cache fichier\n         cache_file = self._get_cache_file_path(cache_key)\n \n         if self._is_cache_valid(cache_file):\n             try:\n-                with open(cache_file, 'r', encoding='utf-8') as f:\n+                with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n                     result = json.load(f)\n \n                 # Mise en cache m\u00e9moire\n                 self._memory_cache[cache_key] = result\n                 self.cache_hits += 1\n@@ -124,11 +134,13 @@\n \n         self.cache_misses += 1\n         logging.debug(f\"Cache miss: {cache_key}\")\n         return None\n \n-    def set(self, project_path: str, analysis_type: str, result: Dict[str, Any], **kwargs) -> None:\n+    def set(\n+        self, project_path: str, analysis_type: str, result: Dict[str, Any], **kwargs\n+    ) -> None:\n         \"\"\"\n         Stocke un r\u00e9sultat dans le cache.\n \n         Args:\n             project_path: Chemin du projet\n@@ -143,21 +155,21 @@\n             \"result\": result,\n             \"metadata\": {\n                 \"created_at\": datetime.now().isoformat(),\n                 \"project_path\": project_path,\n                 \"analysis_type\": analysis_type,\n-                \"parameters\": kwargs\n-            }\n+                \"parameters\": kwargs,\n+            },\n         }\n \n         # Stockage en m\u00e9moire\n         self._memory_cache[cache_key] = cache_data\n \n         # Stockage fichier\n         cache_file = self._get_cache_file_path(cache_key)\n         try:\n-            with open(cache_file, 'w', encoding='utf-8') as f:\n+            with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n                 json.dump(cache_data, f, indent=2, ensure_ascii=False)\n \n             logging.debug(f\"Cache stored: {cache_key}\")\n \n         except Exception as e:\n@@ -173,11 +185,11 @@\n                 return\n \n             cache_files = os.listdir(self.cache_dir)\n \n             for filename in cache_files:\n-                if not filename.endswith('.json'):\n+                if not filename.endswith(\".json\"):\n                     continue\n \n                 cache_file = os.path.join(self.cache_dir, filename)\n \n                 if not self._is_cache_valid(cache_file):\n@@ -189,12 +201,12 @@\n                         pass\n \n             # Nettoyage du cache m\u00e9moire\n             expired_keys = []\n             for key, data in self._memory_cache.items():\n-                if 'metadata' in data:\n-                    created_at = datetime.fromisoformat(data['metadata']['created_at'])\n+                if \"metadata\" in data:\n+                    created_at = datetime.fromisoformat(data[\"metadata\"][\"created_at\"])\n                     if datetime.now() - created_at > timedelta(hours=self.ttl_hours):\n                         expired_keys.append(key)\n \n             for key in expired_keys:\n                 del self._memory_cache[key]\n@@ -206,11 +218,11 @@\n         \"\"\"Vide compl\u00e8tement le cache.\"\"\"\n         try:\n             # Suppression des fichiers\n             if os.path.exists(self.cache_dir):\n                 for filename in os.listdir(self.cache_dir):\n-                    if filename.endswith('.json'):\n+                    if filename.endswith(\".json\"):\n                         try:\n                             os.remove(os.path.join(self.cache_dir, filename))\n                         except OSError:\n                             # Fichier d\u00e9j\u00e0 supprim\u00e9 ou inaccessible\n                             pass\n@@ -241,11 +253,13 @@\n             \"cache_hits\": self.cache_hits,\n             \"cache_misses\": self.cache_misses,\n             \"total_requests\": total_requests,\n             \"hit_rate_percent\": hit_rate,\n             \"memory_cache_size\": len(self._memory_cache),\n-            \"file_cache_size\": len([f for f in os.listdir(self.cache_dir) if f.endswith('.json')])\n+            \"file_cache_size\": len(\n+                [f for f in os.listdir(self.cache_dir) if f.endswith(\".json\")]\n+            ),\n         }\n \n \n # Instance globale du cache\n _analysis_cache = AnalysisCache()\n@@ -259,10 +273,11 @@\n         func: Fonction \u00e0 d\u00e9corer\n \n     Returns:\n         Fonction d\u00e9cor\u00e9e avec cache\n     \"\"\"\n+\n     @wraps(func)\n     def wrapper(project_path: str, *args, **kwargs):\n         # G\u00e9n\u00e9ration de la cl\u00e9 de cache\n         analysis_type = func.__name__\n \n@@ -312,11 +327,13 @@\n     return lru_cache(maxsize=max_size)\n \n \n # Exemple d'utilisation\n @cached_analysis\n-def analyze_project_structure(project_path: str, detailed: bool = False) -> Dict[str, Any]:\n+def analyze_project_structure(\n+    project_path: str, detailed: bool = False\n+) -> Dict[str, Any]:\n     \"\"\"\n     Analyse la structure d'un projet (exemple).\n \n     Args:\n         project_path: Chemin du projet\n@@ -331,11 +348,11 @@\n     return {\n         \"project_path\": project_path,\n         \"files_count\": 100,\n         \"structure\": [\"src/\", \"tests/\", \"docs/\"],\n         \"detailed\": detailed,\n-        \"timestamp\": datetime.now().isoformat()\n+        \"timestamp\": datetime.now().isoformat(),\n     }\n \n \n if __name__ == \"__main__\":\n     # Test du cache\n@@ -345,6 +362,6 @@\n     print(\"Test avec cache...\")\n     result1 = analyze_project_structure(\"/test/project\", detailed=True)\n     result2 = analyze_project_structure(\"/test/project\", detailed=True)  # Cache hit\n \n     print(f\"R\u00e9sultats identiques: {result1 == result2}\")\n-    print(f\"Stats cache: {get_cache_stats()}\")\n\\ No newline at end of file\n+    print(f\"Stats cache: {get_cache_stats()}\")\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/audit_distiller.py\t2025-07-29 17:56:28.580000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/audit_distiller.py\t2025-07-29 18:12:20.933836+00:00\n@@ -22,15 +22,12 @@\n             return {}\n         # Exemple simple : moyenne pond\u00e9r\u00e9e des scores\n         total_score = 0.0\n         total_weight = 0.0\n         for audit in audits:\n-            t = audit.get('type', 'autre')\n-            score = audit.get('score', 0)\n+            t = audit.get(\"type\", \"autre\")\n+            score = audit.get(\"score\", 0)\n             w = self.weights.get(t, 1.0)\n             total_score += score * w\n             total_weight += w\n         global_score = total_score / total_weight if total_weight else 0.0\n-        return {\n-            'global_score': global_score,\n-            'details': audits\n-        }\n+        return {\"global_score\": global_score, \"details\": audits}\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/correction_distiller.py\t2025-07-29 17:56:28.790000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/correction_distiller.py\t2025-07-29 18:12:20.938356+00:00\n@@ -5,28 +5,29 @@\n \"\"\"\n from typing import List, Dict, Any, Optional\n \n \n class CorrectionDistiller:\n-    def __init__(self, strategy: str = 'score'):\n+    def __init__(self, strategy: str = \"score\"):\n         self.strategy = strategy\n \n-    def distill(self,\n-                corrections: List[str],\n-                scores: Optional[List[float]] = None,\n-                context: Optional[Dict[str,\n-                                       Any]] = None) -> str:\n+    def distill(\n+        self,\n+        corrections: List[str],\n+        scores: Optional[List[float]] = None,\n+        context: Optional[Dict[str, Any]] = None,\n+    ) -> str:\n         \"\"\"\n         S\u00e9lectionne ou fusionne la meilleure correction IA.\n         :param corrections: Liste de corrections propos\u00e9es (str)\n         :param scores: Scores optionnels pour chaque correction\n         :param context: Contexte optionnel\n         :return: Correction distill\u00e9e (str)\n         \"\"\"\n         if not corrections:\n-            return ''\n-        if self.strategy == 'score' and scores:\n+            return \"\"\n+        if self.strategy == \"score\" and scores:\n             # Prend la correction avec le meilleur score\n             idx = scores.index(max(scores))\n             return corrections[idx]\n         # Placeholder pour d'autres strat\u00e9gies (fusion, feedback, etc.)\n         return corrections[0]  # fallback\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/quality_scorer.py\t2025-07-29 17:56:29.110000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/quality_scorer.py\t2025-07-29 18:12:20.945933+00:00\n@@ -10,12 +10,11 @@\n     def __init__(self, weights: Optional[Dict[str, float]] = None):\n         # Pond\u00e9ration des crit\u00e8res (ex: {'pertinence': 0.5, 'clart\u00e9': 0.3,\n         # 'impact': 0.2})\n         self.weights = weights or {}\n \n-    def score(self, solution: Any,\n-              context: Optional[Dict[str, Any]] = None) -> float:\n+    def score(self, solution: Any, context: Optional[Dict[str, Any]] = None) -> float:\n         \"\"\"\n         \u00c9value la qualit\u00e9 d'une solution IA.\n         :param solution: Solution \u00e0 scorer (str, dict, ...)\n         :param context: Contexte optionnel\n         :return: Score de qualit\u00e9 (float)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/code_linter.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/code_linter.py\t2025-07-29 18:12:20.939728+00:00\n@@ -16,16 +16,11 @@\n     \"\"\"Linter de code pour Athalia\"\"\"\n \n     def __init__(self, project_path: str, auto_fix: bool = False):\n         self.project_path = Path(project_path)\n         self.auto_fix = auto_fix\n-        self.report = {\n-            \"errors\": [],\n-            \"warnings\": [],\n-            \"fixes\": [],\n-            \"score\": 0\n-        }\n+        self.report = {\"errors\": [], \"warnings\": [], \"fixes\": [], \"score\": 0}\n \n     def run(self) -> Dict[str, Any]:\n         \"\"\"Lance lanalyse de qualit\u00e9 du projet\"\"\"\n         logger.info(f\"\ud83d\udccf Analyse de qualit\u00e9 pour : {self.project_path.name}\")\n \n@@ -42,72 +37,87 @@\n         return self.report\n \n     def _run_flake8(self):\n         \"\"\"Ex\u00e9cution de Flake8\"\"\"\n         try:\n-            result = subprocess.run([\n-                \"flake8\", str(self.project_path), \"--max-line-length=120\"\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"flake8\", str(self.project_path), \"--max-line-length=120\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n \n             if result.stdout:\n-                for line in result.stdout.split('\\n'):\n+                for line in result.stdout.split(\"\\n\"):\n                     if line.strip():\n                         self.report[\"errors\"].append(f\"Flake8: {line}\")\n \n         except Exception as e:\n             self.report[\"errors\"].append(f\"Flake8 non ex\u00e9cut\u00e9: {e}\")\n \n     def _run_black(self):\n         \"\"\"Ex\u00e9cution de Black\"\"\"\n         try:\n-            result = subprocess.run([\n-                \"black\", str(self.project_path), \"--check\"\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"black\", str(self.project_path), \"--check\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n \n             if result.returncode != 0:\n                 self.report[\"warnings\"].append(\"Formatage Black \u00e0 corriger\")\n \n         except Exception as e:\n             self.report[\"warnings\"].append(f\"Black non ex\u00e9cut\u00e9: {e}\")\n \n     def _run_isort(self):\n         \"\"\"Ex\u00e9cution de isort\"\"\"\n         try:\n-            result = subprocess.run([\n-                \"isort\", str(self.project_path), \"--check-only\"\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"isort\", str(self.project_path), \"--check-only\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n \n             if result.returncode != 0:\n                 self.report[\"warnings\"].append(\"Tri des imports \u00e0 corriger\")\n \n         except Exception as e:\n             self.report[\"warnings\"].append(f\"isort non ex\u00e9cut\u00e9: {e}\")\n \n     def _run_mypy(self):\n         \"\"\"Ex\u00e9cution de MyPy\"\"\"\n         try:\n-            result = subprocess.run([\n-                \"mypy\", str(self.project_path)\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"mypy\", str(self.project_path)],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n \n             if result.stdout:\n-                for line in result.stdout.split('\\n'):\n+                for line in result.stdout.split(\"\\n\"):\n                     if line.strip():\n                         self.report[\"warnings\"].append(f\"MyPy: {line}\")\n \n         except Exception as e:\n             self.report[\"warnings\"].append(f\"Mypy non ex\u00e9cut\u00e9: {e}\")\n \n     def _run_bandit(self):\n         \"\"\"Ex\u00e9cution de Bandit pour la s\u00e9curit\u00e9\"\"\"\n         try:\n-            result = subprocess.run([\n-                \"bandit\", \"-r\", str(self.project_path), \"-f\", \"txt\"\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"bandit\", \"-r\", str(self.project_path), \"-f\", \"txt\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n \n             if result.stdout:\n-                for line in result.stdout.split('\\n'):\n+                for line in result.stdout.split(\"\\n\"):\n                     if line.strip():\n                         self.report[\"warnings\"].append(f\"Bandit: {line}\")\n \n         except Exception as e:\n             self.report[\"warnings\"].append(f\"Bandit non ex\u00e9cut\u00e9: {e}\")\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/adaptive_distillation.py\t2025-07-29 17:56:28.480000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/adaptive_distillation.py\t2025-07-29 18:12:21.083386+00:00\n@@ -13,13 +13,12 @@\n     def __init__(self, history_path: Optional[str] = None):\n         \"\"\"\n         Initialise le distillateur adaptatif.\n         :param history_path: Chemin du fichier JSON pour l'historique (optionnel)\n         \"\"\"\n-        self.preference_weights: Dict[str,\n-                                      float] = {}  # Pond\u00e9ration des r\u00e9ponses\n-        self.success_history: List[str] = []     # Historique des succ\u00e8s\n+        self.preference_weights: Dict[str, float] = {}  # Pond\u00e9ration des r\u00e9ponses\n+        self.success_history: List[str] = []  # Historique des succ\u00e8s\n         # {r\u00e9ponse: [succ\u00e8s, \u00e9checs]}\n         self.feedback: Dict[str, List[int]] = {}\n         self.history_path = history_path or \"adaptive_distillation_history.json\"\n         self.load_history()\n \n@@ -62,16 +61,18 @@\n         \"\"\"\n         Trie les r\u00e9ponses selon leur poids appris et taux de succ\u00e8s.\n         :param responses: Liste de r\u00e9ponses IA\n         :return: Liste tri\u00e9e\n         \"\"\"\n+\n         def score(r):\n             w = self.preference_weights.get(r, 0)\n             s, f = self.feedback.get(r, [0, 0])\n             total = s + f\n             taux_succes = s / total if total > 0 else 0\n             return w + taux_succes\n+\n         return sorted(responses, key=score, reverse=True)\n \n     def ensemble_fusion(\n         self, responses: List[str], context: Optional[Dict[str, Any]]\n     ) -> str:\n@@ -80,21 +81,22 @@\n         :param responses: Liste de r\u00e9ponses pond\u00e9r\u00e9es\n         :param context: Contexte optionnel\n         :return: R\u00e9ponse fusionn\u00e9e\n         \"\"\"\n         from collections import Counter\n+\n         if not responses:\n-            return ''\n+            return \"\"\n         counter = Counter(responses)\n         return counter.most_common(1)[0][0]\n \n     def save_history(self):\n         \"\"\"Sauvegarde l'historique et les poids en JSON.\"\"\"\n         data = {\n             \"preference_weights\": self.preference_weights,\n             \"success_history\": self.success_history,\n-            \"feedback\": self.feedback\n+            \"feedback\": self.feedback,\n         }\n         try:\n             with open(self.history_path, \"w\", encoding=\"utf-8\") as f:\n                 json.dump(data, f, indent=2, ensure_ascii=False)\n         except Exception as e:\n@@ -109,7 +111,6 @@\n                     data = json.load(f)\n                 self.preference_weights = data.get(\"preference_weights\", {})\n                 self.success_history = data.get(\"success_history\", [])\n                 self.feedback = data.get(\"feedback\", {})\n             except Exception as e:\n-                print(\n-                    f\"[AdaptiveDistiller] Erreur chargement historique : {e}\")\n+                print(f\"[AdaptiveDistiller] Erreur chargement historique : {e}\")\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/code_genetics.py\t2025-07-29 17:56:28.690000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/code_genetics.py\t2025-07-29 18:12:21.158354+00:00\n@@ -20,11 +20,11 @@\n         length = min(len(f) for f in fragments)\n         child = []\n         for i in range(length):\n             parent = random.choice(fragments)\n             child.append(parent[i])\n-        return ' '.join(child)\n+        return \" \".join(child)\n \n     def mutate(self, solution: str, mutation_rate: float = 0.1) -> str:\n         \"\"\"\n         Mutation simple : modifie al\u00e9atoirement des mots de la solution.\n         :param solution: Solution \u00e0 muter\n@@ -33,30 +33,32 @@\n         \"\"\"\n         words = solution.split()\n         for i in range(len(words)):\n             if random.random() < mutation_rate:\n                 words[i] = f\"MUT_{random.randint(0,999)}\"\n-        return ' '.join(words)\n+        return \" \".join(words)\n \n-    def select(self, solutions: List[str], scorer: Callable[[\n-               str], float], top_k: int = 2) -> List[str]:\n+    def select(\n+        self, solutions: List[str], scorer: Callable[[str], float], top_k: int = 2\n+    ) -> List[str]:\n         \"\"\"\n         S\u00e9lectionne les meilleures solutions selon un score.\n         :param solutions: Liste de solutions\n         :param scorer: Fonction de scoring (str -> float)\n         :param top_k: Nombre de solutions \u00e0 garder\n         :return: Liste des meilleures solutions\n         \"\"\"\n         scored = sorted(solutions, key=scorer, reverse=True)\n         return scored[:top_k]\n \n-    def evolve(self,\n-               solutions: List[str],\n-               scorer: Callable[[str],\n-                                float],\n-               generations: int = 3,\n-               mutation_rate: float = 0.1) -> str:\n+    def evolve(\n+        self,\n+        solutions: List[str],\n+        scorer: Callable[[str], float],\n+        generations: int = 3,\n+        mutation_rate: float = 0.1,\n+    ) -> str:\n         \"\"\"\n         Fait \u00e9voluer les solutions sur plusieurs g\u00e9n\u00e9rations (croisement, mutation, s\u00e9lection).\n         :param solutions: Liste initiale\n         :param scorer: Fonction de scoring\n         :param generations: Nombre de g\u00e9n\u00e9rations\n@@ -65,16 +67,13 @@\n         \"\"\"\n         population = solutions[:]\n         for _ in range(generations):\n             # Croisement\n             children = [\n-                self.crossover(\n-                    random.sample(\n-                        population, min(\n-                            2, len(population)))) for _ in range(\n-                    len(population))]\n+                self.crossover(random.sample(population, min(2, len(population))))\n+                for _ in range(len(population))\n+            ]\n             # Mutation\n             mutated = [self.mutate(child, mutation_rate) for child in children]\n             # S\u00e9lection\n-            population = self.select(\n-                population + mutated, scorer, top_k=len(solutions))\n+            population = self.select(population + mutated, scorer, top_k=len(solutions))\n         return population[0] if population else \"\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/multimodal_distiller.py\t2025-07-29 17:56:28.900000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/multimodal_distiller.py\t2025-07-29 18:12:21.177585+00:00\n@@ -7,14 +7,16 @@\n from typing import List, Dict, Any, Optional\n from athalia_core.ai_robust import RobustAI, AIModel\n \n \n class MultimodalDistiller:\n-    def distill(self,\n-                text_prompts: List[str],\n-                image_paths: List[str],\n-                context: Optional[Dict[str, Any]] = None) -> str:\n+    def distill(\n+        self,\n+        text_prompts: List[str],\n+        image_paths: List[str],\n+        context: Optional[Dict[str, Any]] = None,\n+    ) -> str:\n         \"\"\"\n         Fusionne les r\u00e9ponses texte et image en utilisant LLaVA (Ollama) et\n         d'autres mod\u00e8les si besoin.\n         :param text_prompts: Liste de prompts texte\n         :param image_paths: Liste de chemins d'images (un par prompt ou global)\n@@ -46,14 +48,18 @@\n         :param image_path: Chemin de l'image \u00e0 analyser\n         :return: R\u00e9ponse de LLaVA (str)\n         \"\"\"\n         # Ollama LLaVA supporte --image <path> en CLI\n         import subprocess\n+\n         try:\n-            result = subprocess.run([\n-                'ollama', 'run', 'llava:latest', '--image', image_path, prompt\n-            ], capture_output=True, text=True, timeout=60)\n+            result = subprocess.run(\n+                [\"ollama\", \"run\", \"llava:latest\", \"--image\", image_path, prompt],\n+                capture_output=True,\n+                text=True,\n+                timeout=60,\n+            )\n             if result.returncode == 0:\n                 return result.stdout.strip()\n             else:\n                 return f\"[LLaVA erreur: {result.stderr}]\"\n         except Exception as e:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/predictive_cache.py\t2025-07-29 17:56:29.010000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/predictive_cache.py\t2025-07-29 18:12:21.182625+00:00\n@@ -18,21 +18,21 @@\n         self.misses = 0\n \n     def get(self, key: str) -> Optional[Any]:\n         entry = self.cache.get(key)\n         if entry:\n-            if self.ttl and time.time() - entry['time'] > self.ttl:\n+            if self.ttl and time.time() - entry[\"time\"] > self.ttl:\n                 self.invalidate(key)\n                 self.misses += 1\n                 return None\n             self.hits += 1\n-            return entry['value']\n+            return entry[\"value\"]\n         self.misses += 1\n         return None\n \n     def set(self, key: str, value: Any):\n-        self.cache[key] = {'value': value, 'time': time.time()}\n+        self.cache[key] = {\"value\": value, \"time\": time.time()}\n \n     def predict_key(self, context: Dict) -> str:\n         # Hash du contexte pour g\u00e9n\u00e9rer une cl\u00e9 unique\n         return str(hash(str(context)))\n \n@@ -55,10 +55,10 @@\n     def get_stats(self) -> Dict[str, Any]:\n         \"\"\"Retourne les statistiques d'utilisation du cache.\"\"\"\n         total = self.hits + self.misses\n         hit_rate = self.hits / total if total else 0\n         return {\n-            'hits': self.hits,\n-            'misses': self.misses,\n-            'hit_rate': hit_rate,\n-            'size': len(self.cache)\n+            \"hits\": self.hits,\n+            \"misses\": self.misses,\n+            \"hit_rate\": hit_rate,\n+            \"size\": len(self.cache),\n         }\n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_tester.py\t2025-07-29 18:02:53.600000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_tester.py\t2025-07-29 18:12:21.230222+00:00\n@@ -1,6 +1,5 @@\n-\n import os\n from pathlib import Path\n from typing import Dict, List, Any\n import re\n import argparse\n@@ -16,12 +15,11 @@\n \n class AutoTester:\n     \"\"\"G\u00e9n\u00e9rateur de tests pour Athalia\"\"\"\n \n     def __init__(self, project_path: str = None):\n-        self.project_path: Path = Path(\n-            project_path) if project_path else Path('.')\n+        self.project_path: Path = Path(project_path) if project_path else Path(\".\")\n         self.test_results = {}\n         self.generated_tests = []\n \n     def run(self) -> Dict[str, Any]:\n         \"\"\"M\u00e9thode run() pour lorchestrateur - ex\u00e9cute les tests\"\"\"\n@@ -52,11 +50,11 @@\n         return {\n             \"unit_tests\": unit_tests,\n             \"integration_tests\": integration_tests,\n             \"performance_tests\": performance_tests,\n             \"test_results\": test_results,\n-            \"files_created\": self._get_created_files()\n+            \"files_created\": self._get_created_files(),\n         }\n \n     def _analyze_modules(self) -> List[Dict[str, Any]]:\n         \"\"\"Analyse les modules Python du projet\"\"\"\n         modules = []\n@@ -66,50 +64,49 @@\n             if py_file.name.startswith(\"._\"):\n                 continue\n \n             if py_file.name != \"__init__.py\" and \"test\" not in py_file.name.lower():\n                 try:\n-                    with open(py_file, 'r', encoding='utf-8') as file_handle:\n+                    with open(py_file, \"r\", encoding=\"utf-8\") as file_handle:\n                         content = file_handle.read()\n \n                     tree = ast.parse(content)\n                     module_info = {\n-                        'name': py_file.stem,\n-                        'path': str(py_file),\n-                        'classes': [],\n-                        'functions': [],\n-                        'imports': []\n+                        \"name\": py_file.stem,\n+                        \"path\": str(py_file),\n+                        \"classes\": [],\n+                        \"functions\": [],\n+                        \"imports\": [],\n                     }\n \n                     for item in tree.body:\n                         if isinstance(item, ast.ClassDef):\n-                            class_info = {\n-                                'name': item.name,\n-                                'methods': []\n-                            }\n+                            class_info = {\"name\": item.name, \"methods\": []}\n                             for node in item.body:\n                                 if isinstance(node, ast.FunctionDef):\n-                                    class_info['methods'].append(node.name)\n-                            module_info['classes'].append(class_info)\n+                                    class_info[\"methods\"].append(node.name)\n+                            module_info[\"classes\"].append(class_info)\n                         elif isinstance(item, ast.FunctionDef) and not any(\n-                            decorator.id == 'property' if isinstance(decorator, ast.Name) else False\n+                            (\n+                                decorator.id == \"property\"\n+                                if isinstance(decorator, ast.Name)\n+                                else False\n+                            )\n                             for decorator in (item.decorator_list or [])\n                         ):\n-                            module_info['functions'].append(item.name)\n+                            module_info[\"functions\"].append(item.name)\n                         elif isinstance(item, (ast.Import, ast.ImportFrom)):\n                             if isinstance(item, ast.Import):\n                                 for alias in item.names:\n-                                    module_info['imports'].append(alias.name)\n+                                    module_info[\"imports\"].append(alias.name)\n                             else:\n-                                module_info['imports'].append(\n-                                    item.module or '')\n+                                module_info[\"imports\"].append(item.module or \"\")\n \n                     modules.append(module_info)\n \n                 except Exception as e:\n-                    logger.warning(\n-                        f\"Erreur lors de l'analyse de {py_file}: {e}\")\n+                    logger.warning(f\"Erreur lors de l'analyse de {py_file}: {e}\")\n                     continue\n \n         return modules\n \n     def _generate_unit_tests(self, modules: List[Dict[str, Any]]) -> List[str]:\n@@ -141,11 +138,13 @@\n         pass\n \n     def tearDown(self):\n         \\\"\\\"\\\"Nettoyage apr\u00e8s chaque test\\\"\\\"\\\"\n         pass\n-\"\"\".format(module_name=module[\"name\"].title())\n+\"\"\".format(\n+            module_name=module[\"name\"].title()\n+        )\n \n         # Tests pour les classes\n         for class_info in module[\"classes\"]:\n             test_content += \"\"\"\n     def test_{class_name}_creation(self):\n@@ -156,11 +155,13 @@\n             class_obj = getattr(module, '{class_name}')\n             instance = class_obj()\n             self.assertIsNotNone(instance)\n         except Exception as e:\n             self.skipTest(f\"Impossible de cr\u00e9er {class_name}: {{e}}\")\n-\"\"\".format(class_name=class_info[\"name\"], module_name=module[\"name\"])\n+\"\"\".format(\n+                class_name=class_info[\"name\"], module_name=module[\"name\"]\n+            )\n \n             # Tests pour les m\u00e9thodes\n             for method_name in class_info[\"methods\"]:\n                 if method_name not in [\"__init__\", \"__str__\", \"__repr__\"]:\n                     test_content += \"\"\"\n@@ -176,11 +177,15 @@\n             result = method()\n             # TODO: Ajouter des assertions appropri\u00e9es\n             self.assertIsNotNone(result)\n         except Exception as e:\n             self.skipTest(f\"Impossible de tester {method_name}: {{e}}\")\n-\"\"\".format(class_name=class_info[\"name\"], method_name=method_name, module_name=module[\"name\"])\n+\"\"\".format(\n+                        class_name=class_info[\"name\"],\n+                        method_name=method_name,\n+                        module_name=module[\"name\"],\n+                    )\n \n         # Tests pour les fonctions\n         for func_name in module[\"functions\"]:\n             test_content += \"\"\"\"\n     def test_{func_name}(self):\n@@ -193,22 +198,23 @@\n             result = func()\n             # TODO: Ajouter des assertions appropri\u00e9es\n             self.assertIsNotNone(result)\n         except Exception as e:\n             self.skipTest(f\"Impossible de tester {func_name}: {{e}}\")\n-\"\"\".format(func_name=func_name, module_name=module[\"name\"])\n+\"\"\".format(\n+                func_name=func_name, module_name=module[\"name\"]\n+            )\n \n         test_content += \"\"\"\n \n if __name__ == \"__main__\":\n     unittest.main()\n \"\"\"\n \n         return test_content\n \n-    def _generate_integration_tests(\n-            self, modules: List[Dict[str, Any]]) -> List[str]:\n+    def _generate_integration_tests(self, modules: List[Dict[str, Any]]) -> List[str]:\n         \"\"\"G\u00e9n\u00e8re les tests dint\u00e9gration\"\"\"\n         integration_tests = []\n \n         # Test dint\u00e9gration principal\n         integration_content = \"\"\"#!/usr/bin/env python3\"\n@@ -267,12 +273,11 @@\n \n         integration_tests.append(integration_content)\n \n         return integration_tests\n \n-    def _generate_performance_tests(\n-            self, modules: List[Dict[str, Any]]) -> List[str]:\n+    def _generate_performance_tests(self, modules: List[Dict[str, Any]]) -> List[str]:\n         \"\"\"G\u00e9n\u00e8re les tests de performance\"\"\"\n         performance_tests = []\n \n         # Test de performance principal\n         performance_content = \"\"\"#!/usr/bin/env python3\"\n@@ -348,36 +353,37 @@\n         performance_tests.append(performance_content)\n \n         return performance_tests\n \n     def _save_tests(\n-            self,\n-            unit_tests: List[str],\n-            integration_tests: List[str],\n-            performance_tests: List[str]):\n+        self,\n+        unit_tests: List[str],\n+        integration_tests: List[str],\n+        performance_tests: List[str],\n+    ):\n         \"\"\"Sauvegarde les tests f\"\"\"\n         tests_dir = self.project_path / \"tests\"\n         tests_dir.mkdir(exist_ok=True)\n \n         # Tests unitaires\n         for index, test_content in enumerate(unit_tests):\n             test_file = tests_dir / f\"auto_generated_unit_{index + 1}.py\"\n-            with open(test_file, 'w', encoding='utf-8') as file_handle:\n+            with open(test_file, \"w\", encoding=\"utf-8\") as file_handle:\n                 file_handle.write(test_content)\n             self.generated_tests.append(str(test_file))\n \n         # Tests d'int\u00e9gration\n         for index, test_content in enumerate(integration_tests):\n             test_file = tests_dir / f\"auto_generated_integration_{index + 1}.py\"\n-            with open(test_file, 'w', encoding='utf-8') as file_handle:\n+            with open(test_file, \"w\", encoding=\"utf-8\") as file_handle:\n                 file_handle.write(test_content)\n             self.generated_tests.append(str(test_file))\n \n         # Tests de performance\n         for index, test_content in enumerate(performance_tests):\n             test_file = tests_dir / f\"auto_generated_performance_{index + 1}.py\"\n-            with open(test_file, 'w', encoding='utf-8') as file_handle:\n+            with open(test_file, \"w\", encoding=\"utf-8\") as file_handle:\n                 file_handle.write(test_content)\n             self.generated_tests.append(str(test_file))\n \n         # Fichier de configuration pytest\n         pytest_config = \"\"\"[tool.pytest.ini_options]\n@@ -397,11 +403,11 @@\n     \"performance: marks tests as performance tests\"\n )\n \"\"\"\n \n         pytest_file = self.project_path / \"pytest.ini\"\n-        with open(pytest_file, 'w', encoding='utf-8') as file_handle:\n+        with open(pytest_file, \"w\", encoding=\"utf-8\") as file_handle:\n             file_handle.write(pytest_config)\n \n         # Script de lancement des tests\n         run_tests_script = \"\"\"#!/usr/bin/env bash\n # Script de lancement des tests pour {self.project_path.name}\n@@ -431,11 +437,11 @@\n         # Cr\u00e9er le dossier scripts s'il nexiste pas\n         scripts_dir = self.project_path / \"scripts\"\n         scripts_dir.mkdir(exist_ok=True)\n \n         run_tests_file = scripts_dir / \"run_tests.sh\"\n-        with open(run_tests_file, 'w', encoding='utf-8') as file_handle:\n+        with open(run_tests_file, \"w\", encoding=\"utf-8\") as file_handle:\n             file_handle.write(run_tests_script)\n \n         # Rendre le script ex\u00e9cutable\n         os.chmod(run_tests_file, 0o755)\n \n@@ -445,11 +451,11 @@\n \n         # Supprimer les fichiers de tests auto-g\u00e9n\u00e9r\u00e9s\n         test_patterns = [\n             \"tests/auto_generated_unit_*.py\",\n             \"tests/auto_generated_integration_*.py\",\n-            \"tests/auto_generated_performance_*.py\"\n+            \"tests/auto_generated_performance_*.py\",\n         ]\n \n         for pattern in test_patterns:\n             for test_file in self.project_path.glob(pattern):\n                 try:\n@@ -469,11 +475,11 @@\n     def _run_tests(self) -> Dict[str, Any]:\n         \"\"\"Ex\u00e9cute les tests g\u00e9n\u00e9r\u00e9s et collecte les r\u00e9sultats\"\"\"\n         results = {\n             \"unit_tests\": {\"passed\": 0, \"failed\": 0, \"errors\": []},\n             \"integration_tests\": {\"passed\": 0, \"failed\": 0, \"errors\": []},\n-            \"performance_tests\": {\"passed\": 0, \"failed\": 0, \"errors\": []}\n+            \"performance_tests\": {\"passed\": 0, \"failed\": 0, \"errors\": []},\n         }\n \n         try:\n             # Changer vers le r\u00e9pertoire du projet\n             original_dir = os.getcwd()\n@@ -481,52 +487,67 @@\n \n             # Ex\u00e9cuter les tests unitaires\n             logger.info(\"\ud83e\uddea Ex\u00e9cution des tests unitaires...\")\n             try:\n                 result = subprocess.run(\n-                    [\"python\", \"-m\", \"pytest\", \"tests/auto_generated_unit_*.py\",\n-                     \"-v\", \"--tb=short\"],\n+                    [\n+                        \"python\",\n+                        \"-m\",\n+                        \"pytest\",\n+                        \"tests/auto_generated_unit_*.py\",\n+                        \"-v\",\n+                        \"--tb=short\",\n+                    ],\n                     capture_output=True,\n                     text=True,\n-                    timeout=60\n+                    timeout=60,\n                 )\n \n                 if result.returncode == 0:\n                     results[\"unit_tests\"][\"passed\"] = len(\n-                        re.findall(r\"PASSED\", result.stdout))\n+                        re.findall(r\"PASSED\", result.stdout)\n+                    )\n                 else:\n                     results[\"unit_tests\"][\"failed\"] = len(\n-                        re.findall(r\"FAILED\", result.stdout))\n+                        re.findall(r\"FAILED\", result.stdout)\n+                    )\n                     results[\"unit_tests\"][\"errors\"].append(result.stderr)\n             except subprocess.TimeoutExpired:\n-                results[\"unit_tests\"][\"errors\"].append(\n-                    \"Timeout lors de lex\u00e9cution\")\n+                results[\"unit_tests\"][\"errors\"].append(\"Timeout lors de lex\u00e9cution\")\n             except Exception as e:\n                 results[\"unit_tests\"][\"errors\"].append(str(e))\n \n             # Ex\u00e9cuter les tests dint\u00e9gration\n             logger.info(\"\ud83d\udd17 Ex\u00e9cution des tests dint\u00e9gration...\")\n             try:\n                 result = subprocess.run(\n-                    [\"python\", \"-m\", \"pytest\", \"tests/auto_generated_integration_*.py\",\n-                     \"-v\", \"--tb=short\"],\n+                    [\n+                        \"python\",\n+                        \"-m\",\n+                        \"pytest\",\n+                        \"tests/auto_generated_integration_*.py\",\n+                        \"-v\",\n+                        \"--tb=short\",\n+                    ],\n                     capture_output=True,\n                     text=True,\n-                    timeout=60\n+                    timeout=60,\n                 )\n \n                 if result.returncode == 0:\n                     results[\"integration_tests\"][\"passed\"] = len(\n-                        re.findall(r\"PASSED\", result.stdout))\n+                        re.findall(r\"PASSED\", result.stdout)\n+                    )\n                 else:\n                     results[\"integration_tests\"][\"failed\"] = len(\n-                        re.findall(r\"FAILED\", result.stdout))\n-                    results[\"integration_tests\"][\"errors\"].append(\n-                        result.stderr)\n+                        re.findall(r\"FAILED\", result.stdout)\n+                    )\n+                    results[\"integration_tests\"][\"errors\"].append(result.stderr)\n             except subprocess.TimeoutExpired:\n                 results[\"integration_tests\"][\"errors\"].append(\n-                    \"Timeout lors de lex\u00e9cution\")\n+                    \"Timeout lors de lex\u00e9cution\"\n+                )\n             except Exception as e:\n                 results[\"integration_tests\"][\"errors\"].append(str(e))\n \n             # Retourner au r\u00e9pertoire original\n             os.chdir(original_dir)\n@@ -545,12 +566,18 @@\n         return results\n \n     def _get_created_files(self) -> List[str]:\n         \"\"\"Retourne la liste des fichiers cr\u00e9\u00e9s\"\"\"\n         files = [\"pytest.ini\", \"scripts/run_tests.sh\"] + self.generated_tests\n-        return [str(self.project_path / file_handle) if not file_handle.startswith(\n-            str(self.project_path)) else file_handle for file_handle in files]\n+        return [\n+            (\n+                str(self.project_path / file_handle)\n+                if not file_handle.startswith(str(self.project_path))\n+                else file_handle\n+            )\n+            for file_handle in files\n+        ]\n \n     def generate_test_report(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un rapport de tests\"\"\"\n         report = \"\"\"\n {sep}\n@@ -575,23 +602,30 @@\n    \u2022 Erreurs: {perf_errors}\n \n \ud83d\udcc4 FICHIERS CR\u00c9\u00c9S ({num_files}):\n \"\"\"\n         report = report.format(\n-            sep='=' * 60,\n+            sep=\"=\" * 60,\n             project_name=self.project_path.name,\n-            unit_passed=self.test_results.get('unit_tests', {}).get('passed', 0),\n-            unit_failed=self.test_results.get('unit_tests', {}).get('failed', 0),\n-            unit_errors=len(self.test_results.get('unit_tests', {}).get('errors', [])),\n-            integration_passed=self.test_results.get('integration_tests', {}).get('passed', 0),\n-            integration_failed=self.test_results.get('integration_tests', {}).get('failed', 0),\n-            integration_errors=len(self.test_results.get(\n-                'integration_tests', {}).get('errors', [])),\n-            perf_passed=self.test_results.get('performance_tests', {}).get('passed', 0),\n-            perf_failed=self.test_results.get('performance_tests', {}).get('failed', 0),\n-            perf_errors=len(self.test_results.get('performance_tests', {}).get('errors', [])),\n-            num_files=len(self.generated_tests)\n+            unit_passed=self.test_results.get(\"unit_tests\", {}).get(\"passed\", 0),\n+            unit_failed=self.test_results.get(\"unit_tests\", {}).get(\"failed\", 0),\n+            unit_errors=len(self.test_results.get(\"unit_tests\", {}).get(\"errors\", [])),\n+            integration_passed=self.test_results.get(\"integration_tests\", {}).get(\n+                \"passed\", 0\n+            ),\n+            integration_failed=self.test_results.get(\"integration_tests\", {}).get(\n+                \"failed\", 0\n+            ),\n+            integration_errors=len(\n+                self.test_results.get(\"integration_tests\", {}).get(\"errors\", [])\n+            ),\n+            perf_passed=self.test_results.get(\"performance_tests\", {}).get(\"passed\", 0),\n+            perf_failed=self.test_results.get(\"performance_tests\", {}).get(\"failed\", 0),\n+            perf_errors=len(\n+                self.test_results.get(\"performance_tests\", {}).get(\"errors\", [])\n+            ),\n+            num_files=len(self.generated_tests),\n         )\n         for test_file in self.generated_tests:\n             report += f\"   \u2022 {test_file}\\n\"\n         report += \"\"\"\n \ud83d\ude80 POUR LANCER LES TESTS:\n@@ -606,25 +640,23 @@\n python -m pytest tests/ -v\n ```\n \n {sep2}\n \"\"\".format(\n-            project_path=self.project_path,\n-            sep2='=' * 60\n+            project_path=self.project_path, sep2=\"=\" * 60\n         )\n         return report\n \n \n def main():\n     \"\"\"Point dentr\u00e9e principal\"\"\"\n \n     parser = argparse.ArgumentParser(description=\"G\u00e9n\u00e9ration automatique de tests\")\n     parser.add_argument(\"project_path\", help=\"Chemin du projet \u00e0 tester\")\n     parser.add_argument(\n-        \"--run\",\n-        action=\"store_true\",\n-        help=\"Ex\u00e9cuter les tests apr\u00e8s g\u00e9n\u00e9ration\")\n+        \"--run\", action=\"store_true\", help=\"Ex\u00e9cuter les tests apr\u00e8s g\u00e9n\u00e9ration\"\n+    )\n \n     args = parser.parse_args()\n \n     if not os.path.exists(args.project_path):\n         logger.info(f\"\u274c Le chemin {args.project_path} nexiste pas\")\n@@ -633,16 +665,16 @@\n     tester = AutoTester()\n     result = tester.generate_tests(args.project_path)\n \n     logger.info(\"\u2705 Tests g\u00e9n\u00e9r\u00e9s avec succ\u00e8s !\")\n     logger.info(f\"\\n\ud83d\udcc4 Fichiers cr\u00e9\u00e9s ({len(result['files_created'])}):\")\n-    for file_path in result['files_created']:\n+    for file_path in result[\"files_created\"]:\n         logger.info(f\"   \u2022 {file_path}\")\n \n     if args.run:\n         logger.info(\"\\n\ud83e\uddea Ex\u00e9cution des tests...\")\n-        tester.test_results = result['test_results']\n+        tester.test_results = result[\"test_results\"]\n         logger.info(tester.generate_test_report())\n \n \n if __name__ == \"__main__\":\n     main()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/response_distiller.py\t2025-07-29 17:56:29.220000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/response_distiller.py\t2025-07-29 18:12:21.286138+00:00\n@@ -8,106 +8,99 @@\n from collections import Counter\n import random\n \n \n class ResponseDistiller:\n-    def __init__(self, strategy: str = 'voting'):\n+    def __init__(self, strategy: str = \"voting\"):\n         self.strategy = strategy\n \n-    def distill(self,\n-                responses: List[str],\n-                context: Optional[Dict[str,\n-                                       Any]] = None) -> str:\n+    def distill(\n+        self, responses: List[str], context: Optional[Dict[str, Any]] = None\n+    ) -> str:\n         \"\"\"\n         Fusionne plusieurs r\u00e9ponses IA selon la strat\u00e9gie choisie.\n         :param responses: Liste de r\u00e9ponses IA (str)\n         :param context: Contexte optionnel (pour scoring avanc\u00e9)\n         :return: R\u00e9ponse distill\u00e9e (str)\n         \"\"\"\n-        if self.strategy == 'voting':\n+        if self.strategy == \"voting\":\n             return self.majority_voting(responses)\n-        elif self.strategy == 'stacking':\n+        elif self.strategy == \"stacking\":\n             return self.stacking(responses, context)\n-        elif self.strategy == 'bagging':\n+        elif self.strategy == \"bagging\":\n             return self.bagging(responses)\n-        elif self.strategy == 'consensus':\n+        elif self.strategy == \"consensus\":\n             return self.consensus_scoring(responses)\n-        elif self.strategy == 'creative':\n+        elif self.strategy == \"creative\":\n             return self.creative_fusion(responses)\n-        return responses[0] if responses else ''  # fallback\n+        return responses[0] if responses else \"\"  # fallback\n \n     def majority_voting(self, responses: List[str]) -> str:\n         \"\"\"Retourne la r\u00e9ponse la plus fr\u00e9quente (majorit\u00e9).\"\"\"\n         if not responses:\n-            return ''\n+            return \"\"\n         counter = Counter(responses)\n         return counter.most_common(1)[0][0]\n \n-    def stacking(self,\n-                 responses: List[str],\n-                 context: Optional[Dict[str,\n-                                        Any]] = None) -> str:\n+    def stacking(\n+        self, responses: List[str], context: Optional[Dict[str, Any]] = None\n+    ) -> str:\n         \"\"\"Concat\u00e8ne les parties communes, puis les parties uniques.\"\"\"\n         if not responses:\n-            return ''\n+            return \"\"\n         words = [set(r.split()) for r in responses]\n         common = set.intersection(*words) if words else set()\n         unique = set.union(*words) - common\n-        return ' '.join(sorted(common)) + ' | ' + ' '.join(sorted(unique))\n+        return \" \".join(sorted(common)) + \" | \" + \" \".join(sorted(unique))\n \n     def bagging(self, responses: List[str]) -> str:\n         \"\"\"Retourne une r\u00e9ponse al\u00e9atoire parmi les plus fr\u00e9quentes (bagging).\"\"\"\n         if not responses:\n-            return ''\n+            return \"\"\n         counter = Counter(responses)\n         top_count = counter.most_common(1)[0][1]\n-        top_responses = [\n-            resp for resp,\n-            count in counter.items() if count == top_count]\n+        top_responses = [resp for resp, count in counter.items() if count == top_count]\n         return random.choice(top_responses)\n \n     def consensus_scoring(self, responses: List[str]) -> str:\n         \"\"\"Retourne la plus longue sous-cha\u00eene commune ET les parties divergentes.\"\"\"\n         if not responses:\n-            return ''\n+            return \"\"\n         from difflib import SequenceMatcher\n \n         def lcs(a, b):\n-            match = SequenceMatcher(\n-                None, a, b).find_longest_match(\n-                0, len(a), 0, len(b))\n-            return a[match.a: match.a + match.size]\n+            match = SequenceMatcher(None, a, b).find_longest_match(0, len(a), 0, len(b))\n+            return a[match.a : match.a + match.size]\n+\n         consensus = responses[0]\n         for r in responses[1:]:\n             consensus = lcs(consensus, r)\n         divergents = []\n         for r in responses:\n             if consensus not in r:\n                 divergents.append(r)\n         if consensus and len(consensus) > 2:\n-            return (f\"Consensus: {consensus} | Divergents: \"\n-                    f\"{' || '.join(divergents) if divergents else 'Aucun'}\")\n+            return (\n+                f\"Consensus: {consensus} | Divergents: \"\n+                f\"{' || '.join(divergents) if divergents else 'Aucun'}\"\n+            )\n         return self.majority_voting(responses)\n \n     def creative_fusion(self, responses: List[str]) -> str:\n         \"\"\"\n         Fusion cr\u00e9ative : m\u00e9lange de fragments, ajout d'un tag IA, et concat unique.\n         \"\"\"\n         if not responses:\n-            return ''\n+            return \"\"\n         fragments = set()\n         for r in responses:\n             fragments.update(r.split())\n-        fusion = ' '.join(\n-            random.sample(\n-                list(fragments), min(\n-                    len(fragments), 10)))\n+        fusion = \" \".join(random.sample(list(fragments), min(len(fragments), 10)))\n         return f\"[Fusion IA] {fusion}\"\n \n \n def distill_responses(\n-        responses: list,\n-        strategy: str = 'voting',\n-        context: Optional[dict] = None) -> str:\n+    responses: list, strategy: str = \"voting\", context: Optional[dict] = None\n+) -> str:\n     \"\"\"Fonction utilitaire pour distiller une liste de r\u00e9ponses IA.\"\"\"\n     distiller = ResponseDistiller(strategy=strategy)\n     return distiller.distill(responses, context)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_documenter.py\t2025-07-29 18:02:53.600000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_documenter.py\t2025-07-29 18:12:21.315015+00:00\n@@ -22,13 +22,12 @@\n     \"\"\"G\u00e9n\u00e9rateur de documentation automatique\"\"\"\n \n     project_info: Dict[str, Any]\n     api_docs: Dict[str, Any]\n \n-    def __init__(self, project_path: Optional[str] = None, lang: str = 'fr'):\n-        self.project_path: Path = Path(\n-            project_path) if project_path else Path('.')\n+    def __init__(self, project_path: Optional[str] = None, lang: str = \"fr\"):\n+        self.project_path: Path = Path(project_path) if project_path else Path(\".\")\n         self.project_info = {}\n         self.api_docs = {}\n         self.readme_content = \"\"\n         self.lang = lang\n         self.translations = self._load_translations(lang)\n@@ -50,11 +49,12 @@\n         \"\"\"Documentation compl\u00e8te dun projet\"\"\"\n         self.project_path = Path(project_path)\n \n         logger.info(\n             f\"\ud83d\udcda {self.translations.get('doc_generation', 'G\u00e9n\u00e9ration de documentation pour')} : \"\n-            f\"{self.project_path.name}\")\n+            f\"{self.project_path.name}\"\n+        )\n \n         # Analyse du projet\n         self._analyze_project()\n \n         # G\u00e9n\u00e9ration des documents\n@@ -69,11 +69,11 @@\n         return {\n             \"readme\": readme,\n             \"api_docs\": api_docs,\n             \"setup_guide\": setup_guide,\n             \"usage_guide\": usage_guide,\n-            \"created_files\": self._get_created_files()\n+            \"created_files\": self._get_created_files(),\n         }\n \n     def _analyze_project(self):\n         \"\"\"Analyse du projet pour la documentation\"\"\"\n         if not self.project_path:\n@@ -90,37 +90,37 @@\n             \"license\": self._extract_license(),\n             \"dependencies\": self._extract_dependencies(),\n             \"entry_points\": self._find_entry_points(),\n             \"modules\": modules,\n             \"classes\": self._analyze_classes(modules),\n-            \"functions\": self._analyze_functions(modules)\n+            \"functions\": self._analyze_functions(modules),\n         }\n \n     def _extract_description(self) -> str:\n         \"\"\"Extrait la description du projet\"\"\"\n         # Chercher dans README existant\n         readme_files = list(self.project_path.glob(\"README*\"))\n         if readme_files:\n             try:\n-                with open(readme_files[0], 'r', encoding='utf-8') as f:\n+                with open(readme_files[0], \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n                     # Extraire la premi\u00e8re ligne non vide\n-                    lines = [line.strip()\n-                             for line in content.split('\\n') if line.strip()]\n+                    lines = [\n+                        line.strip() for line in content.split(\"\\n\") if line.strip()\n+                    ]\n                     if lines:\n                         return lines[0]\n             except Exception:\n                 pass\n \n         # Chercher dans setup.py ou pyproject.toml\n         setup_file = self.project_path / \"setup.py\"\n         if setup_file.exists():\n             try:\n-                with open(setup_file, 'r', encoding='utf-8') as f:\n+                with open(setup_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                    match = re.search(\n-                        r'description\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n+                    match = re.search(r'description\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n                     if match:\n                         return match.group(1)\n             except Exception:\n                 pass\n \n@@ -130,27 +130,25 @@\n         \"\"\"Extrait la version du projet\"\"\"\n         # Chercher dans __init__.py\n         init_file = self.project_path / \"__init__.py\"\n         if init_file.exists():\n             try:\n-                with open(init_file, 'r', encoding='utf-8') as f:\n+                with open(init_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                    match = re.search(\n-                        r'__version__\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n+                    match = re.search(r'__version__\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n                     if match:\n                         return match.group(1)\n             except Exception:\n                 pass\n \n         # Chercher dans setup.py\n         setup_file = self.project_path / \"setup.py\"\n         if setup_file.exists():\n             try:\n-                with open(setup_file, 'r', encoding='utf-8') as f:\n+                with open(setup_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                    match = re.search(\n-                        r'version\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n+                    match = re.search(r'version\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n                     if match:\n                         return match.group(1)\n             except Exception:\n                 pass\n \n@@ -160,14 +158,13 @@\n         \"\"\"Extrait lauteur du projet\"\"\"\n         # Chercher dans setup.py\n         setup_file = self.project_path / \"setup.py\"\n         if setup_file.exists():\n             try:\n-                with open(setup_file, 'r', encoding='utf-8') as f:\n+                with open(setup_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                    match = re.search(\n-                        r'author\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n+                    match = re.search(r'author\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', content)\n                     if match:\n                         return match.group(1)\n             except Exception:\n                 pass\n \n@@ -187,26 +184,29 @@\n \n         # Python\n         req_file = self.project_path / \"requirements.txt\"\n         if req_file.exists():\n             try:\n-                with open(req_file, 'r') as f:\n-                    deps = [line.strip() for line in f if line.strip()\n-                            and not line.startswith('#')]\n-                    dependencies['python'] = deps\n+                with open(req_file, \"r\") as f:\n+                    deps = [\n+                        line.strip()\n+                        for line in f\n+                        if line.strip() and not line.startswith(\"#\")\n+                    ]\n+                    dependencies[\"python\"] = deps\n             except Exception:\n                 pass\n \n         # Node.js\n         package_file = self.project_path / \"package.json\"\n         if package_file.exists():\n             try:\n-                with open(package_file, 'r') as f:\n+                with open(package_file, \"r\") as f:\n                     data = json.load(f)\n-                    deps = list(data.get('dependencies', {}).keys())\n-                    dev_deps = list(data.get('devDependencies', {}).keys())\n-                    dependencies['nodejs'] = deps + dev_deps\n+                    deps = list(data.get(\"dependencies\", {}).keys())\n+                    dev_deps = list(data.get(\"devDependencies\", {}).keys())\n+                    dependencies[\"nodejs\"] = deps + dev_deps\n             except Exception:\n                 pass\n \n         return dependencies\n \n@@ -223,17 +223,19 @@\n \n         # Chercher dans setup.py\n         setup_file = self.project_path / \"setup.py\"\n         if setup_file.exists():\n             try:\n-                with open(setup_file, 'r', encoding='utf-8') as f:\n+                with open(setup_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n                     matches = re.findall(\n-                        r'entry_points.*?\\[(.*?)\\]', content, re.DOTALL)\n+                        r\"entry_points.*?\\[(.*?)\\]\", content, re.DOTALL\n+                    )\n                     for match in matches:\n                         entry_points.extend(\n-                            [ep.strip() for ep in match.split(',') if ep.strip()])\n+                            [ep.strip() for ep in match.split(\",\") if ep.strip()]\n+                        )\n             except Exception:\n                 pass\n \n         return entry_points\n \n@@ -241,74 +243,82 @@\n         \"\"\"Analyse les modules du projet\"\"\"\n         modules = []\n         for py_file in self.project_path.rglob(\"*.py\"):\n             if py_file.name != \"__init__.py\":\n                 try:\n-                    with open(py_file, 'r', encoding='utf-8') as f:\n+                    with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                         tree = ast.parse(f.read())\n \n                     module_info = {\n                         \"name\": py_file.stem,\n                         \"docstring\": ast.get_docstring(tree) or \"\",\n                         \"classes\": [],  # Initialis\u00e9 comme liste mutable\n-                        \"functions\": []  # Initialis\u00e9 comme liste mutable\n+                        \"functions\": [],  # Initialis\u00e9 comme liste mutable\n                     }\n \n                     for node in ast.walk(tree):\n                         if isinstance(node, ast.ClassDef):\n-                            module_info[\"classes\"].append({\n-                                \"name\": node.name,\n-                                \"docstring\": ast.get_docstring(node) or \"\",\n-                                \"methods\": [\n-                                    m.name for m in node.body\n-                                    if isinstance(m, ast.FunctionDef)\n-                                ]\n-                            })\n+                            module_info[\"classes\"].append(\n+                                {\n+                                    \"name\": node.name,\n+                                    \"docstring\": ast.get_docstring(node) or \"\",\n+                                    \"methods\": [\n+                                        m.name\n+                                        for m in node.body\n+                                        if isinstance(m, ast.FunctionDef)\n+                                    ],\n+                                }\n+                            )\n                         elif isinstance(node, ast.FunctionDef):\n-                            module_info[\"functions\"].append({\n-                                \"name\": node.name,\n-                                \"docstring\": ast.get_docstring(node) or \"\",\n-                                \"args\": [\n-                                    arg.arg for arg in node.args.args\n-                                    if arg.arg != 'self'\n-                                ]\n-                            })\n+                            module_info[\"functions\"].append(\n+                                {\n+                                    \"name\": node.name,\n+                                    \"docstring\": ast.get_docstring(node) or \"\",\n+                                    \"args\": [\n+                                        arg.arg\n+                                        for arg in node.args.args\n+                                        if arg.arg != \"self\"\n+                                    ],\n+                                }\n+                            )\n \n                     modules.append(module_info)\n                 except Exception as e:\n                     logger.warning(f\"Could not analyze module {py_file}: {e}\")\n         return modules\n \n-    def _analyze_classes(\n-            self, modules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n+    def _analyze_classes(self, modules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n         \"\"\"Analyse les classes du projet\"\"\"\n         classes = []\n \n         for module in modules:\n             for class_info in module[\"classes\"]:\n-                classes.append({\n-                    \"name\": class_info[\"name\"],\n-                    \"module\": module[\"name\"],\n-                    \"docstring\": class_info[\"docstring\"],\n-                    \"methods\": class_info[\"methods\"]\n-                })\n+                classes.append(\n+                    {\n+                        \"name\": class_info[\"name\"],\n+                        \"module\": module[\"name\"],\n+                        \"docstring\": class_info[\"docstring\"],\n+                        \"methods\": class_info[\"methods\"],\n+                    }\n+                )\n \n         return classes\n \n-    def _analyze_functions(\n-            self, modules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n+    def _analyze_functions(self, modules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n         \"\"\"Analyse les fonctions du projet\"\"\"\n         functions = []\n \n         for module in modules:\n             for func_info in module[\"functions\"]:\n-                functions.append({\n-                    \"name\": func_info[\"name\"],\n-                    \"module\": module[\"name\"],\n-                    \"docstring\": func_info[\"docstring\"],\n-                    \"args\": func_info[\"args\"]\n-                })\n+                functions.append(\n+                    {\n+                        \"name\": func_info[\"name\"],\n+                        \"module\": module[\"name\"],\n+                        \"docstring\": func_info[\"docstring\"],\n+                        \"args\": func_info[\"args\"],\n+                    }\n+                )\n \n         return functions\n \n     def _generate_readme(self) -> str:\n         \"\"\"G\u00e9n\u00e8re le README du projet\"\"\"\n@@ -331,33 +341,19 @@\n ## \ud83d\ude80 {installation}\n \n ### {prerequisites}\n \"\"\"\n         readme = readme.format(\n-            project_name=self.project_info['name'],\n-            description=self.project_info['description'],\n-            table_of_contents=t.get(\n-                'table_of_contents',\n-                '\ud83d\udccb Table des mati\u00e8res'),\n-            installation=t.get(\n-                'installation',\n-                'Installation'),\n-            usage=t.get(\n-                'usage',\n-                'Utilisation'),\n-            tests=t.get(\n-                'tests',\n-                'Tests'),\n-            contribution=t.get(\n-                'contribution',\n-                'Contribution'),\n-            license=t.get(\n-                'license',\n-                'Licence'),\n-            prerequisites=t.get(\n-                'prerequisites',\n-                'Pr\u00e9requis')\n+            project_name=self.project_info[\"name\"],\n+            description=self.project_info[\"description\"],\n+            table_of_contents=t.get(\"table_of_contents\", \"\ud83d\udccb Table des mati\u00e8res\"),\n+            installation=t.get(\"installation\", \"Installation\"),\n+            usage=t.get(\"usage\", \"Utilisation\"),\n+            tests=t.get(\"tests\", \"Tests\"),\n+            contribution=t.get(\"contribution\", \"Contribution\"),\n+            license=t.get(\"license\", \"Licence\"),\n+            prerequisites=t.get(\"prerequisites\", \"Pr\u00e9requis\"),\n         )\n \n         # D\u00e9pendances\n         if self.project_info[\"dependencies\"].get(\"python\"):\n             readme += \"**Python :**\\n\"\n@@ -381,13 +377,13 @@\n pip install -r requirements.txt\n ```\n \n ## \ud83d\udcbb {usage}\n \"\"\".format(\n-            installation_step=t.get('installation_step', 'Installation'),\n-            project_name=self.project_info['name'],\n-            usage=t.get('usage', 'Utilisation')\n+            installation_step=t.get(\"installation_step\", \"Installation\"),\n+            project_name=self.project_info[\"name\"],\n+            usage=t.get(\"usage\", \"Utilisation\"),\n         )\n \n         # Points dentr\u00e9e\n         if self.project_info[\"entry_points\"]:\n             readme += \"### Lancement\\n\\n\"\n@@ -445,18 +441,17 @@\n \n {license_content}\n \n ---\n \"\"\".format(\n-            tests=t.get('tests', 'Tests'),\n-            project_name=self.project_info['name'],\n-            contribution=t.get('contribution', 'Contribution'),\n-            license=t.get('license', 'Licence'),\n-            license_content=self.project_info['license']\n-        )\n-        readme += \"*G\u00e9n\u00e9r\u00e9 automatiquement par Athalia* - {}\\n\".format(\n-            current_date)\n+            tests=t.get(\"tests\", \"Tests\"),\n+            project_name=self.project_info[\"name\"],\n+            contribution=t.get(\"contribution\", \"Contribution\"),\n+            license=t.get(\"license\", \"Licence\"),\n+            license_content=self.project_info[\"license\"],\n+        )\n+        readme += \"*G\u00e9n\u00e9r\u00e9 automatiquement par Athalia* - {}\\n\".format(current_date)\n         return readme\n \n     def _generate_api_documentation(self) -> str:\n         \"\"\"G\u00e9n\u00e8re la documentation API du projet\"\"\"\n         t = self.translations\n@@ -560,15 +555,16 @@\n python main.py\n ```\n \n ---\n \"\"\".format(\n-            setup_guide=t.get('setup_guide', 'Guide d\\'installation'),\n-            project_name=self.project_info['name']\n+            setup_guide=t.get(\"setup_guide\", \"Guide d'installation\"),\n+            project_name=self.project_info[\"name\"],\n         )\n         setup_guide += \"*G\u00e9n\u00e9r\u00e9 automatiquement par Athalia* - {}\\n\".format(\n-            current_date)\n+            current_date\n+        )\n         return setup_guide\n \n     def _generate_usage_guide(self) -> str:\n         \"\"\"G\u00e9n\u00e8re le guide dutilisation du projet\"\"\"\n         t = self.translations\n@@ -615,14 +611,14 @@\n ```\n \n ## Fonctionnalit\u00e9s principales\n \n \"\"\".format(\n-            usage_guide=t.get('usage_guide', 'Guide d\\'utilisation'),\n-            project_name=self.project_info['name'],\n-            version=self.project_info['version'],\n-            description=self.project_info['description']\n+            usage_guide=t.get(\"usage_guide\", \"Guide d'utilisation\"),\n+            project_name=self.project_info[\"name\"],\n+            version=self.project_info[\"version\"],\n+            description=self.project_info[\"description\"],\n         )\n \n         # D\u00e9crire les classes principales\n         if self.project_info[\"classes\"]:\n             usage_guide += \"### Classes principales\\n\\n\"\n@@ -630,11 +626,13 @@\n                 usage_guide += f\"#### {class_info['name']}\\n\\n\"\n                 if class_info[\"docstring\"]:\n                     usage_guide += f\"{class_info['docstring']}\\n\\n\"\n                 usage_guide += \"**Exemple dutilisation :**\\n\\n\"\n                 usage_guide += \"```python\\n\"\n-                usage_guide += f\"from {self.project_info['name']} import {class_info['name']}\\n\\n\"\n+                usage_guide += (\n+                    f\"from {self.project_info['name']} import {class_info['name']}\\n\\n\"\n+                )\n                 usage_guide += \"# Cr\u00e9er une instance\\n\"\n                 usage_guide += f\"instance = {class_info['name']}()\\n\"\n                 if class_info[\"methods\"]:\n                     usage_guide += (\n                         f\"# Utiliser une m\u00e9thode\\n\"\n@@ -649,14 +647,15 @@\n                 usage_guide += f\"#### {func_info['name']}\\n\\n\"\n                 if func_info[\"docstring\"]:\n                     usage_guide += f\"{func_info['docstring']}\\n\\n\"\n                 usage_guide += \"**Exemple dutilisation :**\\n\\n\"\n                 usage_guide += \"```python\\n\"\n-                usage_guide += f\"from {self.project_info['name']} import {func_info['name']}\\n\\n\"\n+                usage_guide += (\n+                    f\"from {self.project_info['name']} import {func_info['name']}\\n\\n\"\n+                )\n                 if func_info[\"args\"]:\n-                    args_str = \", \".join(\n-                        (f\"{arg}\" for arg in func_info[\"args\"]))\n+                    args_str = \", \".join((f\"{arg}\" for arg in func_info[\"args\"]))\n                     usage_guide += f\"result = {func_info['name']}({args_str})\\n\"\n                 else:\n                     usage_guide += f\"result = {func_info['name']}()\\n\"\n                 usage_guide += \"```\\n\\n\"\n \n@@ -704,60 +703,56 @@\n - Contact : support@example.com\n \n ---\n \"\"\"\n         usage_guide += \"*G\u00e9n\u00e9r\u00e9 automatiquement par Athalia* - {}\\n\".format(\n-            current_date)\n+            current_date\n+        )\n         return usage_guide\n \n     def _save_documents(\n-            self,\n-            readme: str,\n-            api_docs: str,\n-            setup_guide: str,\n-            usage_guide: str):\n+        self, readme: str, api_docs: str, setup_guide: str, usage_guide: str\n+    ):\n         \"\"\"Sauvegarde les documents g\u00e9n\u00e9r\u00e9s\"\"\"\n-        current_date = datetime.now().strftime('%Y-%m-%d')\n+        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n         t = self.translations\n \n         docs_dir = self.project_path / \"docs\"\n         docs_dir.mkdir(exist_ok=True)\n \n         # README principal\n         readme_file = self.project_path / \"README.md\"\n-        with open(readme_file, 'w', encoding='utf-8') as f:\n+        with open(readme_file, \"w\", encoding=\"utf-8\") as f:\n             f.write(readme)\n \n         # Documentation API\n         api_file = docs_dir / \"API.md\"\n-        with open(api_file, 'w', encoding='utf-8') as f:\n+        with open(api_file, \"w\", encoding=\"utf-8\") as f:\n             f.write(api_docs)\n \n         # Guide dinstallation\n         setup_file = docs_dir / \"INSTALLATION.md\"\n-        with open(setup_file, 'w', encoding='utf-8') as f:\n+        with open(setup_file, \"w\", encoding=\"utf-8\") as f:\n             f.write(setup_guide)\n \n         # Guide dutilisation\n         usage_file = docs_dir / \"USAGE.md\"\n-        with open(usage_file, 'w', encoding='utf-8') as f:\n+        with open(usage_file, \"w\", encoding=\"utf-8\") as f:\n             f.write(usage_guide)\n \n         # Index de documentation\n-        setup_guide_text = t.get('setup_guide', 'Guide d\\'installation')\n-        usage_guide_text = t.get('usage_guide', 'Guide d\\'utilisation')\n-        api_doc_text = t.get('api_documentation', 'Documentation API')\n+        setup_guide_text = t.get(\"setup_guide\", \"Guide d'installation\")\n+        usage_guide_text = t.get(\"usage_guide\", \"Guide d'utilisation\")\n+        api_doc_text = t.get(\"api_documentation\", \"Documentation API\")\n         setup_desc = t.get(\n-            'setup_guide_description',\n-            'Comment installer et configurer le projet')\n+            \"setup_guide_description\", \"Comment installer et configurer le projet\"\n+        )\n         usage_desc = t.get(\n-            'usage_guide_description',\n-            'Comment utiliser les fonctionnalit\u00e9s')\n-        api_desc = t.get(\n-            'api_documentation_description',\n-            'R\u00e9f\u00e9rence compl\u00e8te de l\\'API')\n-        doc_index = t.get('documentation_index', 'Index de documentation')\n+            \"usage_guide_description\", \"Comment utiliser les fonctionnalit\u00e9s\"\n+        )\n+        api_desc = t.get(\"api_documentation_description\", \"R\u00e9f\u00e9rence compl\u00e8te de l'API\")\n+        doc_index = t.get(\"documentation_index\", \"Index de documentation\")\n \n         index_content = f\"\"\"# {doc_index} - {self.project_info['name']}\n \n ## \ud83d\udcda Guides disponibles\n \n@@ -774,33 +769,30 @@\n ---\n *G\u00e9n\u00e9r\u00e9 automatiquement par Athalia* - {current_date}\n \"\"\"\n \n         index_file = docs_dir / \"README.md\"\n-        with open(index_file, 'w', encoding='utf-8') as f:\n+        with open(index_file, \"w\", encoding=\"utf-8\") as f:\n             f.write(index_content)\n \n     def _get_created_files(self) -> List[str]:\n-        files = [\n-            \"README.md\",\n-            \"docs/API.md\",\n-            \"docs/INSTALLATION.md\",\n-            \"docs/USAGE.md\"\n-        ]\n+        files = [\"README.md\", \"docs/API.md\", \"docs/INSTALLATION.md\", \"docs/USAGE.md\"]\n         return [str(self.project_path / f) for f in files]\n \n \n def main():\n     \"\"\"Point dentr\u00e9e du script\"\"\"\n \n     parser = argparse.ArgumentParser(\n-        description=\"G\u00e9n\u00e9ration automatique de documentation\")\n+        description=\"G\u00e9n\u00e9ration automatique de documentation\"\n+    )\n     parser.add_argument(\"project_path\", help=\"Chemin du projet \u00e0 documenter\")\n     parser.add_argument(\n         \"--lang\",\n         default=\"fr\",\n-        help=\"Langue pour la g\u00e9n\u00e9ration de la documentation (fr, en, es, etc.)\")\n+        help=\"Langue pour la g\u00e9n\u00e9ration de la documentation (fr, en, es, etc.)\",\n+    )\n \n     args = parser.parse_args()\n \n     if not os.path.exists(args.project_path):\n         logger.info(f\"\u274c Le chemin {args.project_path} nexiste pas\")\n--- /Volumes/T7/athalia-dev-setup/athalia_core/error_codes.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/error_codes.py\t2025-07-29 18:12:21.329084+00:00\n@@ -70,10 +70,11 @@\n     TIMEOUT_EXCEEDED = 9003\n \n \n class ErrorSeverity(Enum):\n     \"\"\"Niveaux de s\u00e9v\u00e9rit\u00e9 des erreurs.\"\"\"\n+\n     INFO = auto()\n     WARNING = auto()\n     ERROR = auto()\n     CRITICAL = auto()\n \n@@ -135,11 +136,10 @@\n         ErrorCode.FILE_CORRUPTED: ErrorSeverity.CRITICAL,\n         ErrorCode.MODULE_INITIALIZATION_FAILED: ErrorSeverity.CRITICAL,\n         ErrorCode.AI_MODEL_LOAD_FAILED: ErrorSeverity.CRITICAL,\n         ErrorCode.SECURITY_VIOLATION: ErrorSeverity.CRITICAL,\n         ErrorCode.MEMORY_EXHAUSTED: ErrorSeverity.CRITICAL,\n-\n         # Erreurs importantes\n         ErrorCode.INVALID_CONFIGURATION: ErrorSeverity.ERROR,\n         ErrorCode.PERMISSION_DENIED: ErrorSeverity.ERROR,\n         ErrorCode.FILE_ACCESS_DENIED: ErrorSeverity.ERROR,\n         ErrorCode.MODULE_DEPENDENCY_MISSING: ErrorSeverity.ERROR,\n@@ -152,11 +152,10 @@\n         ErrorCode.AUTHENTICATION_FAILED: ErrorSeverity.ERROR,\n         ErrorCode.AUTHORIZATION_DENIED: ErrorSeverity.ERROR,\n         ErrorCode.VULNERABILITY_DETECTED: ErrorSeverity.ERROR,\n         ErrorCode.PERFORMANCE_DEGRADED: ErrorSeverity.ERROR,\n         ErrorCode.CPU_OVERLOAD: ErrorSeverity.ERROR,\n-\n         # Avertissements\n         ErrorCode.INVALID_INPUT: ErrorSeverity.WARNING,\n         ErrorCode.MISSING_REQUIRED_PARAMETER: ErrorSeverity.WARNING,\n         ErrorCode.FILE_NOT_FOUND: ErrorSeverity.WARNING,\n         ErrorCode.DIRECTORY_NOT_FOUND: ErrorSeverity.WARNING,\n@@ -177,21 +176,24 @@\n     }\n \n     return severity_mapping.get(error_code, ErrorSeverity.INFO)\n \n \n-def format_error_message(error_code: ErrorCode, details: str = \"\", context: Dict[str, Any] = None) -> str:\n+def format_error_message(\n+    error_code: ErrorCode, details: str = \"\", context: Dict[str, Any] = None\n+) -> str:\n     \"\"\"Formate un message d'erreur complet.\"\"\"\n     description = get_error_description(error_code)\n     severity = get_error_severity(error_code)\n \n-    message = (f\"[{severity.name}] {error_code.name} ({error_code.value}): \"\n-               f\"{description}\")\n+    message = (\n+        f\"[{severity.name}] {error_code.name} ({error_code.value}): \" f\"{description}\"\n+    )\n \n     if details:\n         message += f\" - {details}\"\n \n     if context:\n         context_str = \", \".join([f\"{k}={v}\" for k, v in context.items()])\n         message += f\" [Context: {context_str}]\"\n \n-    return message\n\\ No newline at end of file\n+    return message\n--- /Volumes/T7/athalia-dev-setup/athalia_core/error_handling.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/error_handling.py\t2025-07-29 18:12:21.379065+00:00\n@@ -10,18 +10,28 @@\n import sys\n from typing import Optional, Dict, Any, Callable\n from pathlib import Path\n from datetime import datetime\n \n-from .error_codes import ErrorCode, ErrorSeverity, format_error_message, get_error_severity\n+from .error_codes import (\n+    ErrorCode,\n+    ErrorSeverity,\n+    format_error_message,\n+    get_error_severity,\n+)\n \n \n class AthaliaError(Exception):\n     \"\"\"Exception de base pour Athalia avec code d'erreur.\"\"\"\n \n-    def __init__(self, error_code: ErrorCode, message: str = \"\", details: str = \"\", \n-                 context: Dict[str, Any] = None):\n+    def __init__(\n+        self,\n+        error_code: ErrorCode,\n+        message: str = \"\",\n+        details: str = \"\",\n+        context: Dict[str, Any] = None,\n+    ):\n         self.error_code = error_code\n         self.message = message\n         self.details = details\n         self.context = context or {}\n         self.timestamp = datetime.now()\n@@ -35,18 +45,18 @@\n         super().__init__(full_message)\n \n     def to_dict(self) -> Dict[str, Any]:\n         \"\"\"Convertit l'erreur en dictionnaire pour s\u00e9rialisation.\"\"\"\n         return {\n-            'error_code': self.error_code.name,\n-            'error_value': self.error_code.value,\n-            'message': self.message,\n-            'details': self.details,\n-            'context': self.context,\n-            'timestamp': self.timestamp.isoformat(),\n-            'severity': self.severity.name,\n-            'traceback': traceback.format_exc()\n+            \"error_code\": self.error_code.name,\n+            \"error_value\": self.error_code.value,\n+            \"message\": self.message,\n+            \"details\": self.details,\n+            \"context\": self.context,\n+            \"timestamp\": self.timestamp.isoformat(),\n+            \"severity\": self.severity.name,\n+            \"traceback\": traceback.format_exc(),\n         }\n \n \n class ErrorHandler:\n     \"\"\"Gestionnaire centralis\u00e9 des erreurs.\"\"\"\n@@ -60,11 +70,11 @@\n         # Configuration du logging\n         self._setup_logging()\n \n     def _setup_logging(self):\n         \"\"\"Configure le syst\u00e8me de logging.\"\"\"\n-        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n+        log_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n \n         # Handler pour console\n         console_handler = logging.StreamHandler(sys.stdout)\n         console_handler.setLevel(logging.INFO)\n         console_handler.setFormatter(logging.Formatter(log_format))\n@@ -76,29 +86,25 @@\n             file_handler.setLevel(logging.DEBUG)\n             file_handler.setFormatter(logging.Formatter(log_format))\n             handlers.append(file_handler)\n \n         # Configuration du logger principal\n-        logging.basicConfig(\n-            level=logging.DEBUG,\n-            handlers=handlers,\n-            force=True\n-        )\n-\n-        self.logger = logging.getLogger('athalia.error_handler')\n-\n-    def handle_error(self, error: Exception, context: Dict[str, Any] = None) -> AthaliaError:\n+        logging.basicConfig(level=logging.DEBUG, handlers=handlers, force=True)\n+\n+        self.logger = logging.getLogger(\"athalia.error_handler\")\n+\n+    def handle_error(\n+        self, error: Exception, context: Dict[str, Any] = None\n+    ) -> AthaliaError:\n         \"\"\"G\u00e8re une erreur et la convertit en AthaliaError.\"\"\"\n         if isinstance(error, AthaliaError):\n             athalia_error = error\n         else:\n             # Convertir l'erreur en AthaliaError\n             error_code = self._classify_error(error)\n             athalia_error = AthaliaError(\n-                error_code=error_code,\n-                message=str(error),\n-                context=context or {}\n+                error_code=error_code, message=str(error), context=context or {}\n             )\n \n         # Log de l'erreur\n         self._log_error(athalia_error)\n \n@@ -122,20 +128,20 @@\n         \"\"\"Classifie une exception en code d'erreur Athalia.\"\"\"\n         error_type = type(error).__name__\n \n         # Mapping des types d'erreurs Python vers les codes Athalia\n         error_mapping = {\n-            'FileNotFoundError': ErrorCode.FILE_NOT_FOUND,\n-            'PermissionError': ErrorCode.PERMISSION_DENIED,\n-            'ImportError': ErrorCode.MODULE_IMPORT_ERROR,\n-            'ModuleNotFoundError': ErrorCode.MODULE_NOT_FOUND,\n-            'ValueError': ErrorCode.INVALID_INPUT,\n-            'TypeError': ErrorCode.INVALID_INPUT,\n-            'KeyError': ErrorCode.MISSING_REQUIRED_PARAMETER,\n-            'TimeoutError': ErrorCode.TIMEOUT_EXCEEDED,\n-            'MemoryError': ErrorCode.MEMORY_EXHAUSTED,\n-            'OSError': ErrorCode.FILE_ACCESS_DENIED,\n+            \"FileNotFoundError\": ErrorCode.FILE_NOT_FOUND,\n+            \"PermissionError\": ErrorCode.PERMISSION_DENIED,\n+            \"ImportError\": ErrorCode.MODULE_IMPORT_ERROR,\n+            \"ModuleNotFoundError\": ErrorCode.MODULE_NOT_FOUND,\n+            \"ValueError\": ErrorCode.INVALID_INPUT,\n+            \"TypeError\": ErrorCode.INVALID_INPUT,\n+            \"KeyError\": ErrorCode.MISSING_REQUIRED_PARAMETER,\n+            \"TimeoutError\": ErrorCode.TIMEOUT_EXCEEDED,\n+            \"MemoryError\": ErrorCode.MEMORY_EXHAUSTED,\n+            \"OSError\": ErrorCode.FILE_ACCESS_DENIED,\n         }\n \n         return error_mapping.get(error_type, ErrorCode.UNKNOWN_ERROR)\n \n     def _log_error(self, error: AthaliaError):\n@@ -149,21 +155,25 @@\n         elif error.severity == ErrorSeverity.WARNING:\n             self.logger.warning(log_message)\n         else:\n             self.logger.info(log_message)\n \n-    def register_callback(self, error_code: ErrorCode, callback: Callable[[AthaliaError], None]):\n+    def register_callback(\n+        self, error_code: ErrorCode, callback: Callable[[AthaliaError], None]\n+    ):\n         \"\"\"Enregistre un callback pour un type d'erreur sp\u00e9cifique.\"\"\"\n         self.error_callbacks[error_code] = callback\n \n     def get_error_summary(self) -> Dict[str, Any]:\n         \"\"\"Retourne un r\u00e9sum\u00e9 des erreurs.\"\"\"\n         return {\n-            'total_errors': self.error_count,\n-            'critical_errors': len(self.critical_errors),\n-            'has_critical_errors': len(self.critical_errors) > 0,\n-            'critical_error_details': [error.to_dict() for error in self.critical_errors]\n+            \"total_errors\": self.error_count,\n+            \"critical_errors\": len(self.critical_errors),\n+            \"has_critical_errors\": len(self.critical_errors) > 0,\n+            \"critical_error_details\": [\n+                error.to_dict() for error in self.critical_errors\n+            ],\n         }\n \n     def clear_errors(self):\n         \"\"\"Efface l'historique des erreurs.\"\"\"\n         self.error_count = 0\n@@ -187,39 +197,52 @@\n def handle_error(error: Exception, context: Dict[str, Any] = None) -> AthaliaError:\n     \"\"\"Fonction utilitaire pour g\u00e9rer une erreur.\"\"\"\n     return get_error_handler().handle_error(error, context)\n \n \n-def raise_athalia_error(error_code: ErrorCode, message: str = \"\", details: str = \"\", \n-                       context: Dict[str, Any] = None):\n+def raise_athalia_error(\n+    error_code: ErrorCode,\n+    message: str = \"\",\n+    details: str = \"\",\n+    context: Dict[str, Any] = None,\n+):\n     \"\"\"L\u00e8ve une AthaliaError avec gestion automatique.\"\"\"\n     error = AthaliaError(error_code, message, details, context)\n     get_error_handler().handle_error(error)\n     raise error\n \n \n # D\u00e9corateur pour gestion automatique d'erreurs\n def error_handler(error_code: ErrorCode = ErrorCode.UNKNOWN_ERROR):\n     \"\"\"D\u00e9corateur pour gestion automatique d'erreurs.\"\"\"\n+\n     def decorator(func: Callable) -> Callable:\n         def wrapper(*args, **kwargs):\n             try:\n                 return func(*args, **kwargs)\n             except Exception as e:\n                 if isinstance(e, AthaliaError):\n                     raise\n                 else:\n-                    raise_athalia_error(error_code, f\"Erreur dans {func.__name__}\", str(e))\n+                    raise_athalia_error(\n+                        error_code, f\"Erreur dans {func.__name__}\", str(e)\n+                    )\n+\n         return wrapper\n+\n     return decorator\n \n \n # Context manager pour gestion d'erreurs\n class ErrorContext:\n     \"\"\"Context manager pour gestion d'erreurs dans un bloc de code.\"\"\"\n \n-    def __init__(self, error_code: ErrorCode = ErrorCode.UNKNOWN_ERROR, context: Dict[str, Any] = None):\n+    def __init__(\n+        self,\n+        error_code: ErrorCode = ErrorCode.UNKNOWN_ERROR,\n+        context: Dict[str, Any] = None,\n+    ):\n         self.error_code = error_code\n         self.context = context or {}\n \n     def __enter__(self):\n         return self\n@@ -227,7 +250,12 @@\n     def __exit__(self, exc_type, exc_val, exc_tb):\n         if exc_type is not None:\n             if isinstance(exc_val, AthaliaError):\n                 return False  # Laisse l'erreur se propager\n             else:\n-                raise_athalia_error(self.error_code, f\"Erreur dans le contexte\", str(exc_val), self.context)\n-        return True\n\\ No newline at end of file\n+                raise_athalia_error(\n+                    self.error_code,\n+                    f\"Erreur dans le contexte\",\n+                    str(exc_val),\n+                    self.context,\n+                )\n+        return True\n--- /Volumes/T7/athalia-dev-setup/athalia_core/config_manager.py\t2025-07-29 17:56:26.440000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/config_manager.py\t2025-07-29 18:12:21.375310+00:00\n@@ -23,11 +23,11 @@\n \n     Returns:\n         Dict contenant la configuration charg\u00e9e\n     \"\"\"\n     try:\n-        with open(config_path, 'r', encoding='utf-8') as file:\n+        with open(config_path, \"r\", encoding=\"utf-8\") as file:\n             return yaml.safe_load(file) or {}\n     except Exception as e:\n         logging.warning(f\"Erreur lors du chargement de {config_path}: {e}\")\n         return {}\n \n@@ -45,25 +45,22 @@\n     \"\"\"\n     try:\n         # Cr\u00e9er le r\u00e9pertoire parent si n\u00e9cessaire\n         os.makedirs(os.path.dirname(config_path), exist_ok=True)\n \n-        with open(config_path, 'w', encoding='utf-8') as file:\n-            yaml.dump(\n-                config,\n-                file,\n-                default_flow_style=False,\n-                allow_unicode=True)\n+        with open(config_path, \"w\", encoding=\"utf-8\") as file:\n+            yaml.dump(config, file, default_flow_style=False, allow_unicode=True)\n         return True\n     except Exception as e:\n         logging.error(f\"Erreur lors de la sauvegarde de {config_path}: {e}\")\n         return False\n \n \n @dataclass\n class AthaliaConfig:\n     \"\"\"Configuration centralis\u00e9e d'f\"\"\"\n+\n     # G\u00e9n\u00e9ral\n     lang: str = \"f\"\n     verbose: bool = True\n     auto_fix: bool = True\n     dry_run: bool = False\n@@ -136,185 +133,192 @@\n         config = AthaliaConfig()\n \n         # Charger depuis le fichier YAML\n         if os.path.exists(self.config_file):\n             try:\n-                with open(self.config_file, 'r', encoding='utf-8') as file_handle:\n+                with open(self.config_file, \"r\", encoding=\"utf-8\") as file_handle:\n                     yaml_config = yaml.safe_load(file_handle)\n                     config = self._merge_yaml_config(config, yaml_config)\n             except Exception as e:\n-                logging.warning(\n-                    f\"Erreur lors du chargement de {self.config_file}: {e}\")\n+                logging.warning(f\"Erreur lors du chargement de {self.config_file}: {e}\")\n \n         # Surcharger avec les variables d'environnement\n         config = self._merge_env_config(config)\n \n         return config\n \n-    def _merge_yaml_config(self, config: AthaliaConfig,\n-                           yaml_data: Dict[str, Any]) -> AthaliaConfig:\n+    def _merge_yaml_config(\n+        self, config: AthaliaConfig, yaml_data: Dict[str, Any]\n+    ) -> AthaliaConfig:\n         \"\"\"Fusionne la configuration YAML avec la config par f\"\"\"\n         if not yaml_data:\n             return config\n \n         # G\u00e9n\u00e9ral\n-        if 'general' in yaml_data:\n-            general = yaml_data['general']\n-            config.lang = general.get('lang', config.lang)\n-            config.verbose = general.get('verbose', config.verbose)\n-            config.auto_fix = general.get('auto_fix', config.auto_fix)\n-            config.dry_run = general.get('dry_run', config.dry_run)\n-            config.log_level = general.get('log_level', config.log_level)\n-            config.log_file = general.get('log_file', config.log_file)\n+        if \"general\" in yaml_data:\n+            general = yaml_data[\"general\"]\n+            config.lang = general.get(\"lang\", config.lang)\n+            config.verbose = general.get(\"verbose\", config.verbose)\n+            config.auto_fix = general.get(\"auto_fix\", config.auto_fix)\n+            config.dry_run = general.get(\"dry_run\", config.dry_run)\n+            config.log_level = general.get(\"log_level\", config.log_level)\n+            config.log_file = general.get(\"log_file\", config.log_file)\n \n         # Modules\n-        if 'modules' in yaml_data:\n-            config.modules = yaml_data['modules']\n+        if \"modules\" in yaml_data:\n+            config.modules = yaml_data[\"modules\"]\n \n         # Plugins\n-        if 'plugins' in yaml_data:\n-            config.plugins = yaml_data['plugins']\n+        if \"plugins\" in yaml_data:\n+            config.plugins = yaml_data[\"plugins\"]\n \n         # Templates\n-        if 'templates' in yaml_data:\n-            config.templates = yaml_data['templates']\n+        if \"templates\" in yaml_data:\n+            config.templates = yaml_data[\"templates\"]\n \n         # Base de donn\u00e9es\n-        if 'database' in yaml_data:\n-            db = yaml_data['database']\n-            config.db_path = db.get('path', config.db_path)\n-            config.db_backup = db.get('backup', config.db_backup)\n+        if \"database\" in yaml_data:\n+            db = yaml_data[\"database\"]\n+            config.db_path = db.get(\"path\", config.db_path)\n+            config.db_backup = db.get(\"backup\", config.db_backup)\n             config.db_backup_retention = db.get(\n-                'backup_retention', config.db_backup_retention)\n+                \"backup_retention\", config.db_backup_retention\n+            )\n \n         # IA\n-        if 'ai' in yaml_data:\n-            ai = yaml_data['ai']\n-            config.ai_models = ai.get('models', config.ai_models)\n-            config.ai_timeout = ai.get('timeout', config.ai_timeout)\n-            config.ai_max_retries = ai.get(\n-                'max_retries', config.ai_max_retries)\n+        if \"ai\" in yaml_data:\n+            ai = yaml_data[\"ai\"]\n+            config.ai_models = ai.get(\"models\", config.ai_models)\n+            config.ai_timeout = ai.get(\"timeout\", config.ai_timeout)\n+            config.ai_max_retries = ai.get(\"max_retries\", config.ai_max_retries)\n             config.ai_fallback_enabled = ai.get(\n-                'fallback_enabled', config.ai_fallback_enabled)\n+                \"fallback_enabled\", config.ai_fallback_enabled\n+            )\n \n         # Tests\n-        if 'testing' in yaml_data:\n-            testing = yaml_data['testing']\n-            config.test_auto_run = testing.get(\n-                'auto_run', config.test_auto_run)\n-            config.test_coverage = testing.get(\n-                'coverage', config.test_coverage)\n-            config.test_parallel = testing.get(\n-                'parallel', config.test_parallel)\n-            config.test_timeout = testing.get('timeout', config.test_timeout)\n+        if \"testing\" in yaml_data:\n+            testing = yaml_data[\"testing\"]\n+            config.test_auto_run = testing.get(\"auto_run\", config.test_auto_run)\n+            config.test_coverage = testing.get(\"coverage\", config.test_coverage)\n+            config.test_parallel = testing.get(\"parallel\", config.test_parallel)\n+            config.test_timeout = testing.get(\"timeout\", config.test_timeout)\n \n         # CI / CD\n-        if 'cicd' in yaml_data:\n-            cicd = yaml_data['cicd']\n+        if \"cicd\" in yaml_data:\n+            cicd = yaml_data[\"cicd\"]\n             config.cicd_github_actions = cicd.get(\n-                'github_actions', config.cicd_github_actions)\n-            config.cicd_docker = cicd.get('docker', config.cicd_docker)\n-            config.cicd_deployment = cicd.get(\n-                'deployment', config.cicd_deployment)\n+                \"github_actions\", config.cicd_github_actions\n+            )\n+            config.cicd_docker = cicd.get(\"docker\", config.cicd_docker)\n+            config.cicd_deployment = cicd.get(\"deployment\", config.cicd_deployment)\n \n         # Nettoyage\n-        if 'cleanup' in yaml_data:\n-            cleanup = yaml_data['cleanup']\n+        if \"cleanup\" in yaml_data:\n+            cleanup = yaml_data[\"cleanup\"]\n             config.cleanup_auto_clean = cleanup.get(\n-                'auto_clean', config.cleanup_auto_clean)\n-            config.cleanup_patterns = cleanup.get(\n-                'patterns', config.cleanup_patterns)\n+                \"auto_clean\", config.cleanup_auto_clean\n+            )\n+            config.cleanup_patterns = cleanup.get(\"patterns\", config.cleanup_patterns)\n \n         # Dashboard\n-        if 'dashboard' in yaml_data:\n-            dashboard = yaml_data['dashboard']\n+        if \"dashboard\" in yaml_data:\n+            dashboard = yaml_data[\"dashboard\"]\n             config.dashboard_auto_generate = dashboard.get(\n-                'auto_generate', config.dashboard_auto_generate)\n-            config.dashboard_port = dashboard.get(\n-                'port', config.dashboard_port)\n-            config.dashboard_host = dashboard.get(\n-                'host', config.dashboard_host)\n+                \"auto_generate\", config.dashboard_auto_generate\n+            )\n+            config.dashboard_port = dashboard.get(\"port\", config.dashboard_port)\n+            config.dashboard_host = dashboard.get(\"host\", config.dashboard_host)\n             config.dashboard_auto_open = dashboard.get(\n-                'auto_open', config.dashboard_auto_open)\n+                \"auto_open\", config.dashboard_auto_open\n+            )\n \n         # Profils\n-        if 'profiles' in yaml_data:\n-            profiles = yaml_data['profiles']\n+        if \"profiles\" in yaml_data:\n+            profiles = yaml_data[\"profiles\"]\n             config.profiles_auto_create = profiles.get(\n-                'auto_create', config.profiles_auto_create)\n+                \"auto_create\", config.profiles_auto_create\n+            )\n             config.profiles_default_user = profiles.get(\n-                'default_user', config.profiles_default_user)\n+                \"default_user\", config.profiles_default_user\n+            )\n             config.profiles_history_retention = profiles.get(\n-                'history_retention', config.profiles_history_retention)\n+                \"history_retention\", config.profiles_history_retention\n+            )\n \n         # S\u00e9curit\u00e9\n-        if 'security' in yaml_data:\n-            security = yaml_data['security']\n+        if \"security\" in yaml_data:\n+            security = yaml_data[\"security\"]\n             config.security_audit_enabled = security.get(\n-                'audit_enabled', config.security_audit_enabled)\n+                \"audit_enabled\", config.security_audit_enabled\n+            )\n             config.security_secrets_detection = security.get(\n-                'secrets_detection', config.security_secrets_detection)\n+                \"secrets_detection\", config.security_secrets_detection\n+            )\n             config.security_vulnerability_scan = security.get(\n-                'vulnerability_scan', config.security_vulnerability_scan)\n+                \"vulnerability_scan\", config.security_vulnerability_scan\n+            )\n \n         # Analytics\n-        if 'analytics' in yaml_data:\n-            analytics = yaml_data['analytics']\n+        if \"analytics\" in yaml_data:\n+            analytics = yaml_data[\"analytics\"]\n             config.analytics_enabled = analytics.get(\n-                'enabled', config.analytics_enabled)\n+                \"enabled\", config.analytics_enabled\n+            )\n             config.analytics_metrics_retention = analytics.get(\n-                'metrics_retention', config.analytics_metrics_retention)\n+                \"metrics_retention\", config.analytics_metrics_retention\n+            )\n             config.analytics_auto_export = analytics.get(\n-                'auto_export', config.analytics_auto_export)\n+                \"auto_export\", config.analytics_auto_export\n+            )\n \n         return config\n \n     def _merge_env_config(self, config: AthaliaConfig) -> AthaliaConfig:\n         \"\"\"Surcharge la configuration avec les variables d'f\"\"\"\n         # Variables d'environnement prioritaires\n-        config.lang = os.getenv('ATHALIA_LANG', config.lang)\n-        config.verbose = os.getenv(\n-            'ATHALIA_VERBOSE', str(\n-                config.verbose)).lower() == 'true'\n-        config.auto_fix = os.getenv(\n-            'ATHALIA_AUTO_FIX', str(\n-                config.auto_fix)).lower() == 'true'\n-        config.dry_run = os.getenv(\n-            'ATHALIA_DRY_RUN', str(\n-                config.dry_run)).lower() == 'true'\n-        config.log_level = os.getenv('ATHALIA_LOG_LEVEL', config.log_level)\n-        config.log_file = os.getenv('ATHALIA_LOG_FILE', config.log_file)\n+        config.lang = os.getenv(\"ATHALIA_LANG\", config.lang)\n+        config.verbose = (\n+            os.getenv(\"ATHALIA_VERBOSE\", str(config.verbose)).lower() == \"true\"\n+        )\n+        config.auto_fix = (\n+            os.getenv(\"ATHALIA_AUTO_FIX\", str(config.auto_fix)).lower() == \"true\"\n+        )\n+        config.dry_run = (\n+            os.getenv(\"ATHALIA_DRY_RUN\", str(config.dry_run)).lower() == \"true\"\n+        )\n+        config.log_level = os.getenv(\"ATHALIA_LOG_LEVEL\", config.log_level)\n+        config.log_file = os.getenv(\"ATHALIA_LOG_FILE\", config.log_file)\n \n         # Base de donn\u00e9es\n-        config.db_path = os.getenv('ATHALIA_DB_PATH', config.db_path)\n+        config.db_path = os.getenv(\"ATHALIA_DB_PATH\", config.db_path)\n \n         # API Key\n-        self.api_key = os.getenv('ATHALIA_API_KEY', '')\n+        self.api_key = os.getenv(\"ATHALIA_API_KEY\", \"\")\n \n         return config\n \n     def _setup_logging(self):\n         \"\"\"Configure le logging selon la f\"\"\"\n-        log_level = getattr(\n-            logging,\n-            self.config.log_level.upper(),\n-            logging.INFO)\n+        log_level = getattr(logging, self.config.log_level.upper(), logging.INFO)\n \n         logging.basicConfig(\n             level=log_level,\n-            format=('%(asctime)string_data - %(name)string_data - '\n-                    '%(levelname)string_data-%(message)string_data'),\n+            format=(\n+                \"%(asctime)string_data - %(name)string_data - \"\n+                \"%(levelname)string_data-%(message)string_data\"\n+            ),\n             handlers=[\n-                logging.FileHandler(\n-                    self.config.log_file),\n-                logging.StreamHandler()])\n+                logging.FileHandler(self.config.log_file),\n+                logging.StreamHandler(),\n+            ],\n+        )\n \n     def get(self, key: str, default: Any = None) -> Any:\n         \"\"\"R\u00e9cup\u00e8re une valeur de configuration\"\"\"\n         # Support pour les cl\u00e9s imbriqu\u00e9es (ex: 'test.key')\n-        if '.' in key:\n-            parts = key.split('.')\n+        if \".\" in key:\n+            parts = key.split(\".\")\n             current = self.config\n             for part in parts:\n                 if isinstance(current, dict):\n                     if part not in current:\n                         return default\n@@ -336,43 +340,42 @@\n     def get_enabled_plugins(self) -> List[str]:\n         \"\"\"R\u00e9cup\u00e8re la liste des plugins f\"\"\"\n         if not self.config.plugins:\n             return []\n \n-        if self.config.plugins.get('auto_discovery', True):\n+        if self.config.plugins.get(\"auto_discovery\", True):\n             # Auto-d\u00e9couverte des plugins\n             plugins_dir = Path(\"f\")\n             if plugins_dir.exists():\n                 return [p.stem for p in plugins_dir.glob(\"*.f\") if p.is_file()]\n \n-        return self.config.plugins.get('enabled', [])\n+        return self.config.plugins.get(\"enabled\", [])\n \n     def get_available_templates(self) -> List[str]:\n         \"\"\"R\u00e9cup\u00e8re la liste des templates f\"\"\"\n         if not self.config.templates:\n             return [\"f\", \"f\", \"f\", \"f\", \"f\", \"f\"]\n \n-        if self.config.templates.get('auto_discovery', True):\n+        if self.config.templates.get(\"auto_discovery\", True):\n             # Auto-d\u00e9couverte des templates\n             templates_dir = Path(\"f\")\n             if templates_dir.exists():\n                 return [t.name for t in templates_dir.iterdir() if t.is_dir()]\n \n-        return self.config.templates.get(\n-            'available', [\"f\", \"f\", \"f\", \"f\", \"f\", \"f\"])\n+        return self.config.templates.get(\"available\", [\"f\", \"f\", \"f\", \"f\", \"f\", \"f\"])\n \n     def get_cleanup_patterns(self) -> List[str]:\n         \"\"\"R\u00e9cup\u00e8re les patterns de f\"\"\"\n         if not self.config.cleanup_patterns:\n             return [\"._*\", \"f\", \"*.f\", \".f\", \".f\", \".f\", \"*.f\"]\n         return self.config.cleanup_patterns\n \n     def set(self, key: str, value: Any) -> None:\n         \"\"\"D\u00e9finit une valeur de configuration\"\"\"\n         # Support pour les cl\u00e9s imbriqu\u00e9es (ex: 'test.key')\n-        if '.' in key:\n-            parts = key.split('.')\n+        if \".\" in key:\n+            parts = key.split(\".\")\n             current = self.config\n             for part in parts[:-1]:\n                 if not hasattr(current, part):\n                     setattr(current, part, {})\n                 current = getattr(current, part)\n@@ -389,51 +392,48 @@\n             # Validation basique - v\u00e9rifier que c'est un dict\n             if not isinstance(config, dict):\n                 return False\n \n             # Validation des cl\u00e9s requises\n-            required_keys = ['general', 'modules']\n+            required_keys = [\"general\", \"modules\"]\n             for key in required_keys:\n                 if key not in config:\n                     return False\n \n             return True\n         except Exception:\n             return False\n \n-    def merge_configs(self,\n-                      base_config: Dict[str,\n-                                        Any],\n-                      override_config: Dict[str,\n-                                            Any]) -> Dict[str,\n-                                                          Any]:\n+    def merge_configs(\n+        self, base_config: Dict[str, Any], override_config: Dict[str, Any]\n+    ) -> Dict[str, Any]:\n         \"\"\"Fusionne deux configurations\"\"\"\n         merged = base_config.copy()\n \n         for key, value in override_config.items():\n-            if key in merged and isinstance(\n-                    merged[key],\n-                    dict) and isinstance(\n-                    value,\n-                    dict):\n+            if (\n+                key in merged\n+                and isinstance(merged[key], dict)\n+                and isinstance(value, dict)\n+            ):\n                 merged[key] = self.merge_configs(merged[key], value)\n             else:\n                 merged[key] = value\n \n         return merged\n \n-    def resolve_environment_variables(\n-            self, config: Dict[str, Any]) -> Dict[str, Any]:\n+    def resolve_environment_variables(self, config: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"R\u00e9sout les variables d'environnement dans une configuration\"\"\"\n         resolved = {}\n \n         for key, value in config.items():\n-            if isinstance(value, str) and '${' in value and '}' in value:\n+            if isinstance(value, str) and \"${\" in value and \"}\" in value:\n                 # Chercher toutes les variables d'environnement dans la cha\u00eene\n                 result = value\n                 import re\n-                pattern = r'\\$\\{([^}]+)\\}'\n+\n+                pattern = r\"\\$\\{([^}]+)\\}\"\n                 for match in re.finditer(pattern, value):\n                     env_var = match.group(1)\n                     env_value = os.getenv(env_var)\n                     if env_value is not None:\n                         result = result.replace(f\"${{{env_var}}}\", env_value)\n@@ -446,68 +446,68 @@\n         return resolved\n \n     def to_dict(self) -> Dict[str, Any]:\n         \"\"\"Convertit la configuration en f\"\"\"\n         return {\n-            'general': {\n-                'lang': self.config.lang,\n-                'verbose': self.config.verbose,\n-                'auto_fix': self.config.auto_fix,\n-                'dry_run': self.config.dry_run,\n-                'log_level': self.config.log_level,\n-                'log_file': self.config.log_file\n-            },\n-            'modules': self.config.modules,\n-            'plugins': self.config.plugins,\n-            'templates': self.config.templates,\n-            'database': {\n-                'path': self.config.db_path,\n-                'backup': self.config.db_backup,\n-                'backup_retention': self.config.db_backup_retention\n-            },\n-            'ai': {\n-                'models': self.config.ai_models,\n-                'timeout': self.config.ai_timeout,\n-                'max_retries': self.config.ai_max_retries,\n-                'fallback_enabled': self.config.ai_fallback_enabled\n-            },\n-            'testing': {\n-                'auto_run': self.config.test_auto_run,\n-                'coverage': self.config.test_coverage,\n-                'parallel': self.config.test_parallel,\n-                'timeout': self.config.test_timeout\n-            },\n-            'cicd': {\n-                'github_actions': self.config.cicd_github_actions,\n-                'docker': self.config.cicd_docker,\n-                'deployment': self.config.cicd_deployment\n-            },\n-            'cleanup': {\n-                'auto_clean': self.config.cleanup_auto_clean,\n-                'patterns': self.config.cleanup_patterns\n-            },\n-            'dashboard': {\n-                'auto_generate': self.config.dashboard_auto_generate,\n-                'port': self.config.dashboard_port,\n-                'host': self.config.dashboard_host,\n-                'auto_open': self.config.dashboard_auto_open\n-            },\n-            'profiles': {\n-                'auto_create': self.config.profiles_auto_create,\n-                'default_user': self.config.profiles_default_user,\n-                'history_retention': self.config.profiles_history_retention\n-            },\n-            'security': {\n-                'audit_enabled': self.config.security_audit_enabled,\n-                'secrets_detection': self.config.security_secrets_detection,\n-                'vulnerability_scan': self.config.security_vulnerability_scan\n-            },\n-            'analytics': {\n-                'enabled': self.config.analytics_enabled,\n-                'metrics_retention': self.config.analytics_metrics_retention,\n-                'auto_export': self.config.analytics_auto_export\n-            }\n+            \"general\": {\n+                \"lang\": self.config.lang,\n+                \"verbose\": self.config.verbose,\n+                \"auto_fix\": self.config.auto_fix,\n+                \"dry_run\": self.config.dry_run,\n+                \"log_level\": self.config.log_level,\n+                \"log_file\": self.config.log_file,\n+            },\n+            \"modules\": self.config.modules,\n+            \"plugins\": self.config.plugins,\n+            \"templates\": self.config.templates,\n+            \"database\": {\n+                \"path\": self.config.db_path,\n+                \"backup\": self.config.db_backup,\n+                \"backup_retention\": self.config.db_backup_retention,\n+            },\n+            \"ai\": {\n+                \"models\": self.config.ai_models,\n+                \"timeout\": self.config.ai_timeout,\n+                \"max_retries\": self.config.ai_max_retries,\n+                \"fallback_enabled\": self.config.ai_fallback_enabled,\n+            },\n+            \"testing\": {\n+                \"auto_run\": self.config.test_auto_run,\n+                \"coverage\": self.config.test_coverage,\n+                \"parallel\": self.config.test_parallel,\n+                \"timeout\": self.config.test_timeout,\n+            },\n+            \"cicd\": {\n+                \"github_actions\": self.config.cicd_github_actions,\n+                \"docker\": self.config.cicd_docker,\n+                \"deployment\": self.config.cicd_deployment,\n+            },\n+            \"cleanup\": {\n+                \"auto_clean\": self.config.cleanup_auto_clean,\n+                \"patterns\": self.config.cleanup_patterns,\n+            },\n+            \"dashboard\": {\n+                \"auto_generate\": self.config.dashboard_auto_generate,\n+                \"port\": self.config.dashboard_port,\n+                \"host\": self.config.dashboard_host,\n+                \"auto_open\": self.config.dashboard_auto_open,\n+            },\n+            \"profiles\": {\n+                \"auto_create\": self.config.profiles_auto_create,\n+                \"default_user\": self.config.profiles_default_user,\n+                \"history_retention\": self.config.profiles_history_retention,\n+            },\n+            \"security\": {\n+                \"audit_enabled\": self.config.security_audit_enabled,\n+                \"secrets_detection\": self.config.security_secrets_detection,\n+                \"vulnerability_scan\": self.config.security_vulnerability_scan,\n+            },\n+            \"analytics\": {\n+                \"enabled\": self.config.analytics_enabled,\n+                \"metrics_retention\": self.config.analytics_metrics_retention,\n+                \"auto_export\": self.config.analytics_auto_export,\n+            },\n         }\n \n \n # Instance globale\n config_manager = ConfigManager()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/generation.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/generation.py\t2025-07-29 18:12:21.379760+00:00\n@@ -13,34 +13,34 @@\n \n \n def generate_blueprint_mock(idea: str = \"\", *args, **kwargs):\n     \"\"\"G\u00e9n\u00e8re un blueprint mock pour les tests.\"\"\"\n     return {\n-        'project_name': extract_project_name(idea),\n-        'description': idea or 'Projet de test',\n-        'project_type': 'generic',\n-        'modules': ['core', 'tests'],\n-        'structure': ['src/', 'tests/', 'README.md'],\n-        'dependencies': ['numpy', 'pandas'],\n-        'prompts': ['prompts/main.yaml'],\n-        'booster_ia': True,\n-        'docker': False,\n-        'ci_cd': False,\n-        'tests': True,\n-        'documentation': True\n+        \"project_name\": extract_project_name(idea),\n+        \"description\": idea or \"Projet de test\",\n+        \"project_type\": \"generic\",\n+        \"modules\": [\"core\", \"tests\"],\n+        \"structure\": [\"src/\", \"tests/\", \"README.md\"],\n+        \"dependencies\": [\"numpy\", \"pandas\"],\n+        \"prompts\": [\"prompts/main.yaml\"],\n+        \"booster_ia\": True,\n+        \"docker\": False,\n+        \"ci_cd\": False,\n+        \"tests\": True,\n+        \"documentation\": True,\n     }\n \n \n def extract_project_name(idea: str) -> str:\n     \"\"\"Extrait un nom de projet de l'id\u00e9e.\"\"\"\n     # Cherche des mots cl\u00e9s sp\u00e9cifiques\n     patterns = [\n-        r'calculatrice\\s+(\\w+)',\n-        r'application\\s+(\\w+)',\n-        r'robot\\s+(\\w+)',\n-        r'api\\s+(\\w+)',\n-        r'(\\w+)\\s+avec'\n+        r\"calculatrice\\s+(\\w+)\",\n+        r\"application\\s+(\\w+)\",\n+        r\"robot\\s+(\\w+)\",\n+        r\"api\\s+(\\w+)\",\n+        r\"(\\w+)\\s+avec\",\n     ]\n \n     for pattern in patterns:\n         match = re.search(pattern, idea, re.IGNORECASE)\n         if match:\n@@ -55,12 +55,12 @@\n     return \"projet_ia\"\n \n \n def generate_project(blueprint: dict, outdir, *args, **kwargs):\n     \"\"\"G\u00e9n\u00e8re un projet \u00e0 partir d'un blueprint.\"\"\"\n-    dry_run = kwargs.get('dry_run', False)\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    dry_run = kwargs.get(\"dry_run\", False)\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n     project_path = Path(outdir) / project_name\n \n     if dry_run:\n         # Mode dry-run : g\u00e9n\u00e9rer seulement le rapport\n         report_content = f\"\"\"[DRY-RUN] G\u00e9n\u00e9ration du projet {project_name}\n@@ -80,20 +80,20 @@\n \n [DRY-RUN] Aucun fichier r\u00e9el cr\u00e9\u00e9.\"\"\"\n \n         # Cr\u00e9er le rapport dans le r\u00e9pertoire parent (outdir)\n         report_file = Path(outdir) / \"dry_run_report.txt\"\n-        report_file.write_text(report_content, encoding='utf-8')\n+        report_file.write_text(report_content, encoding=\"utf-8\")\n         return str(project_path)\n \n     # Mode normal : g\u00e9n\u00e9rer le projet\n     project_path.mkdir(parents=True, exist_ok=True)\n \n     # Cr\u00e9er la structure de base\n-    (project_path / 'src').mkdir(exist_ok=True)\n-    (project_path / 'tests').mkdir(exist_ok=True)\n-    (project_path / 'docs').mkdir(exist_ok=True)\n+    (project_path / \"src\").mkdir(exist_ok=True)\n+    (project_path / \"tests\").mkdir(exist_ok=True)\n+    (project_path / \"docs\").mkdir(exist_ok=True)\n \n     # G\u00e9n\u00e9rer les fichiers de base\n     generate_readme(blueprint, project_path)\n     generate_main_code(blueprint, project_path)\n     generate_test_code(blueprint, project_path)\n@@ -102,12 +102,12 @@\n     return str(project_path)\n \n \n def generate_readme(blueprint: dict, project_path: Optional[Path] = None) -> str:\n     \"\"\"G\u00e9n\u00e8re un README basique.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n-    description = blueprint.get('description', 'Projet g\u00e9n\u00e9r\u00e9 par Athalia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n+    description = blueprint.get(\"description\", \"Projet g\u00e9n\u00e9r\u00e9 par Athalia\")\n \n     readme_content = f\"\"\"# {project_name}\n \n {description}\n \n@@ -132,22 +132,22 @@\n ---\n *G\u00e9n\u00e9r\u00e9 automatiquement par Athalia*\n \"\"\"\n \n     if project_path:\n-        readme_file = project_path / 'README.md'\n-        readme_file.write_text(readme_content, encoding='utf-8')\n+        readme_file = project_path / \"README.md\"\n+        readme_file.write_text(readme_content, encoding=\"utf-8\")\n \n     return readme_content\n \n \n def generate_main_code(blueprint: dict, project_path: Optional[Path] = None) -> str:\n     \"\"\"G\u00e9n\u00e8re le code principal.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n-    project_type = blueprint.get('project_type', 'generic')\n-\n-    if project_type == 'api':\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n+    project_type = blueprint.get(\"project_type\", \"generic\")\n+\n+    if project_type == \"api\":\n         main_content = f\"\"\"#!/usr/bin/env python3\n \\\"\\\"\\\"\n {project_name} - API avec FastAPI\n \\\"\\\"\\\"\n \n@@ -200,20 +200,20 @@\n if __name__ == \"__main__\":\n     main()\n \"\"\"\n \n     if project_path:\n-        main_file = project_path / 'src' / 'main.py'\n+        main_file = project_path / \"src\" / \"main.py\"\n         main_file.parent.mkdir(exist_ok=True)\n-        main_file.write_text(main_content, encoding='utf-8')\n+        main_file.write_text(main_content, encoding=\"utf-8\")\n \n     return main_content\n \n \n def generate_test_code(blueprint: dict, project_path: Optional[Path] = None) -> str:\n     \"\"\"G\u00e9n\u00e8re le code de test.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     test_content = f\"\"\"#!/usr/bin/env python3\n \\\"\\\"\\\"\n Tests pour {project_name}\n \\\"\\\"\\\"\n@@ -255,47 +255,44 @@\n if __name__ == '__main__':\n     unittest.main()\n \"\"\"\n \n     if project_path:\n-        test_file = project_path / 'tests' / 'test_main.py'\n+        test_file = project_path / \"tests\" / \"test_main.py\"\n         test_file.parent.mkdir(exist_ok=True)\n-        test_file.write_text(test_content, encoding='utf-8')\n+        test_file.write_text(test_content, encoding=\"utf-8\")\n \n     return test_content\n \n \n def generate_requirements(blueprint: dict, project_path: Optional[Path] = None) -> str:\n     \"\"\"G\u00e9n\u00e8re un fichier requirements.txt basique.\"\"\"\n     if project_path is None:\n-        project_path = Path('.')\n-\n-    requirements_file = project_path / 'requirements.txt'\n+        project_path = Path(\".\")\n+\n+    requirements_file = project_path / \"requirements.txt\"\n \n     # D\u00e9pendances de base\n-    base_deps = [\n-        'pytest>=7.0.0',\n-        'pytest-cov>=4.0.0'\n-    ]\n+    base_deps = [\"pytest>=7.0.0\", \"pytest-cov>=4.0.0\"]\n \n     # Ajouter les d\u00e9pendances sp\u00e9cifiques au projet\n-    project_deps = blueprint.get('dependencies', [])\n+    project_deps = blueprint.get(\"dependencies\", [])\n     if isinstance(project_deps, list):\n         base_deps.extend(project_deps)\n \n     # Ajouter des d\u00e9pendances selon le type de projet\n-    project_type = blueprint.get('project_type', 'generic')\n-    if project_type == 'api':\n-        base_deps.extend(['fastapi>=0.100.0', 'uvicorn>=0.20.0'])\n-    elif project_type == 'web':\n-        base_deps.extend(['flask>=2.3.0', 'jinja2>=3.1.0'])\n-    elif project_type == 'data':\n-        base_deps.extend(['pandas>=2.0.0', 'numpy>=1.24.0'])\n-\n-    requirements_content = '\\n'.join(base_deps) + '\\n'\n-\n-    with open(requirements_file, 'w', encoding='utf-8') as f:\n+    project_type = blueprint.get(\"project_type\", \"generic\")\n+    if project_type == \"api\":\n+        base_deps.extend([\"fastapi>=0.100.0\", \"uvicorn>=0.20.0\"])\n+    elif project_type == \"web\":\n+        base_deps.extend([\"flask>=2.3.0\", \"jinja2>=3.1.0\"])\n+    elif project_type == \"data\":\n+        base_deps.extend([\"pandas>=2.0.0\", \"numpy>=1.24.0\"])\n+\n+    requirements_content = \"\\n\".join(base_deps) + \"\\n\"\n+\n+    with open(requirements_file, \"w\", encoding=\"utf-8\") as f:\n         f.write(requirements_content)\n \n     return str(requirements_file)\n \n \n@@ -305,98 +302,94 @@\n     import yaml\n \n     outdir = Path(outdir)\n     outdir.mkdir(parents=True, exist_ok=True)\n \n-    blueprint_file = outdir / 'blueprint.yaml'\n-    with open(blueprint_file, 'w', encoding='utf-8') as f:\n+    blueprint_file = outdir / \"blueprint.yaml\"\n+    with open(blueprint_file, \"w\", encoding=\"utf-8\") as f:\n         yaml.dump(blueprint, f, allow_unicode=True)\n \n     return str(blueprint_file)\n \n \n def inject_booster_ia_elements(outdir):\n     \"\"\"Injecte les \u00e9l\u00e9ments Booster IA.\"\"\"\n     from pathlib import Path\n \n     outdir = Path(outdir)\n-    (outdir / 'booster_ia.txt').write_text('Booster IA inject\u00e9')\n-    (outdir / 'prompts').mkdir(exist_ok=True)\n-    (outdir / 'setup').mkdir(exist_ok=True)\n-    (outdir / 'agents').mkdir(exist_ok=True)\n-\n-    return str(outdir / 'booster_ia.txt')\n+    (outdir / \"booster_ia.txt\").write_text(\"Booster IA inject\u00e9\")\n+    (outdir / \"prompts\").mkdir(exist_ok=True)\n+    (outdir / \"setup\").mkdir(exist_ok=True)\n+    (outdir / \"agents\").mkdir(exist_ok=True)\n+\n+    return str(outdir / \"booster_ia.txt\")\n \n \n def scan_existing_project(outdir):\n     \"\"\"Scanne un projet existant.\"\"\"\n     from pathlib import Path\n \n     outdir = Path(outdir)\n     files = {\n-        f.name: True for f in outdir.iterdir()\n-        if f.is_file() and f.name in [\n-            'README.md',\n-            'test_module.py',\n-            'onboarding.md',\n-            'script.py'\n-        ]\n+        f.name: True\n+        for f in outdir.iterdir()\n+        if f.is_file()\n+        and f.name in [\"README.md\", \"test_module.py\", \"onboarding.md\", \"script.py\"]\n     }\n-    files['Modules trouv\u00e9s: test_module.py'] = True\n+    files[\"Modules trouv\u00e9s: test_module.py\"] = True\n     return files\n \n \n def merge_or_suffix_file(\n     file_path: str,\n     content: str,\n     file_type: Optional[str] = None,\n-    section_header: Optional[str] = None\n+    section_header: Optional[str] = None,\n ):\n     \"\"\"Fusionne ou suffixe un fichier.\"\"\"\n     from pathlib import Path\n \n     file = Path(file_path)\n     action = None\n \n     if not file.exists():\n         file.write_text(content)\n-        action = 'created'\n+        action = \"created\"\n         return str(file), action\n     else:\n         if section_header is not None and isinstance(section_header, str):\n-            file.write_text(\n-                file.read_text() + f\"\\n{section_header}\\n{content}\")\n-            action = 'merged'\n+            file.write_text(file.read_text() + f\"\\n{section_header}\\n{content}\")\n+            action = \"merged\"\n             return str(file), action\n-        elif file_type is not None and file_type in ['test', 'prompt', 'onboarding']:\n-            file.write_text(file.read_text() + '\\n' + content)\n-            action = f'merged-{file_type}'\n+        elif file_type is not None and file_type in [\"test\", \"prompt\", \"onboarding\"]:\n+            file.write_text(file.read_text() + \"\\n\" + content)\n+            action = f\"merged-{file_type}\"\n             return str(file), action\n         else:\n             if file.suffix:\n                 suffix_file = file.with_name(f\"{file.stem}_auto{file.suffix}\")\n             else:\n                 suffix_file = file.with_name(f\"{file.name}_auto\")\n             suffix_file.write_text(content)\n-            action = 'suffixed'\n+            action = \"suffixed\"\n             return str(suffix_file), action\n \n \n def backup_file(file_path: str):\n     \"\"\"Cr\u00e9e une sauvegarde d'un fichier.\"\"\"\n     from pathlib import Path\n \n     file = Path(file_path)\n-    backup = file.with_suffix(file.suffix + '.backup')\n+    backup = file.with_suffix(file.suffix + \".backup\")\n     backup.write_text(file.read_text())\n     return str(backup)\n \n \n # Fonctions de compatibilit\u00e9\n def generate_api_docs(blueprint: dict) -> str:\n     \"\"\"G\u00e9n\u00e8re la documentation API.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     return f\"\"\"# Documentation API - {project_name}\n \n ## Endpoints\n \n@@ -438,11 +431,11 @@\n \"\"\"\n \n \n def generate_dockerfile(blueprint: dict) -> str:\n     \"\"\"G\u00e9n\u00e8re un Dockerfile.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     return f\"\"\"# Dockerfile pour {project_name}\n FROM python:3.9-slim\n \n WORKDIR /app\n@@ -458,11 +451,11 @@\n \"\"\"\n \n \n def generate_docker_compose(blueprint: dict) -> str:\n     \"\"\"G\u00e9n\u00e8re un docker-compose.yml.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     docker_compose = f\"\"\"version: '3.8'\n \n services:\n   {project_name}:\n@@ -472,6 +465,6 @@\n     volumes:\n       - .:/app\n     environment:\n       - DEBUG=true\n \"\"\"\n-    return docker_compose\n\\ No newline at end of file\n+    return docker_compose\n--- /Volumes/T7/athalia-dev-setup/athalia_core/onboarding.py\t2025-07-29 17:56:26.540000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/onboarding.py\t2025-07-29 18:12:21.390904+00:00\n@@ -6,28 +6,31 @@\n \"\"\"\n \n \n def generate_onboarding_md(blueprint, outdir):\n     from pathlib import Path\n+\n     outdir = Path(outdir)\n-    (outdir / 'ONBOARDING.f(f').write_text(\n-        '# Onboarding\\nProjet: ' + blueprint.get('project_name', '')\n+    (outdir / \"ONBOARDING.f(f\").write_text(\n+        \"# Onboarding\\nProjet: \" + blueprint.get(\"project_name\", \"\")\n     )\n-    return str(outdir / 'ONBOARDING.f(f')\n+    return str(outdir / \"ONBOARDING.f(f\")\n \n \n def generate_onboard_cli(blueprint, outdir):\n     from pathlib import Path\n+\n     outdir = Path(outdir)\n-    (outdir / 'onboard.f(f').write_text(\n-        '# Onboard CLI\\nProjet: ' + blueprint.get('project_name', '')\n+    (outdir / \"onboard.f(f\").write_text(\n+        \"# Onboard CLI\\nProjet: \" + blueprint.get(\"project_name\", \"\")\n     )\n-    return str(outdir / 'onboard.f(f')\n+    return str(outdir / \"onboard.f(f\")\n \n \n def generate_onboarding_html_advanced(blueprint, outdir):\n     from pathlib import Path\n+\n     outdir = Path(outdir)\n-    (outdir / 'ONBOARDING.html(f').write_text(\n-        '<html><body><h1>Onboarding avanc\u00e9</h1></body></html>'\n+    (outdir / \"ONBOARDING.html(f\").write_text(\n+        \"<html><body><h1>Onboarding avanc\u00e9</h1></body></html>\"\n     )\n-    return str(outdir / 'ONBOARDING.html(f')\n+    return str(outdir / \"ONBOARDING.html(f\")\n--- /Volumes/T7/athalia-dev-setup/athalia_core/generation_simple.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/generation_simple.py\t2025-07-29 18:12:21.417808+00:00\n@@ -11,34 +11,34 @@\n \n \n def generate_blueprint_mock(idea: str = \"\", *args, **kwargs):\n     \"\"\"G\u00e9n\u00e8re un blueprint mock pour les tests.\"\"\"\n     return {\n-        'project_name': extract_project_name(idea),\n-        'description': idea or 'Projet de test',\n-        'project_type': 'generic',\n-        'modules': ['core', 'tests'],\n-        'structure': ['src/', 'tests/', 'README.md'],\n-        'dependencies': ['numpy', 'pandas'],\n-        'prompts': ['prompts/main.yaml'],\n-        'booster_ia': True,\n-        'docker': False,\n-        'ci_cd': False,\n-        'tests': True,\n-        'documentation': True\n+        \"project_name\": extract_project_name(idea),\n+        \"description\": idea or \"Projet de test\",\n+        \"project_type\": \"generic\",\n+        \"modules\": [\"core\", \"tests\"],\n+        \"structure\": [\"src/\", \"tests/\", \"README.md\"],\n+        \"dependencies\": [\"numpy\", \"pandas\"],\n+        \"prompts\": [\"prompts/main.yaml\"],\n+        \"booster_ia\": True,\n+        \"docker\": False,\n+        \"ci_cd\": False,\n+        \"tests\": True,\n+        \"documentation\": True,\n     }\n \n \n def extract_project_name(idea: str) -> str:\n     \"\"\"Extrait un nom de projet de l'id\u00e9e.\"\"\"\n     # Cherche des mots cl\u00e9s sp\u00e9cifiques\n     patterns = [\n-        r'calculatrice\\s+(\\w+)',\n-        r'application\\s+(\\w+)',\n-        r'robot\\s+(\\w+)',\n-        r'api\\s+(\\w+)',\n-        r'(\\w+)\\s+avec'\n+        r\"calculatrice\\s+(\\w+)\",\n+        r\"application\\s+(\\w+)\",\n+        r\"robot\\s+(\\w+)\",\n+        r\"api\\s+(\\w+)\",\n+        r\"(\\w+)\\s+avec\",\n     ]\n \n     for pattern in patterns:\n         match = re.search(pattern, idea, re.IGNORECASE)\n         if match:\n@@ -53,18 +53,18 @@\n     return \"projet_ia\"\n \n \n def generate_project(blueprint: dict, outdir, *args, **kwargs):\n     \"\"\"G\u00e9n\u00e8re un projet \u00e0 partir d'un blueprint.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n     project_path = Path(outdir) / project_name\n     project_path.mkdir(parents=True, exist_ok=True)\n \n     # Cr\u00e9er la structure de base\n-    (project_path / 'src').mkdir(exist_ok=True)\n-    (project_path / 'tests').mkdir(exist_ok=True)\n-    (project_path / 'docs').mkdir(exist_ok=True)\n+    (project_path / \"src\").mkdir(exist_ok=True)\n+    (project_path / \"tests\").mkdir(exist_ok=True)\n+    (project_path / \"docs\").mkdir(exist_ok=True)\n \n     # G\u00e9n\u00e9rer les fichiers de base\n     generate_readme(blueprint, project_path)\n     generate_main_code(blueprint, project_path)\n     generate_test_code(blueprint, project_path)\n@@ -72,12 +72,12 @@\n     return str(project_path)\n \n \n def generate_readme(blueprint: dict, project_path: Path = None) -> str:\n     \"\"\"G\u00e9n\u00e8re un README basique.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n-    description = blueprint.get('description', 'Projet g\u00e9n\u00e9r\u00e9 par Athalia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n+    description = blueprint.get(\"description\", \"Projet g\u00e9n\u00e9r\u00e9 par Athalia\")\n \n     readme_content = f\"\"\"# {project_name}\n \n {description}\n \n@@ -102,22 +102,22 @@\n ---\n *G\u00e9n\u00e9r\u00e9 automatiquement par Athalia*\n \"\"\"\n \n     if project_path:\n-        readme_file = project_path / 'README.md'\n-        readme_file.write_text(readme_content, encoding='utf-8')\n+        readme_file = project_path / \"README.md\"\n+        readme_file.write_text(readme_content, encoding=\"utf-8\")\n \n     return readme_content\n \n \n def generate_main_code(blueprint: dict, project_path: Path = None) -> str:\n     \"\"\"G\u00e9n\u00e8re le code principal.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n-    project_type = blueprint.get('project_type', 'generic')\n-\n-    if project_type == 'api':\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n+    project_type = blueprint.get(\"project_type\", \"generic\")\n+\n+    if project_type == \"api\":\n         main_content = f\"\"\"#!/usr/bin/env python3\n \\\"\\\"\\\"\n {project_name} - API avec FastAPI\n \\\"\\\"\\\"\n \n@@ -170,20 +170,20 @@\n if __name__ == \"__main__\":\n     main()\n \"\"\"\n \n     if project_path:\n-        main_file = project_path / 'src' / 'main.py'\n+        main_file = project_path / \"src\" / \"main.py\"\n         main_file.parent.mkdir(exist_ok=True)\n-        main_file.write_text(main_content, encoding='utf-8')\n+        main_file.write_text(main_content, encoding=\"utf-8\")\n \n     return main_content\n \n \n def generate_test_code(blueprint: dict, project_path: Path = None) -> str:\n     \"\"\"G\u00e9n\u00e8re le code de test.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     test_content = f\"\"\"#!/usr/bin/env python3\n \\\"\\\"\\\"\n Tests pour {project_name}\n \\\"\\\"\\\"\n@@ -225,13 +225,13 @@\n if __name__ == '__main__':\n     unittest.main()\n \"\"\"\n \n     if project_path:\n-        test_file = project_path / 'tests' / 'test_main.py'\n+        test_file = project_path / \"tests\" / \"test_main.py\"\n         test_file.parent.mkdir(exist_ok=True)\n-        test_file.write_text(test_content, encoding='utf-8')\n+        test_file.write_text(test_content, encoding=\"utf-8\")\n \n     return test_content\n \n \n def save_blueprint(blueprint: dict, outdir):\n@@ -240,99 +240,94 @@\n     import yaml\n \n     outdir = Path(outdir)\n     outdir.mkdir(parents=True, exist_ok=True)\n \n-    blueprint_file = outdir / 'blueprint.yaml'\n-    with open(blueprint_file, 'w', encoding='utf-8') as f:\n+    blueprint_file = outdir / \"blueprint.yaml\"\n+    with open(blueprint_file, \"w\", encoding=\"utf-8\") as f:\n         yaml.dump(blueprint, f, allow_unicode=True)\n \n     return str(blueprint_file)\n \n \n def inject_booster_ia_elements(outdir):\n     \"\"\"Injecte les \u00e9l\u00e9ments Booster IA.\"\"\"\n     from pathlib import Path\n \n     outdir = Path(outdir)\n-    (outdir / 'booster_ia.txt').write_text('Booster IA inject\u00e9')\n-    (outdir / 'prompts').mkdir(exist_ok=True)\n-    (outdir / 'setup').mkdir(exist_ok=True)\n-    (outdir / 'agents').mkdir(exist_ok=True)\n-\n-    return str(outdir / 'booster_ia.txt')\n+    (outdir / \"booster_ia.txt\").write_text(\"Booster IA inject\u00e9\")\n+    (outdir / \"prompts\").mkdir(exist_ok=True)\n+    (outdir / \"setup\").mkdir(exist_ok=True)\n+    (outdir / \"agents\").mkdir(exist_ok=True)\n+\n+    return str(outdir / \"booster_ia.txt\")\n \n \n def scan_existing_project(outdir):\n     \"\"\"Scanne un projet existant.\"\"\"\n     from pathlib import Path\n \n     outdir = Path(outdir)\n     files = {\n-        f.name: True for f in outdir.iterdir()\n-        if f.is_file() and f.name in [\n-            'README.md',\n-            'test_module.py',\n-            'onboarding.md',\n-            'script.py'\n-        ]\n+        f.name: True\n+        for f in outdir.iterdir()\n+        if f.is_file()\n+        and f.name in [\"README.md\", \"test_module.py\", \"onboarding.md\", \"script.py\"]\n     }\n-    files['Modules trouv\u00e9s: test_module.py'] = True\n+    files[\"Modules trouv\u00e9s: test_module.py\"] = True\n     return files\n \n \n def merge_or_suffix_file(\n     file_path: str,\n     content: str,\n     file_type: Optional[str] = None,\n-    section_header: Optional[str] = None\n+    section_header: Optional[str] = None,\n ):\n     \"\"\"Fusionne ou suffixe un fichier.\"\"\"\n     from pathlib import Path\n \n     file = Path(file_path)\n     action = None\n \n     if not file.exists():\n         file.write_text(content)\n-        action = 'created'\n+        action = \"created\"\n         return str(file), action\n     else:\n         if section_header is not None and isinstance(section_header, str):\n-            file.write_text(\n-                file.read_text() + f\"\\n{section_header}\\n{content}\")\n-            action = 'merged'\n+            file.write_text(file.read_text() + f\"\\n{section_header}\\n{content}\")\n+            action = \"merged\"\n             return str(file), action\n-        elif (file_type is not None\n-              and file_type in ['test', 'prompt', 'onboarding']):\n-            file.write_text(file.read_text() + '\\n' + content)\n-            action = f'merged-{file_type}'\n+        elif file_type is not None and file_type in [\"test\", \"prompt\", \"onboarding\"]:\n+            file.write_text(file.read_text() + \"\\n\" + content)\n+            action = f\"merged-{file_type}\"\n             return str(file), action\n         else:\n             if file.suffix:\n                 suffix_file = file.with_name(f\"{file.stem}_auto{file.suffix}\")\n             else:\n                 suffix_file = file.with_name(f\"{file.name}_auto\")\n             suffix_file.write_text(content)\n-            action = 'suffixed'\n+            action = \"suffixed\"\n             return str(suffix_file), action\n \n \n def backup_file(file_path: str):\n     \"\"\"Cr\u00e9e une sauvegarde d'un fichier.\"\"\"\n     from pathlib import Path\n \n     file = Path(file_path)\n-    backup = file.with_suffix(file.suffix + '.backup')\n+    backup = file.with_suffix(file.suffix + \".backup\")\n     backup.write_text(file.read_text())\n     return str(backup)\n \n \n # Fonctions de compatibilit\u00e9\n def generate_api_docs(blueprint: dict) -> str:\n     \"\"\"G\u00e9n\u00e8re la documentation API.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     return f\"\"\"# Documentation API - {project_name}\n \n ## Endpoints\n \n@@ -374,11 +369,11 @@\n \"\"\"\n \n \n def generate_dockerfile(blueprint: dict) -> str:\n     \"\"\"G\u00e9n\u00e8re un Dockerfile.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     return f\"\"\"# Dockerfile pour {project_name}\n FROM python:3.9-slim\n \n WORKDIR /app\n@@ -394,11 +389,11 @@\n \"\"\"\n \n \n def generate_docker_compose(blueprint: dict) -> str:\n     \"\"\"G\u00e9n\u00e8re un docker-compose.yml.\"\"\"\n-    project_name = blueprint.get('project_name', 'projet_ia')\n+    project_name = blueprint.get(\"project_name\", \"projet_ia\")\n \n     return f\"\"\"version: '3.8'\n \n services:\n   {project_name}:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_analyzer.py\t2025-07-29 17:56:27.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_analyzer.py\t2025-07-29 18:12:21.451324+00:00\n@@ -22,20 +22,22 @@\n from .performance_analyzer import PerformanceAnalyzer\n \n # Import de l'orchestrateur unifi\u00e9 (optionnel)\n try:\n     from .unified_orchestrator import UnifiedOrchestrator\n+\n     UNIFIED_ORCHESTRATOR_AVAILABLE = True\n except ImportError:\n     UNIFIED_ORCHESTRATOR_AVAILABLE = False\n \n logger = logging.getLogger(__name__)\n \n \n @dataclass\n class ComprehensiveAnalysis:\n     \"\"\"Analyse compl\u00e8te du projet\"\"\"\n+\n     project_name: str\n     analysis_date: datetime\n     ast_analysis: Dict[str, Any]\n     pattern_analysis: Dict[str, Any]\n     architecture_analysis: Dict[str, Any]\n@@ -58,11 +60,12 @@\n         self.performance_analyzer = PerformanceAnalyzer(self.root_path)\n \n         logger.info(f\"\ud83e\udde0 Intelligent Analyzer initialis\u00e9 dans {self.root_path}\")\n \n     def analyze_project_comprehensive(\n-            self, project_path: str = None) -> ComprehensiveAnalysis:\n+        self, project_path: str = None\n+    ) -> ComprehensiveAnalysis:\n         \"\"\"Analyser un projet de mani\u00e8re compl\u00e8te avec tous les modules\"\"\"\n         project_path = Path(project_path or self.root_path)\n         project_name = project_path.name\n \n         logger.info(f\"\ud83e\udde0 Analyse compl\u00e8te du projet: {project_name}\")\n@@ -71,25 +74,26 @@\n         logger.info(\"\ud83d\udcca \u00c9tape 1/4: Analyse AST de base...\")\n         ast_analysis = self._perform_ast_analysis(project_path)\n \n         # 2. Analyse des patterns et doublons\n         logger.info(\"\ud83d\udd0d \u00c9tape 2/4: Analyse des patterns et doublons...\")\n-        pattern_analysis = self.pattern_detector.analyze_project_patterns(\n-            project_path)\n+        pattern_analysis = self.pattern_detector.analyze_project_patterns(project_path)\n \n         # 3. Analyse d'architecture\n         logger.info(\"\ud83c\udfd7\ufe0f \u00c9tape 3/4: Analyse d'architecture...\")\n         architecture_analysis = self.architecture_analyzer.analyze_entire_architecture()\n \n         # 4. Analyse de performance\n         logger.info(\"\u26a1 \u00c9tape 4/4: Analyse de performance...\")\n         performance_analysis = self.performance_analyzer.analyze_project_performance(\n-            project_path)\n+            project_path\n+        )\n \n         # Calculer le score global\n         overall_score = self._calculate_overall_score(\n-            ast_analysis, pattern_analysis, architecture_analysis, performance_analysis)\n+            ast_analysis, pattern_analysis, architecture_analysis, performance_analysis\n+        )\n \n         # G\u00e9n\u00e9rer les recommandations globales\n         recommendations = self._generate_comprehensive_recommendations(\n             pattern_analysis, architecture_analysis, performance_analysis\n         )\n@@ -107,18 +111,19 @@\n             pattern_analysis=pattern_analysis,\n             architecture_analysis=architecture_analysis,\n             performance_analysis=performance_analysis,\n             overall_score=overall_score,\n             recommendations=recommendations,\n-            optimization_plan=optimization_plan\n+            optimization_plan=optimization_plan,\n         )\n \n         # Sauvegarder l'analyse compl\u00e8te\n         self._save_comprehensive_analysis(comprehensive_analysis)\n \n         logger.info(\n-            f\"\u2705 Analyse compl\u00e8te termin\u00e9e - Score global: {overall_score:.1f}/100\")\n+            f\"\u2705 Analyse compl\u00e8te termin\u00e9e - Score global: {overall_score:.1f}/100\"\n+        )\n \n         return comprehensive_analysis\n \n     def _perform_ast_analysis(self, project_path: Path) -> Dict[str, Any]:\n         \"\"\"Effectuer l'analyse AST de base\"\"\"\n@@ -127,40 +132,44 @@\n \n         for py_file in python_files:\n             try:\n                 file_analysis = self.ast_analyzer.analyze_file(py_file)\n                 if file_analysis:\n-                    file_analyses.append({\n-                        \"file_path\": str(file_analysis.file_path),\n-                        \"functions_count\": len(file_analysis.functions),\n-                        \"classes_count\": len(file_analysis.classes),\n-                        \"complexity_score\": file_analysis.complexity_score,\n-                        \"total_lines\": file_analysis.total_lines\n-                    })\n+                    file_analyses.append(\n+                        {\n+                            \"file_path\": str(file_analysis.file_path),\n+                            \"functions_count\": len(file_analysis.functions),\n+                            \"classes_count\": len(file_analysis.classes),\n+                            \"complexity_score\": file_analysis.complexity_score,\n+                            \"total_lines\": file_analysis.total_lines,\n+                        }\n+                    )\n             except Exception as e:\n-                logger.warning(\n-                    f\"Erreur lors de l'analyse AST de {py_file}: {e}\")\n+                logger.warning(f\"Erreur lors de l'analyse AST de {py_file}: {e}\")\n \n         return {\n             \"files_analyzed\": len(file_analyses),\n             \"total_files\": len(python_files),\n             \"file_details\": file_analyses,\n             \"summary\": {\n                 \"total_functions\": sum(f[\"functions_count\"] for f in file_analyses),\n                 \"total_classes\": sum(f[\"classes_count\"] for f in file_analyses),\n                 \"average_complexity\": (\n-                    sum(f[\"complexity_score\"] for f in file_analyses) / len(file_analyses)\n-                    if file_analyses else 0\n-                )\n-            }\n+                    sum(f[\"complexity_score\"] for f in file_analyses)\n+                    / len(file_analyses)\n+                    if file_analyses\n+                    else 0\n+                ),\n+            },\n         }\n \n     def _calculate_overall_score(\n-        self, ast_analysis: Dict[str, Any],\n+        self,\n+        ast_analysis: Dict[str, Any],\n         pattern_analysis: Dict[str, Any],\n         architecture_analysis: Any,\n-        performance_analysis: Any\n+        performance_analysis: Any,\n     ) -> float:\n         \"\"\"Calculer le score global bas\u00e9 sur toutes les analyses\"\"\"\n         scores = []\n         weights = []\n \n@@ -193,42 +202,40 @@\n         perf_score = performance_analysis.overall_score\n         scores.append(perf_score)\n         weights.append(3.5)\n \n         # Calculer la moyenne pond\u00e9r\u00e9e\n-        total_score = sum(\n-            score\n-            * weight for score,\n-            weight in zip(\n-                scores,\n-                weights))\n+        total_score = sum(score * weight for score, weight in zip(scores, weights))\n         total_weight = sum(weights)\n \n         return total_score / total_weight if total_weight > 0 else 100.0\n \n     def _generate_comprehensive_recommendations(\n-        self, pattern_analysis: Dict[str, Any],\n+        self,\n+        pattern_analysis: Dict[str, Any],\n         architecture_analysis: Any,\n-        performance_analysis: Any\n+        performance_analysis: Any,\n     ) -> List[str]:\n         \"\"\"G\u00e9n\u00e9rer des recommandations globales\"\"\"\n         recommendations = []\n \n         # Recommandations des patterns\n         if pattern_analysis[\"duplicates\"]:\n             high_severity_duplicates = [\n-                d for d in pattern_analysis[\"duplicates\"]\n+                d\n+                for d in pattern_analysis[\"duplicates\"]\n                 if d.severity in [\"high\", \"medium\"]\n             ]\n             if high_severity_duplicates:\n                 recommendations.append(\n                     f\"\ud83d\udd27 {len(high_severity_duplicates)} doublons critiques - fusion prioritaire\"\n                 )\n \n         if pattern_analysis[\"antipatterns\"]:\n             high_impact_antipatterns = [\n-                a for a in pattern_analysis[\"antipatterns\"]\n+                a\n+                for a in pattern_analysis[\"antipatterns\"]\n                 if a.impact in [\"high\", \"critical\"]\n             ]\n             if high_impact_antipatterns:\n                 recommendations.append(\n                     f\"\u26a0\ufe0f {len(high_impact_antipatterns)} anti-patterns critiques - \"\n@@ -243,11 +250,12 @@\n             )\n \n         # Recommandations de performance\n         if performance_analysis.issues:\n             high_impact_perf_issues = [\n-                i for i in performance_analysis.issues\n+                i\n+                for i in performance_analysis.issues\n                 if i.impact in [\"high\", \"critical\"]\n             ]\n             if high_impact_perf_issues:\n                 recommendations.append(\n                     f\"\u26a1 {len(high_impact_perf_issues)} probl\u00e8mes de performance critiques\"\n@@ -263,88 +271,98 @@\n             )\n \n         return recommendations\n \n     def _create_optimization_plan(\n-        self, pattern_analysis: Dict[str, Any],\n+        self,\n+        pattern_analysis: Dict[str, Any],\n         architecture_analysis: Any,\n-        performance_analysis: Any\n+        performance_analysis: Any,\n     ) -> Dict[str, Any]:\n         \"\"\"Cr\u00e9er un plan d'optimisation global\"\"\"\n         plan = {\n             \"priority_tasks\": [],\n             \"medium_priority_tasks\": [],\n             \"low_priority_tasks\": [],\n             \"estimated_effort\": 0,\n-            \"expected_improvement\": 0\n+            \"expected_improvement\": 0,\n         }\n \n         # T\u00e2ches prioritaires (impact \u00e9lev\u00e9)\n         if pattern_analysis[\"duplicates\"]:\n             high_severity_duplicates = [\n-                d for d in pattern_analysis[\"duplicates\"]\n-                if d.severity == \"high\"\n+                d for d in pattern_analysis[\"duplicates\"] if d.severity == \"high\"\n             ]\n             if high_severity_duplicates:\n-                plan[\"priority_tasks\"].append({\n-                    \"task\": \"merge_high_severity_duplicates\",\n-                    \"description\": f\"Fusionner {len(high_severity_duplicates)} doublons critiques\",\n-                    \"effort\": \"high\",\n-                    \"impact\": \"high\"\n-                })\n+                plan[\"priority_tasks\"].append(\n+                    {\n+                        \"task\": \"merge_high_severity_duplicates\",\n+                        \"description\": (\n+                            f\"Fusionner {len(high_severity_duplicates)} doublons critiques\"\n+                        ),\n+                        \"effort\": \"high\",\n+                        \"impact\": \"high\",\n+                    }\n+                )\n                 # heures\n                 plan[\"estimated_effort\"] += len(high_severity_duplicates) * 2\n \n         if performance_analysis.issues:\n             critical_perf_issues = [\n-                i for i in performance_analysis.issues\n-                if i.impact == \"critical\"\n+                i for i in performance_analysis.issues if i.impact == \"critical\"\n             ]\n             if critical_perf_issues:\n-                plan[\"priority_tasks\"].append({\n-                    \"task\": \"fix_critical_performance_issues\",\n-                    \"description\": (f\"Corriger {len(critical_perf_issues)} \"\n-                                    f\"probl\u00e8mes de performance critiques\"),\n-                    \"effort\": \"high\",\n-                    \"impact\": \"high\"\n-                })\n+                plan[\"priority_tasks\"].append(\n+                    {\n+                        \"task\": \"fix_critical_performance_issues\",\n+                        \"description\": (\n+                            f\"Corriger {len(critical_perf_issues)} \"\n+                            f\"probl\u00e8mes de performance critiques\"\n+                        ),\n+                        \"effort\": \"high\",\n+                        \"impact\": \"high\",\n+                    }\n+                )\n                 # heures\n                 plan[\"estimated_effort\"] += len(critical_perf_issues) * 3\n \n         # T\u00e2ches de priorit\u00e9 moyenne\n         if pattern_analysis[\"antipatterns\"]:\n             medium_impact_antipatterns = [\n-                a for a in pattern_analysis[\"antipatterns\"]\n-                if a.impact == \"medium\"\n+                a for a in pattern_analysis[\"antipatterns\"] if a.impact == \"medium\"\n             ]\n             if medium_impact_antipatterns:\n-                plan[\"medium_priority_tasks\"].append({\n-                    \"task\": \"refactor_medium_impact_antipatterns\",\n-                    \"description\": f\"Refactoriser {len(medium_impact_antipatterns)} anti-patterns\",\n-                    \"effort\": \"medium\",\n-                    \"impact\": \"medium\"\n-                })\n+                plan[\"medium_priority_tasks\"].append(\n+                    {\n+                        \"task\": \"refactor_medium_impact_antipatterns\",\n+                        \"description\": (\n+                            f\"Refactoriser {len(medium_impact_antipatterns)} anti-patterns\"\n+                        ),\n+                        \"effort\": \"medium\",\n+                        \"impact\": \"medium\",\n+                    }\n+                )\n                 # heures\n-                plan[\"estimated_effort\"] += len(\n-                    medium_impact_antipatterns) * 1.5\n+                plan[\"estimated_effort\"] += len(medium_impact_antipatterns) * 1.5\n \n         # Calculer l'am\u00e9lioration attendue\n         total_improvement = 0\n         if performance_analysis.issues:\n             total_improvement += sum(\n-                i.estimated_improvement for i in performance_analysis.issues)\n+                i.estimated_improvement for i in performance_analysis.issues\n+            )\n \n         plan[\"expected_improvement\"] = total_improvement\n \n         return plan\n \n     def _save_comprehensive_analysis(self, analysis: ComprehensiveAnalysis):\n         \"\"\"Sauvegarder l'analyse compl\u00e8te\"\"\"\n         output_file = (\n-            self.root_path / \"data\"\n-            / f\"comprehensive_analysis_{analysis.project_name}_\"\n-            f\"{analysis.analysis_date.strftime('%Y%m%d_%H%M%S')}.json\")\n+            self.root_path / \"data\" / f\"comprehensive_analysis_{analysis.project_name}_\"\n+            f\"{analysis.analysis_date.strftime('%Y%m%d_%H%M%S')}.json\"\n+        )\n \n         # Convertir en dictionnaire pour la s\u00e9rialisation JSON\n         analysis_dict = {\n             \"project_name\": analysis.project_name,\n             \"analysis_date\": analysis.analysis_date.isoformat(),\n@@ -352,87 +370,81 @@\n             \"recommendations\": analysis.recommendations,\n             \"optimization_plan\": analysis.optimization_plan,\n             \"ast_analysis\": analysis.ast_analysis,\n             \"pattern_analysis\": {\n                 \"summary\": analysis.pattern_analysis[\"summary\"],\n-                \"duplicates_count\": len(\n-                    analysis.pattern_analysis[\"duplicates\"]),\n-                \"antipatterns_count\": len(\n-                    analysis.pattern_analysis[\"antipatterns\"])},\n+                \"duplicates_count\": len(analysis.pattern_analysis[\"duplicates\"]),\n+                \"antipatterns_count\": len(analysis.pattern_analysis[\"antipatterns\"]),\n+            },\n             \"architecture_analysis\": {\n-                \"modules_count\": len(\n-                    analysis.architecture_analysis.modules),\n+                \"modules_count\": len(analysis.architecture_analysis.modules),\n                 \"performance_issues_count\": len(\n-                    analysis.architecture_analysis.performance_issues)},\n+                    analysis.architecture_analysis.performance_issues\n+                ),\n+            },\n             \"performance_analysis\": {\n                 \"overall_score\": analysis.performance_analysis.overall_score,\n-                \"issues_count\": len(\n-                    analysis.performance_analysis.issues)}}\n+                \"issues_count\": len(analysis.performance_analysis.issues),\n+            },\n+        }\n \n         output_file.parent.mkdir(parents=True, exist_ok=True)\n-        with open(output_file, 'w', encoding='utf-8') as f:\n+        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n             json.dump(analysis_dict, f, indent=2, ensure_ascii=False)\n \n         logger.info(f\"\ud83d\udcbe Analyse sauvegard\u00e9e dans {output_file}\")\n \n     def get_learning_insights(self) -> Dict[str, Any]:\n         \"\"\"Obtenir des insights d'apprentissage de tous les modules\"\"\"\n         return {\n             \"ast_insights\": \"Analyse AST de base disponible\",\n             \"pattern_insights\": self.pattern_detector.get_learning_insights(),\n             \"architecture_insights\": self.architecture_analyzer.get_optimization_plan(),\n-            \"performance_insights\": self.performance_analyzer.get_performance_insights()}\n+            \"performance_insights\": (\n+                self.performance_analyzer.get_performance_insights()\n+            ),\n+        }\n \n     def generate_intelligent_coordination(self) -> Dict[str, Any]:\n         \"\"\"G\u00e9n\u00e9rer une coordination intelligente\"\"\"\n         return {\n             \"timestamp\": datetime.now().isoformat(),\n             \"modules_available\": {\n                 \"ast_analyzer\": True,\n                 \"pattern_detector\": True,\n                 \"architecture_analyzer\": True,\n                 \"performance_analyzer\": True,\n-                \"unified_orchestrator\": UNIFIED_ORCHESTRATOR_AVAILABLE\n+                \"unified_orchestrator\": UNIFIED_ORCHESTRATOR_AVAILABLE,\n             },\n             \"recommendations\": [\n                 \"Utiliser l'analyse compl\u00e8te pour les projets complexes\",\n-                (\"Activer l'orchestrateur unifi\u00e9 pour \"\n-                 \"l'industrialisation compl\u00e8te\")\n-            ]\n+                (\"Activer l'orchestrateur unifi\u00e9 pour \" \"l'industrialisation compl\u00e8te\"),\n+            ],\n         }\n \n     def orchestrate_with_unified(\n-        self, project_path: str = None,\n-        config: Optional[Dict[str, Any]] = None\n+        self, project_path: str = None, config: Optional[Dict[str, Any]] = None\n     ) -> Dict[str, Any]:\n         \"\"\"Utiliser l'orchestrateur unifi\u00e9 pour une orchestration compl\u00e8te\"\"\"\n         if not UNIFIED_ORCHESTRATOR_AVAILABLE:\n             logger.warning(\n                 \"Orchestrateur unifi\u00e9 non disponible, utilisation de l'analyse standard\"\n             )\n             return self.analyze_project_comprehensive(project_path)\n \n         logger.info(\"\ud83c\udfaf Utilisation de l'orchestrateur unifi\u00e9\")\n         unified_orchestrator = UnifiedOrchestrator(self.root_path)\n-        return unified_orchestrator.orchestrate_project_complete(\n-            project_path, config)\n+        return unified_orchestrator.orchestrate_project_complete(project_path, config)\n \n \n def main():\n     \"\"\"Fonction principale pour l'analyse en ligne de commande\"\"\"\n     import argparse\n \n-    parser = argparse.ArgumentParser(\n-        description=\"Analyseur intelligent Athalia\")\n-    parser.add_argument(\n-        \"--project-path\",\n-        type=str,\n-        help=\"Chemin du projet \u00e0 analyser\")\n-    parser.add_argument(\n-        \"--output\",\n-        type=str,\n-        help=\"Fichier de sortie pour le rapport\")\n+    parser = argparse.ArgumentParser(description=\"Analyseur intelligent Athalia\")\n+    parser.add_argument(\"--project-path\", type=str, help=\"Chemin du projet \u00e0 analyser\")\n+    parser.add_argument(\"--output\", type=str, help=\"Fichier de sortie pour le rapport\")\n \n     args = parser.parse_args()\n \n     # Initialiser l'analyseur\n     analyzer = IntelligentAnalyzer()\n@@ -447,30 +459,31 @@\n     print(f\"Date d'analyse: {analysis.analysis_date}\")\n \n     print(\"\\n\ud83d\udcca R\u00c9SUM\u00c9:\")\n     print(f\"- Fichiers analys\u00e9s: {analysis.ast_analysis['files_analyzed']}\")\n     print(\n-        f\"- Doublons d\u00e9tect\u00e9s: {analysis.pattern_analysis['summary']['total_duplicates']}\")\n+        f\"- Doublons d\u00e9tect\u00e9s: {analysis.pattern_analysis['summary']['total_duplicates']}\"\n+    )\n     print(\n-        f\"- Anti-patterns: {analysis.pattern_analysis['summary']['total_antipatterns']}\")\n-    print(\n-        f\"- Probl\u00e8mes de performance: {len(analysis.performance_analysis['issues'])}\")\n+        f\"- Anti-patterns: {analysis.pattern_analysis['summary']['total_antipatterns']}\"\n+    )\n+    print(f\"- Probl\u00e8mes de performance: {len(analysis.performance_analysis['issues'])}\")\n \n     print(\"\\n\ud83d\udca1 RECOMMANDATIONS:\")\n     for i, rec in enumerate(analysis.recommendations, 1):\n         print(f\"{i}. {rec}\")\n \n     print(\"\\n\ud83d\ude80 PLAN D'OPTIMISATION:\")\n     plan = analysis.optimization_plan\n     print(f\"- Effort estim\u00e9: {plan['estimated_effort']:.1f} heures\")\n     print(f\"- Am\u00e9lioration attendue: {plan['expected_improvement']:.1f}%\")\n \n-    if plan['priority_tasks']:\n+    if plan[\"priority_tasks\"]:\n         print(f\"- T\u00e2ches prioritaires: {len(plan['priority_tasks'])}\")\n \n     if args.output:\n-        with open(args.output, 'w', encoding='utf-8') as f:\n+        with open(args.output, \"w\", encoding=\"utf-8\") as f:\n             json.dump(asdict(analysis), f, indent=2, ensure_ascii=False)\n         print(f\"\\n\ud83d\udcbe Rapport sauvegard\u00e9 dans {args.output}\")\n \n \n if __name__ == \"__main__\":\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ready_check.py\t2025-07-29 17:56:27.940000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ready_check.py\t2025-07-29 18:12:21.470597+00:00\n@@ -1,18 +1,19 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n import os\n import logging\n import builtins\n+\n _real_open = builtins.open\n \n logger = logging.getLogger(__name__)\n \n \n-def open_patch(file, mode='r', *args, **kwargs):\n-    if mode == 'f':\n-        mode = 'w'\n+def open_patch(file, mode=\"r\", *args, **kwargs):\n+    if mode == \"f\":\n+        mode = \"w\"\n     return _real_open(file, mode, *args, **kwargs)\n \n \n builtins.open = open_patch\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/__init__.py\t2025-07-29 17:56:31.360000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/__init__.py\t2025-07-29 18:12:21.477874+00:00\n@@ -22,13 +22,13 @@\n from .docker_robotics import DockerRoboticsManager\n from .rust_analyzer import RustAnalyzer\n from .robotics_ci import RoboticsCI\n \n __all__ = [\n-    'ReachyAuditor',\n-    'ROS2Validator',\n-    'DockerRoboticsManager',\n-    'RustAnalyzer',\n-    'RoboticsCI'\n+    \"ReachyAuditor\",\n+    \"ROS2Validator\",\n+    \"DockerRoboticsManager\",\n+    \"RustAnalyzer\",\n+    \"RoboticsCI\",\n ]\n \n __version__ = \"1.0.0\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/main.py\t2025-07-29 18:02:53.620000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/main.py\t2025-07-29 18:12:21.477225+00:00\n@@ -1,11 +1,15 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n from athalia_core.ci import generate_github_ci_yaml, add_coverage_badge\n from athalia_core.cleanup import clean_old_tests_and_caches\n+\n # from athalia_core.dashboard import generate_dashboard_html, generate_multi_project_mermaid\n-from athalia_core.onboarding import generate_onboard_cli, generate_onboarding_html_advanced\n+from athalia_core.onboarding import (\n+    generate_onboard_cli,\n+    generate_onboarding_html_advanced,\n+)\n from athalia_core.security import security_audit_project\n import os\n from datetime import datetime\n import logging\n import shutil\n@@ -18,11 +22,11 @@\n except ImportError:\n     # Fallback vers le logging standard si le module avanc\u00e9 n'est pas'\n     # disponible\n     athalia_logger = None\n \n-    def log_main(msg, level='INFO', **kwargs):\n+    def log_main(msg, level=\"INFO\", **kwargs):\n         logging.getLogger(__name__).info(msg)\n \n \n \"\"\"\n Point d'entr\u00e9e CLI du pipeline Athalia.\n@@ -87,26 +91,29 @@\n def main(test_mode=False):\n     global running\n \n     # V\u00e9rifier si une instance est d\u00e9j\u00e0 en cours\n     import psutil\n+\n     current_pid = os.getpid()\n     athalia_processes = []\n \n-    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n+    for proc in psutil.process_iter([\"pid\", \"name\", \"cmdline\"]):\n         try:\n-            if proc.info['cmdline'] and 'athalia_core.main' in ' '.join(\n-                    proc.info['cmdline']):\n-                if proc.info['pid'] != current_pid:\n-                    athalia_processes.append(proc.info['pid'])\n+            if proc.info[\"cmdline\"] and \"athalia_core.main\" in \" \".join(\n+                proc.info[\"cmdline\"]\n+            ):\n+                if proc.info[\"pid\"] != current_pid:\n+                    athalia_processes.append(proc.info[\"pid\"])\n         except (psutil.NoSuchProcess, psutil.AccessDenied):\n             pass\n \n     if athalia_processes:\n         logger.warning(\n             f\"\u26a0\ufe0f {len(athalia_processes)} autre(s) instance(s) d'athalia_core.main \"\n-            f\"d\u00e9tect\u00e9e(s): {athalia_processes}\")\n+            f\"d\u00e9tect\u00e9e(s): {athalia_processes}\"\n+        )\n         if not test_mode:\n             logger.info(\"\ud83d\udd04 Arr\u00eat des instances pr\u00e9c\u00e9dentes...\")\n             for pid in athalia_processes:\n                 try:\n                     psutil.Process(pid).terminate()\n@@ -128,54 +135,57 @@\n         logger.info(\"\ud83d\udca1 Conseil: Utilisez Ctrl+C pour un arr\u00eat propre\")\n \n     while running:\n         try:\n             choix = menu()\n-            if choix == '1':\n+            if choix == \"1\":\n                 idea = safe_input(\"D\u00e9cris ton projet IA en une phrase : \")\n                 if not idea:\n                     logger.info(\"Description requise.\")\n                     continue\n                 # blueprint = generate_blueprint_ia(idea)\n                 # outdir = blueprint['project_name']\n                 # save_blueprint(blueprint, outdir)\n                 # generate_project(blueprint, outdir)\n                 logger.info(\"Projet g\u00e9n\u00e9r\u00e9 dans le dossier sp\u00e9cifi\u00e9.\")\n-            elif choix == '2':\n+            elif choix == \"2\":\n                 outdir = safe_input(\"Nom du dossier projet \u00e0 nettoyer : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 clean_old_tests_and_caches(outdir)\n                 logger.info(f\"Nettoyage termin\u00e9 pour {outdir}\")\n-            elif choix == '3':\n+            elif choix == \"3\":\n                 outdir = safe_input(\"Nom du dossier projet pour la CI : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 generate_github_ci_yaml(outdir)\n                 add_coverage_badge(outdir)\n                 logger.info(f\"CI et badge coverage g\u00e9n\u00e9r\u00e9s pour {outdir}\")\n-            elif choix == '4':\n+            elif choix == \"4\":\n                 # Pour d\u00e9mo, dashboard sur tous les projets ia_project*\n                 projects_info = []\n-                for dict_data in os.listdir('.'):\n-                    if (os.path.isdir(dict_data)\n-                        and (dict_data.startswith('ia_project')\n-                             or dict_data.startswith('artistic_')\n-                             or dict_data.startswith('projet_'))):\n-                        projects_info.append({\n-                            'name': dict_data,\n-                            'date': datetime.now().strftime('%Y-%m-%d %H:%M'),\n-                            'tests': 'OK',\n-                            'perf': 'OK'\n-                        })\n+                for dict_data in os.listdir(\".\"):\n+                    if os.path.isdir(dict_data) and (\n+                        dict_data.startswith(\"ia_project\")\n+                        or dict_data.startswith(\"artistic_\")\n+                        or dict_data.startswith(\"projet_\")\n+                    ):\n+                        projects_info.append(\n+                            {\n+                                \"name\": dict_data,\n+                                \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n+                                \"tests\": \"OK\",\n+                                \"perf\": \"OK\",\n+                            }\n+                        )\n                 # generate_dashboard_html(projects_info) # Fonction non disponible\n                 # generate_multi_project_mermaid(projects_info) # Fonction non\n                 # disponible\n                 logger.info(\"Dashboard g\u00e9n\u00e9r\u00e9.\")\n-            elif choix == '5':\n+            elif choix == \"5\":\n                 outdir = safe_input(\"Nom du dossier projet pour onboarding : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 # blueprint = generate_blueprint_mock(\"Onboarding\")\n@@ -183,61 +193,62 @@\n                 # Il faut un blueprint ici, mais comme il n'est pas g\u00e9n\u00e9r\u00e9, on'\n                 # passe un dict vide pour \u00e9viter lerreur\n                 generate_onboard_cli({}, outdir)\n                 generate_onboarding_html_advanced({}, outdir)\n                 logger.info(f\"Guides d'onboarding g\u00e9n\u00e9r\u00e9s dans {outdir}\")\n-            elif choix == '6':\n-                outdir = safe_input(\n-                    \"Nom du dossier projet \u00e0 auditer (s\u00e9curit\u00e9) : \")\n+            elif choix == \"6\":\n+                outdir = safe_input(\"Nom du dossier projet \u00e0 auditer (s\u00e9curit\u00e9) : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 security_audit_project(outdir)\n                 logger.info(f\"Audit s\u00e9curit\u00e9 termin\u00e9 pour {outdir}\")\n-            elif choix == '7':\n+            elif choix == \"7\":\n                 outdir = safe_input(\"Nom du dossier projet \u00e0 scanner : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 # report = scan_existing_project(outdir)\n                 # if report:\n                 #     logger.info(\"Fichiers / dossiers critiques d\u00e9tect\u00e9s :\\n\" + \"\\n\".join(report))\n                 # else:\n                 #     logger.info(\"Aucun fichier critique d\u00e9tect\u00e9.\")\n                 logger.info(\"Scan termin\u00e9.\")\n-            elif choix == '8':\n+            elif choix == \"8\":\n                 idea = safe_input(\"D\u00e9cris ton projet IA (dry-run) : \")\n                 if not idea:\n                     logger.info(\"Description requise.\")\n                     continue\n                 # blueprint = generate_blueprint_ia(idea)\n                 # outdir = blueprint['project_name']\n                 # save_blueprint(blueprint, outdir)\n                 # actions = generate_project(blueprint, outdir, dry_run=True)\n                 logger.info(\"Simulation dry-run termin\u00e9e.\")\n-            elif choix == '9':\n-                outdir = safe_input(\n-                    \"Nom du dossier projet pour voir le rapport : \")\n+            elif choix == \"9\":\n+                outdir = safe_input(\"Nom du dossier projet pour voir le rapport : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 report_file = os.path.join(outdir, \"integration_report.log\")\n                 if os.path.exists(report_file):\n                     logger.info(open(report_file).read())\n                 else:\n                     logger.info(\"Aucun rapport dint\u00e9gration trouv\u00e9.\")\n-            elif choix == '10':\n+            elif choix == \"10\":\n                 outdir = safe_input(\"Nom du dossier projet \u00e0 rollback : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 backup_dir = os.path.join(outdir, \".backups\")\n                 if not os.path.exists(backup_dir):\n                     logger.info(\"Aucune sauvegarde trouv\u00e9e.\")\n                 else:\n-                    backups = [file_handle for file_handle in os.listdir(\n-                        backup_dir) if file_handle.endswith('.bak')]\n+                    backups = [\n+                        file_handle\n+                        for file_handle in os.listdir(backup_dir)\n+                        if file_handle.endswith(\".bak\")\n+                    ]\n                     if not backups:\n                         logger.info(\"Aucune sauvegarde .bak trouv\u00e9e.\")\n                     else:\n                         logger.info(\"Sauvegardes disponibles :\")\n                         for index, b in enumerate(backups):\n@@ -249,31 +260,30 @@\n                                 continue\n                             idx = int(idx_input) - 1\n                             if 0 <= idx < len(backups):\n                                 src = os.path.join(backup_dir, backups[idx])\n                                 dest = os.path.join(\n-                                    outdir, backups[idx].split('.bak')[0])\n+                                    outdir, backups[idx].split(\".bak\")[0]\n+                                )\n                                 shutil.copy2(src, dest)\n                                 logger.info(f\"Restaur\u00e9 {dest} depuis {src}\")\n                             else:\n                                 logger.info(\"Num\u00e9ro invalide.\")\n                         except (ValueError, IndexError):\n                             logger.info(\"Num\u00e9ro invalide.\")\n-            elif choix == '11':\n-                outdir = safe_input(\n-                    \"Nom du dossier projet pour voir les logs : \")\n+            elif choix == \"11\":\n+                outdir = safe_input(\"Nom du dossier projet pour voir les logs : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 log_file = os.path.join(outdir, \"integration_report.log\")\n                 if os.path.exists(log_file):\n                     logger.info(open(log_file).read())\n                 else:\n                     logger.info(\"Aucun log dint\u00e9gration trouv\u00e9.\")\n-            elif choix == '12':\n-                outdir = safe_input(\n-                    \"Nom du dossier projet \u00e0 auditer intelligemment : \")\n+            elif choix == \"12\":\n+                outdir = safe_input(\"Nom du dossier projet \u00e0 auditer intelligemment : \")\n                 if not outdir:\n                     logger.info(\"Nom de dossier requis.\")\n                     continue\n                 try:\n                     # report = generate_audit_report(outdir) # This line was\n@@ -282,18 +292,19 @@\n                     logger.info(\"\ud83d\udd0d RAPPORT DAUDIT\")\n                     logger.info(\"=\" * 50)\n                     # logger.info(report) # This line was removed as per the\n                     # edit hint.\n                     logger.info(\n-                        f\"\\nRapport d\u00e9taill\u00e9 sauvegard\u00e9 dans {outdir}/audit_report.json\")\n+                        f\"\\nRapport d\u00e9taill\u00e9 sauvegard\u00e9 dans {outdir}/audit_report.json\"\n+                    )\n                 except Exception as e:\n                     logger.info(f\"Erreur audit intelligent: {e}\")\n-            elif choix == '13':\n+            elif choix == \"13\":\n                 logger.info(\"Au revoir !\")\n                 running = False\n                 break\n-            elif choix == '14':\n+            elif choix == \"14\":\n                 surveillance_mode()\n             else:\n                 logger.info(\"Choix invalide.\")\n             if test_mode:\n                 break  # On sort apr\u00e8s un tour en mode test\n--- /Volumes/T7/athalia-dev-setup/athalia_core/plugins_validator.py\t2025-07-29 18:08:50.400000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/plugins_validator.py\t2025-07-29 18:12:21.523136+00:00\n@@ -22,29 +22,21 @@\n         self.plugins_dir = Path(plugins_dir)\n         self.validation_results = {\n             \"valid_plugins\": [],\n             \"invalid_plugins\": [],\n             \"warnings\": [],\n-            \"errors\": []\n+            \"errors\": [],\n         }\n \n     def validate_plugin(self, plugin_path: str) -> Dict[str, Any]:\n         \"\"\"Valide un plugin sp\u00e9cifique\"\"\"\n         plugin_path_obj = Path(plugin_path)\n-        \n+\n         if not plugin_path_obj.exists():\n-            return {\n-                \"valid\": False,\n-                \"errors\": [f\"Plugin {plugin_path} n'existe pas\"]\n-            }\n-\n-        results = {\n-            \"valid\": True,\n-            \"errors\": [],\n-            \"warnings\": [],\n-            \"metadata\": {}\n-        }\n+            return {\"valid\": False, \"errors\": [f\"Plugin {plugin_path} n'existe pas\"]}\n+\n+        results = {\"valid\": True, \"errors\": [], \"warnings\": [], \"metadata\": {}}\n \n         # V\u00e9rifier la structure du plugin\n         if not self._check_plugin_structure(plugin_path_obj, results):\n             results[\"valid\"] = False\n \n@@ -58,79 +50,89 @@\n         # V\u00e9rifier les d\u00e9pendances\n         self._check_dependencies(plugin_path_obj, results)\n \n         return results\n \n-    def _check_plugin_structure(self, plugin_path: Path, results: Dict[str, Any]) -> bool:\n+    def _check_plugin_structure(\n+        self, plugin_path: Path, results: Dict[str, Any]\n+    ) -> bool:\n         \"\"\"V\u00e9rifie la structure du plugin\"\"\"\n         required_files = [\"__init__.py\"]\n-        \n+\n         for file in required_files:\n             if not (plugin_path / file).exists():\n                 results[\"errors\"].append(f\"Fichier requis manquant: {file}\")\n                 return False\n-        \n+\n         return True\n \n     def _check_python_syntax(self, plugin_path: Path, results: Dict[str, Any]) -> bool:\n         \"\"\"V\u00e9rifie la syntaxe Python du plugin\"\"\"\n         python_files = list(plugin_path.rglob(\"*.py\"))\n-        \n+\n         for py_file in python_files:\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                \n+\n                 # V\u00e9rifier la syntaxe avec ast\n                 ast.parse(content)\n-                \n+\n             except SyntaxError as e:\n                 results[\"errors\"].append(f\"Erreur de syntaxe dans {py_file}: {e}\")\n                 return False\n             except Exception as e:\n                 results[\"warnings\"].append(f\"Impossible de v\u00e9rifier {py_file}: {e}\")\n-        \n+\n         return True\n \n     def _check_metadata(self, plugin_path: Path, results: Dict[str, Any]):\n         \"\"\"V\u00e9rifie les m\u00e9tadonn\u00e9es du plugin\"\"\"\n         metadata_files = [\"plugin.yaml\", \"plugin.yml\", \"plugin.json\"]\n-        \n+\n         for metadata_file in metadata_files:\n             metadata_path = plugin_path / metadata_file\n             if metadata_path.exists():\n                 try:\n-                    if metadata_file.endswith('.json'):\n-                        with open(metadata_path, 'r') as f:\n+                    if metadata_file.endswith(\".json\"):\n+                        with open(metadata_path, \"r\") as f:\n                             metadata = json.load(f)\n                     else:\n-                        with open(metadata_path, 'r') as f:\n+                        with open(metadata_path, \"r\") as f:\n                             metadata = yaml.safe_load(f)\n-                    \n+\n                     results[\"metadata\"] = metadata\n-                    \n+\n                     # V\u00e9rifier les champs requis\n                     required_fields = [\"name\", \"version\", \"description\"]\n                     for field in required_fields:\n                         if field not in metadata:\n-                            results[\"warnings\"].append(f\"Champ requis manquant: {field}\")\n-                    \n+                            results[\"warnings\"].append(\n+                                f\"Champ requis manquant: {field}\"\n+                            )\n+\n                     break\n                 except Exception as e:\n-                    results[\"warnings\"].append(f\"Impossible de lire {metadata_file}: {e}\")\n+                    results[\"warnings\"].append(\n+                        f\"Impossible de lire {metadata_file}: {e}\"\n+                    )\n \n     def _check_dependencies(self, plugin_path: Path, results: Dict[str, Any]):\n         \"\"\"V\u00e9rifie les d\u00e9pendances du plugin\"\"\"\n         requirements_files = [\"requirements.txt\", \"setup.py\", \"pyproject.toml\"]\n-        \n+\n         for req_file in requirements_files:\n             req_path = plugin_path / req_file\n             if req_path.exists():\n                 try:\n                     if req_file == \"requirements.txt\":\n-                        with open(req_path, 'r') as f:\n-                            deps = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n+                        with open(req_path, \"r\") as f:\n+                            deps = [\n+                                line.strip()\n+                                for line in f\n+                                if line.strip() and not line.startswith(\"#\")\n+                            ]\n                         results[\"metadata\"][\"dependencies\"] = deps\n                     # Autres formats peuvent \u00eatre ajout\u00e9s ici\n                 except Exception as e:\n                     results[\"warnings\"].append(f\"Impossible de lire {req_file}: {e}\")\n \n@@ -139,59 +141,68 @@\n         if not self.plugins_dir.exists():\n             return {\n                 \"valid_plugins\": [],\n                 \"invalid_plugins\": [],\n                 \"warnings\": [\"R\u00e9pertoire plugins n'existe pas\"],\n-                \"errors\": []\n+                \"errors\": [],\n             }\n \n         for plugin_dir in self.plugins_dir.iterdir():\n             if plugin_dir.is_dir() and (plugin_dir / \"__init__.py\").exists():\n                 result = self.validate_plugin(str(plugin_dir))\n-                \n+\n                 if result[\"valid\"]:\n-                    self.validation_results[\"valid_plugins\"].append({\n-                        \"path\": str(plugin_dir),\n-                        \"metadata\": result.get(\"metadata\", {})\n-                    })\n+                    self.validation_results[\"valid_plugins\"].append(\n+                        {\n+                            \"path\": str(plugin_dir),\n+                            \"metadata\": result.get(\"metadata\", {}),\n+                        }\n+                    )\n                 else:\n-                    self.validation_results[\"invalid_plugins\"].append({\n-                        \"path\": str(plugin_dir),\n-                        \"errors\": result[\"errors\"]\n-                    })\n-                \n+                    self.validation_results[\"invalid_plugins\"].append(\n+                        {\"path\": str(plugin_dir), \"errors\": result[\"errors\"]}\n+                    )\n+\n                 self.validation_results[\"warnings\"].extend(result[\"warnings\"])\n                 self.validation_results[\"errors\"].extend(result[\"errors\"])\n \n         return self.validation_results\n \n     def generate_validation_report(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un rapport de validation\"\"\"\n         report = []\n         report.append(\"# Rapport de Validation des Plugins\")\n         report.append(\"\")\n-        \n-        report.append(f\"## Plugins Valides ({len(self.validation_results['valid_plugins'])})\")\n+\n+        report.append(\n+            f\"## Plugins Valides ({len(self.validation_results['valid_plugins'])})\"\n+        )\n         for plugin in self.validation_results[\"valid_plugins\"]:\n             report.append(f\"- {plugin['path']}\")\n             if plugin.get(\"metadata\"):\n-                report.append(f\"  - Version: {plugin['metadata'].get('version', 'N/A')}\")\n-                report.append(f\"  - Description: {plugin['metadata'].get('description', 'N/A')}\")\n-        \n+                report.append(\n+                    f\"  - Version: {plugin['metadata'].get('version', 'N/A')}\"\n+                )\n+                report.append(\n+                    f\"  - Description: {plugin['metadata'].get('description', 'N/A')}\"\n+                )\n+\n         report.append(\"\")\n-        report.append(f\"## Plugins Invalides ({len(self.validation_results['invalid_plugins'])})\")\n+        report.append(\n+            f\"## Plugins Invalides ({len(self.validation_results['invalid_plugins'])})\"\n+        )\n         for plugin in self.validation_results[\"invalid_plugins\"]:\n             report.append(f\"- {plugin['path']}\")\n             for error in plugin[\"errors\"]:\n                 report.append(f\"  - \u274c {error}\")\n-        \n+\n         if self.validation_results[\"warnings\"]:\n             report.append(\"\")\n             report.append(\"## Avertissements\")\n             for warning in self.validation_results[\"warnings\"]:\n                 report.append(f\"- \u26a0\ufe0f {warning}\")\n-        \n+\n         return \"\\n\".join(report)\n \n \n def validate_plugin(plugin_path: str) -> Dict[str, Any]:\n     \"\"\"Fonction utilitaire pour valider un plugin\"\"\"\n@@ -200,6 +211,6 @@\n \n \n def validate_all_plugins(plugins_dir: str = \"plugins\") -> Dict[str, Any]:\n     \"\"\"Fonction utilitaire pour valider tous les plugins\"\"\"\n     validator = PluginValidator(plugins_dir)\n-    return validator.validate_all_plugins() \n\\ No newline at end of file\n+    return validator.validate_all_plugins()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/correction_optimizer.py\t2025-07-29 17:56:27.390000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/correction_optimizer.py\t2025-07-29 18:12:21.522050+00:00\n@@ -16,10 +16,11 @@\n \n \n @dataclass\n class CorrectionResult:\n     \"\"\"R\u00e9sultat d'une correction\"\"\"\n+\n     success: bool\n     original_content: str\n     corrected_content: str\n     corrections_applied: List[Dict[str, Any]]\n     duration: float\n@@ -36,45 +37,45 @@\n         self.performance_stats = defaultdict(list)\n \n         # Techniques de correction disponibles (seront impl\u00e9ment\u00e9es au besoin)\n         self.correction_techniques = []\n \n-    def optimize_correction(\n-            self,\n-            file_path: str,\n-            content: str) -> CorrectionResult:\n+    def optimize_correction(self, file_path: str, content: str) -> CorrectionResult:\n         \"\"\"Correction optimis\u00e9e multi-passes avec apprentissage\"\"\"\n         start_time = time.time()\n \n         try:\n             # Correction multi-passes\n             corrected_content = content\n             corrections_applied = []\n \n             # Pass 1: Corrections syntaxiques basiques (toujours appliqu\u00e9es)\n             corrected_content, basic_corrections = self._apply_basic_corrections(\n-                corrected_content)\n+                corrected_content\n+            )\n             corrections_applied.extend(basic_corrections)\n \n             # Pass 2: Corrections AST-based (seulement si n\u00e9cessaire)\n             try:\n                 ast.parse(corrected_content)\n                 # Le code compile, pas besoin de corrections AST\n                 pass\n             except SyntaxError:\n                 corrected_content, ast_corrections = self._apply_ast_corrections(\n-                    corrected_content)\n+                    corrected_content\n+                )\n                 corrections_applied.extend(ast_corrections)\n \n             # Pass 3: Corrections contextuelles (seulement si n\u00e9cessaire)\n             try:\n                 ast.parse(corrected_content)\n                 # Le code compile, pas besoin de corrections contextuelles\n                 pass\n             except SyntaxError:\n-                corrected_content, context_corrections = self._apply_contextual_corrections(\n-                    corrected_content)\n+                corrected_content, context_corrections = (\n+                    self._apply_contextual_corrections(corrected_content)\n+                )\n                 corrections_applied.extend(context_corrections)\n \n             # Pass 4: Validation finale\n             success = self._validate_correction(corrected_content)\n \n@@ -83,62 +84,67 @@\n                 file_path=file_path,\n                 correction_type=\"optimized_multi_pass\",\n                 success=success,\n                 old_content=content,\n                 new_content=corrected_content,\n-                duration=time.time() - start_time\n+                duration=time.time() - start_time,\n             )\n \n             # Apprentissage des patterns\n-            self._learn_from_correction(\n-                file_path, content, corrected_content, success)\n+            self._learn_from_correction(file_path, content, corrected_content, success)\n \n             return CorrectionResult(\n                 success=success,\n                 original_content=content,\n                 corrected_content=corrected_content,\n                 corrections_applied=corrections_applied,\n-                duration=time.time() - start_time\n+                duration=time.time() - start_time,\n             )\n \n         except Exception:\n             log_error(\n-                Exception(\"Correction failed\"),\n-                f\"correction_optimizer_{file_path}\")\n+                Exception(\"Correction failed\"), f\"correction_optimizer_{file_path}\"\n+            )\n             return CorrectionResult(\n                 success=False,\n                 original_content=content,\n                 corrected_content=content,\n                 corrections_applied=[],\n                 duration=time.time() - start_time,\n-                error_message=\"Correction failed\"\n+                error_message=\"Correction failed\",\n             )\n \n     def _apply_basic_corrections(\n-            self, content: str) -> Tuple[str, List[Dict[str, Any]]]:\n+        self, content: str\n+    ) -> Tuple[str, List[Dict[str, Any]]]:\n         \"\"\"Applique les corrections basiques\"\"\"\n         corrections = []\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n \n         # Correction d'indentation basique d'abord\n         for i, line in enumerate(lines):\n             original_line = line\n \n             # Si c'est une ligne de code (pas vide, pas commentaire) et qu'elle\n             # n'a pas d'indentation\n-            if (line.strip() and not line.strip().startswith('#')\n-                    and not line.startswith(' ')):\n+            if (\n+                line.strip()\n+                and not line.strip().startswith(\"#\")\n+                and not line.startswith(\" \")\n+            ):\n                 # Si la ligne pr\u00e9c\u00e9dente se termine par ':', ajouter\n                 # l'indentation\n-                if i > 0 and lines[i - 1].strip().endswith(':'):\n-                    line = '    ' + line\n-                    corrections.append({\n-                        'type': 'basic_indentation',\n-                        'line': i + 1,\n-                        'original': original_line,\n-                        'corrected': line\n-                    })\n+                if i > 0 and lines[i - 1].strip().endswith(\":\"):\n+                    line = \"    \" + line\n+                    corrections.append(\n+                        {\n+                            \"type\": \"basic_indentation\",\n+                            \"line\": i + 1,\n+                            \"original\": original_line,\n+                            \"corrected\": line,\n+                        }\n+                    )\n \n             lines[i] = line\n \n         # Correction des espaces mal plac\u00e9s\n         for i, line in enumerate(lines):\n@@ -147,72 +153,72 @@\n             # Supprimer les espaces en fin de ligne\n             line = line.rstrip()\n \n             # Corriger les espaces autour des op\u00e9rateurs (mais pas dans les cha\u00eenes)\n             # Utiliser une approche plus prudente\n-            if '=' in line and not line.strip().startswith('#'):\n+            if \"=\" in line and not line.strip().startswith(\"#\"):\n                 # \u00c9viter de corriger les assignations de param\u00e8tres par d\u00e9faut\n-                if 'def ' in line and '=' in line:\n+                if \"def \" in line and \"=\" in line:\n                     # Pour les param\u00e8tres de fonction, ne corriger que les\n                     # espaces\n-                    parts = line.split('=')\n+                    parts = line.split(\"=\")\n                     if len(parts) == 2:\n                         param_part = parts[0].strip()\n                         default_part = parts[1].strip()\n-                        line = param_part + ' = ' + default_part\n+                        line = param_part + \" = \" + default_part\n                 else:\n                     # Pour les autres assignations\n-                    parts = line.split('=')\n+                    parts = line.split(\"=\")\n                     if len(parts) == 2:\n                         var_part = parts[0].strip()\n                         value_part = parts[1].strip()\n-                        line = var_part + ' = ' + value_part\n+                        line = var_part + \" = \" + value_part\n \n             # Corriger les espaces autour des op\u00e9rateurs arithm\u00e9tiques\n-            line = re.sub(r'(\\w)\\s*([+\\-*/])\\s*(\\w)', r'\\1 \\2 \\3', line)\n+            line = re.sub(r\"(\\w)\\s*([+\\-*/])\\s*(\\w)\", r\"\\1 \\2 \\3\", line)\n \n             # Corriger les espaces dans les parenth\u00e8ses (mais pas ajouter de\n             # parenth\u00e8ses)\n-            line = re.sub(r'\\(\\s+', '(', line)\n-            line = re.sub(r'\\s+\\)', ')', line)\n+            line = re.sub(r\"\\(\\s+\", \"(\", line)\n+            line = re.sub(r\"\\s+\\)\", \")\", line)\n \n             # Corriger les espaces dans les param\u00e8tres de fonction\n-            if 'def ' in line and '(' in line and ')' in line:\n+            if \"def \" in line and \"(\" in line and \")\" in line:\n                 # Extraire la partie avant les parenth\u00e8ses\n-                before_paren = line[:line.find('(')]\n-                after_paren = line[line.find(')'):]\n-                params_part = line[line.find('(') + 1:line.find(')')]\n+                before_paren = line[: line.find(\"(\")]\n+                after_paren = line[line.find(\")\") :]\n+                params_part = line[line.find(\"(\") + 1 : line.find(\")\")]\n \n                 # Nettoyer les param\u00e8tres\n-                params = [p.strip()\n-                          for p in params_part.split(',') if p.strip()]\n-                cleaned_params = ', '.join(params)\n+                params = [p.strip() for p in params_part.split(\",\") if p.strip()]\n+                cleaned_params = \", \".join(params)\n \n                 # Reconstruire la ligne sans ajouter de parenth\u00e8ses en trop\n-                line = before_paren + '(' + cleaned_params + ')' + after_paren\n+                line = before_paren + \"(\" + cleaned_params + \")\" + after_paren\n \n                 # S'assurer qu'il n'y a pas de parenth\u00e8ses en trop\n-                while line.count('(') > line.count(')'):\n+                while line.count(\"(\") > line.count(\")\"):\n                     # Supprimer les parenth\u00e8ses en trop \u00e0 la fin\n-                    if line.endswith(')'):\n+                    if line.endswith(\")\"):\n                         line = line[:-1]\n                     else:\n                         break\n \n             if line != original_line:\n-                corrections.append({\n-                    'type': 'basic_spacing',\n-                    'line': i + 1,\n-                    'original': original_line,\n-                    'corrected': line\n-                })\n+                corrections.append(\n+                    {\n+                        \"type\": \"basic_spacing\",\n+                        \"line\": i + 1,\n+                        \"original\": original_line,\n+                        \"corrected\": line,\n+                    }\n+                )\n                 lines[i] = line\n \n-        return '\\n'.join(lines), corrections\n-\n-    def _apply_ast_corrections(\n-            self, content: str) -> Tuple[str, List[Dict[str, Any]]]:\n+        return \"\\n\".join(lines), corrections\n+\n+    def _apply_ast_corrections(self, content: str) -> Tuple[str, List[Dict[str, Any]]]:\n         \"\"\"Corrections bas\u00e9es sur l'analyse AST\"\"\"\n         corrections: List[Dict[str, Any]] = []\n \n         try:\n             # Essayer de parser le code\n@@ -220,25 +226,28 @@\n             return content, corrections\n         except SyntaxError as e:\n             # Analyser l'erreur de syntaxe\n             error_info = self._analyze_syntax_error(e, content)\n \n-            if error_info['type'] == 'indentation':\n-                corrected_content, indentation_corrections = self._fix_indentation_error(\n-                    content, error_info)\n+            if error_info[\"type\"] == \"indentation\":\n+                corrected_content, indentation_corrections = (\n+                    self._fix_indentation_error(content, error_info)\n+                )\n                 corrections.extend(indentation_corrections)\n                 return corrected_content, corrections\n \n-            elif error_info['type'] == 'bracket_balance':\n+            elif error_info[\"type\"] == \"bracket_balance\":\n                 corrected_content, bracket_corrections = self._fix_bracket_balance(\n-                    content, error_info)\n+                    content, error_info\n+                )\n                 corrections.extend(bracket_corrections)\n                 return corrected_content, corrections\n \n-            elif error_info['type'] == 'string_issue':\n+            elif error_info[\"type\"] == \"string_issue\":\n                 corrected_content, string_corrections = self._fix_string_issues(\n-                    content, error_info)\n+                    content, error_info\n+                )\n                 corrections.extend(string_corrections)\n                 return corrected_content, corrections\n \n             # Si on ne peut pas corriger sp\u00e9cifiquement, essayer les\n             # corrections g\u00e9n\u00e9rales\n@@ -246,335 +255,337 @@\n                 # Essayer de corriger les probl\u00e8mes courants\n                 corrected_content = content\n \n                 # Correction des parenth\u00e8ses manquantes\n                 corrected_content, bracket_corrections = self._fix_bracket_balance(\n-                    corrected_content, error_info)\n+                    corrected_content, error_info\n+                )\n                 corrections.extend(bracket_corrections)\n \n                 # Correction des cha\u00eenes\n                 corrected_content, string_corrections = self._fix_string_issues(\n-                    corrected_content, error_info)\n+                    corrected_content, error_info\n+                )\n                 corrections.extend(string_corrections)\n \n                 return corrected_content, corrections\n \n         return content, corrections\n \n     def _apply_contextual_corrections(\n-            self, content: str) -> Tuple[str, List[Dict[str, Any]]]:\n+        self, content: str\n+    ) -> Tuple[str, List[Dict[str, Any]]]:\n         \"\"\"Corrections contextuelles bas\u00e9es sur l'analyse du code\"\"\"\n         corrections = []\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n \n         # Analyse du contexte\n         context = self._analyze_context(lines)\n \n         # Corrections bas\u00e9es sur le contexte\n         for i, line in enumerate(lines):\n             original_line = line\n \n             # Correction des imports manquants\n-            if context['missing_imports'] and 'import' not in line:\n-                for missing_import in context['missing_imports']:\n+            if context[\"missing_imports\"] and \"import\" not in line:\n+                for missing_import in context[\"missing_imports\"]:\n                     if missing_import in line:\n                         line = f\"import {missing_import}\\n{line}\"\n-                        corrections.append({\n-                            'type': 'missing_import',\n-                            'line': i + 1,\n-                            'import': missing_import\n-                        })\n+                        corrections.append(\n+                            {\n+                                \"type\": \"missing_import\",\n+                                \"line\": i + 1,\n+                                \"import\": missing_import,\n+                            }\n+                        )\n \n             # Correction des variables non d\u00e9finies (mais pas les mots-cl\u00e9s)\n-            if context['undefined_variables']:\n+            if context[\"undefined_variables\"]:\n                 keywords = {\n-                    'def',\n-                    'class',\n-                    'import',\n-                    'from',\n-                    'if',\n-                    'else',\n-                    'for',\n-                    'while',\n-                    'try',\n-                    'except',\n-                    'finally',\n-                    'with',\n-                    'as',\n-                    'in',\n-                    'is',\n-                    'and',\n-                    'or',\n-                    'not',\n-                    'True',\n-                    'False',\n-                    'None',\n-                    'return',\n-                    'pass',\n-                    'break',\n-                    'continue',\n-                    'raise',\n-                    'yield',\n-                    'del',\n-                    'global',\n-                    'nonlocal',\n-                    'assert',\n-                    'lambda'}\n-\n-                for var in context['undefined_variables']:\n-                    if var in line and '=' not in line and var not in keywords:\n+                    \"def\",\n+                    \"class\",\n+                    \"import\",\n+                    \"from\",\n+                    \"if\",\n+                    \"else\",\n+                    \"for\",\n+                    \"while\",\n+                    \"try\",\n+                    \"except\",\n+                    \"finally\",\n+                    \"with\",\n+                    \"as\",\n+                    \"in\",\n+                    \"is\",\n+                    \"and\",\n+                    \"or\",\n+                    \"not\",\n+                    \"True\",\n+                    \"False\",\n+                    \"None\",\n+                    \"return\",\n+                    \"pass\",\n+                    \"break\",\n+                    \"continue\",\n+                    \"raise\",\n+                    \"yield\",\n+                    \"del\",\n+                    \"global\",\n+                    \"nonlocal\",\n+                    \"assert\",\n+                    \"lambda\",\n+                }\n+\n+                for var in context[\"undefined_variables\"]:\n+                    if var in line and \"=\" not in line and var not in keywords:\n                         # Essayer de deviner le type\n-                        if var.startswith('is_') or var.startswith('has_'):\n+                        if var.startswith(\"is_\") or var.startswith(\"has_\"):\n                             line = line.replace(var, f\"{var} = False\")\n-                        elif var.endswith('_list') or var.endswith('_items'):\n+                        elif var.endswith(\"_list\") or var.endswith(\"_items\"):\n                             line = line.replace(var, f\"{var} = []\")\n                         else:\n                             line = line.replace(var, f\"{var} = None\")\n \n-                        corrections.append({\n-                            'type': 'undefined_variable',\n-                            'line': i + 1,\n-                            'variable': var\n-                        })\n+                        corrections.append(\n+                            {\n+                                \"type\": \"undefined_variable\",\n+                                \"line\": i + 1,\n+                                \"variable\": var,\n+                            }\n+                        )\n \n             if line != original_line:\n                 lines[i] = line\n \n-        return '\\n'.join(lines), corrections\n-\n-    def _analyze_syntax_error(self, error: SyntaxError,\n-                              content: str) -> Dict[str, Any]:\n+        return \"\\n\".join(lines), corrections\n+\n+    def _analyze_syntax_error(self, error: SyntaxError, content: str) -> Dict[str, Any]:\n         \"\"\"Analyse une erreur de syntaxe pour d\u00e9terminer le type de correction\"\"\"\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n         line_num = error.lineno - 1 if error.lineno else 0\n         line_content = lines[line_num] if line_num < len(lines) else \"\"\n \n         error_info = {\n-            'line': line_num + 1,\n-            'content': line_content,\n-            'message': str(error),\n-            'type': 'unknown'\n+            \"line\": line_num + 1,\n+            \"content\": line_content,\n+            \"message\": str(error),\n+            \"type\": \"unknown\",\n         }\n \n         # D\u00e9tection du type d'erreur\n-        if 'indentation' in str(error).lower():\n-            error_info['type'] = 'indentation'\n-        elif 'unexpected EOF' in str(error):\n-            error_info['type'] = 'bracket_balance'\n-        elif 'EOL while scanning string literal' in str(error):\n-            error_info['type'] = 'string_issue'\n-        elif 'invalid syntax' in str(error):\n-            error_info['type'] = 'syntax'\n+        if \"indentation\" in str(error).lower():\n+            error_info[\"type\"] = \"indentation\"\n+        elif \"unexpected EOF\" in str(error):\n+            error_info[\"type\"] = \"bracket_balance\"\n+        elif \"EOL while scanning string literal\" in str(error):\n+            error_info[\"type\"] = \"string_issue\"\n+        elif \"invalid syntax\" in str(error):\n+            error_info[\"type\"] = \"syntax\"\n \n         return error_info\n \n     def _fix_indentation_error(\n-            self, content: str, error_info: Dict[str, Any]) -> Tuple[str, List[Dict[str, Any]]]:\n+        self, content: str, error_info: Dict[str, Any]\n+    ) -> Tuple[str, List[Dict[str, Any]]]:\n         \"\"\"Corrige les erreurs d'indentation\"\"\"\n         corrections = []\n-        lines = content.split('\\n')\n-        line_num = error_info['line'] - 1\n+        lines = content.split(\"\\n\")\n+        line_num = error_info[\"line\"] - 1\n \n         # Analyser l'indentation du contexte\n         if line_num > 0:\n             prev_line = lines[line_num - 1]\n-            if prev_line.strip().endswith(':'):\n+            if prev_line.strip().endswith(\":\"):\n                 # Ligne pr\u00e9c\u00e9dente se termine par ':', donc indentation\n                 # n\u00e9cessaire\n-                current_indent = len(lines[line_num]) - \\\n-                    len(lines[line_num].lstrip())\n+                current_indent = len(lines[line_num]) - len(lines[line_num].lstrip())\n                 expected_indent = len(prev_line) - len(prev_line.lstrip()) + 4\n \n                 if current_indent != expected_indent:\n-                    corrected_line = ' ' * expected_indent + \\\n-                        lines[line_num].lstrip()\n+                    corrected_line = \" \" * expected_indent + lines[line_num].lstrip()\n                     lines[line_num] = corrected_line\n \n-                    corrections.append({\n-                        'type': 'indentation_fix',\n-                        'line': line_num + 1,\n-                        'original_indent': current_indent,\n-                        'corrected_indent': expected_indent\n-                    })\n-\n-        return '\\n'.join(lines), corrections\n+                    corrections.append(\n+                        {\n+                            \"type\": \"indentation_fix\",\n+                            \"line\": line_num + 1,\n+                            \"original_indent\": current_indent,\n+                            \"corrected_indent\": expected_indent,\n+                        }\n+                    )\n+\n+        return \"\\n\".join(lines), corrections\n \n     def _fix_bracket_balance(\n-            self, content: str, error_info: Dict[str, Any]) -> Tuple[str, List[Dict[str, Any]]]:\n+        self, content: str, error_info: Dict[str, Any]\n+    ) -> Tuple[str, List[Dict[str, Any]]]:\n         \"\"\"Corrige les probl\u00e8mes de parenth\u00e8ses/accolades\"\"\"\n         corrections = []\n \n         # Compter les parenth\u00e8ses\n-        open_parens = content.count('(')\n-        close_parens = content.count(')')\n-        open_braces = content.count('{')\n-        close_braces = content.count('}')\n-        open_brackets = content.count('[')\n-        close_brackets = content.count(']')\n+        open_parens = content.count(\"(\")\n+        close_parens = content.count(\")\")\n+        open_braces = content.count(\"{\")\n+        close_braces = content.count(\"}\")\n+        open_brackets = content.count(\"[\")\n+        close_brackets = content.count(\"]\")\n \n         # Ajouter les parenth\u00e8ses manquantes\n         if open_parens > close_parens:\n-            content += ')' * (open_parens - close_parens)\n-            corrections.append({\n-                'type': 'missing_parentheses',\n-                'count': open_parens - close_parens\n-            })\n+            content += \")\" * (open_parens - close_parens)\n+            corrections.append(\n+                {\"type\": \"missing_parentheses\", \"count\": open_parens - close_parens}\n+            )\n \n         if open_braces > close_braces:\n-            content += '}' * (open_braces - close_braces)\n-            corrections.append({\n-                'type': 'missing_braces',\n-                'count': open_braces - close_braces\n-            })\n+            content += \"}\" * (open_braces - close_braces)\n+            corrections.append(\n+                {\"type\": \"missing_braces\", \"count\": open_braces - close_braces}\n+            )\n \n         if open_brackets > close_brackets:\n-            content += ']' * (open_brackets - close_brackets)\n-            corrections.append({\n-                'type': 'missing_brackets',\n-                'count': open_brackets - close_brackets\n-            })\n+            content += \"]\" * (open_brackets - close_brackets)\n+            corrections.append(\n+                {\"type\": \"missing_brackets\", \"count\": open_brackets - close_brackets}\n+            )\n \n         # Correction sp\u00e9cifique pour les parenth\u00e8ses dans les expressions\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n         for i, line in enumerate(lines):\n             original_line = line\n \n             # Chercher les expressions avec parenth\u00e8ses non ferm\u00e9es\n-            if '(' in line and line.count('(') > line.count(')'):\n+            if \"(\" in line and line.count(\"(\") > line.count(\")\"):\n                 # Essayer de fermer les parenth\u00e8ses \u00e0 la fin de la ligne\n-                missing_parens = line.count('(') - line.count(')')\n-                line += ')' * missing_parens\n+                missing_parens = line.count(\"(\") - line.count(\")\")\n+                line += \")\" * missing_parens\n \n                 if line != original_line:\n-                    corrections.append({\n-                        'type': 'line_parentheses_fix',\n-                        'line': i + 1,\n-                        'missing_parens': missing_parens\n-                    })\n+                    corrections.append(\n+                        {\n+                            \"type\": \"line_parentheses_fix\",\n+                            \"line\": i + 1,\n+                            \"missing_parens\": missing_parens,\n+                        }\n+                    )\n                     lines[i] = line\n \n-        return '\\n'.join(lines), corrections\n+        return \"\\n\".join(lines), corrections\n \n     def _fix_string_issues(\n-            self, content: str, error_info: Dict[str, Any]) -> Tuple[str, List[Dict[str, Any]]]:\n+        self, content: str, error_info: Dict[str, Any]\n+    ) -> Tuple[str, List[Dict[str, Any]]]:\n         \"\"\"Corrige les probl\u00e8mes de cha\u00eenes de caract\u00e8res\"\"\"\n         corrections = []\n \n         # Correction des guillemets non ferm\u00e9s\n-        lines = content.split('\\n')\n+        lines = content.split(\"\\n\")\n         for i, line in enumerate(lines):\n             original_line = line\n \n             # Compter les guillemets\n             single_quotes = line.count(\"'\")\n             double_quotes = line.count('\"')\n \n             # Ajouter les guillemets manquants\n             if single_quotes % 2 == 1:\n                 line += \"'\"\n-                corrections.append({\n-                    'type': 'missing_single_quote',\n-                    'line': i + 1\n-                })\n+                corrections.append({\"type\": \"missing_single_quote\", \"line\": i + 1})\n \n             if double_quotes % 2 == 1:\n                 line += '\"'\n-                corrections.append({\n-                    'type': 'missing_double_quote',\n-                    'line': i + 1\n-                })\n+                corrections.append({\"type\": \"missing_double_quote\", \"line\": i + 1})\n \n             if line != original_line:\n                 lines[i] = line\n \n-        return '\\n'.join(lines), corrections\n+        return \"\\n\".join(lines), corrections\n \n     def _analyze_context(self, lines: List[str]) -> Dict[str, Any]:\n         \"\"\"Analyse le contexte du code pour les corrections contextuelles\"\"\"\n         context: Dict[str, Any] = {\n-            'missing_imports': set(),\n-            'undefined_variables': set(),\n-            'defined_variables': set(),\n-            'imports': set()\n+            \"missing_imports\": set(),\n+            \"undefined_variables\": set(),\n+            \"defined_variables\": set(),\n+            \"imports\": set(),\n         }\n \n         for line in lines:\n             # D\u00e9tecter les imports\n-            if line.strip().startswith('import ') or line.strip().startswith('from '):\n-                context['imports'].add(line.strip())\n+            if line.strip().startswith(\"import \") or line.strip().startswith(\"from \"):\n+                context[\"imports\"].add(line.strip())\n \n             # D\u00e9tecter les variables d\u00e9finies\n-            if '=' in line and not line.strip().startswith('#'):\n-                var_name = line.split('=')[0].strip()\n-                if var_name and not var_name.startswith('#'):\n-                    context['defined_variables'].add(var_name)\n+            if \"=\" in line and not line.strip().startswith(\"#\"):\n+                var_name = line.split(\"=\")[0].strip()\n+                if var_name and not var_name.startswith(\"#\"):\n+                    context[\"defined_variables\"].add(var_name)\n \n             # D\u00e9tecter les fonctions d\u00e9finies et leurs param\u00e8tres\n-            if line.strip().startswith('def '):\n+            if line.strip().startswith(\"def \"):\n                 func_def = line.strip()\n-                func_name = func_def.split('def ')[1].split('(')[0].strip()\n-                context['defined_variables'].add(func_name)\n+                func_name = func_def.split(\"def \")[1].split(\"(\")[0].strip()\n+                context[\"defined_variables\"].add(func_name)\n \n                 # Extraire les param\u00e8tres de la fonction\n-                if '(' in func_def and ')' in func_def:\n-                    params_part = func_def[func_def.find(\n-                        '(') + 1:func_def.find(')')]\n+                if \"(\" in func_def and \")\" in func_def:\n+                    params_part = func_def[func_def.find(\"(\") + 1 : func_def.find(\")\")]\n                     params = [\n-                        p.strip().split('=')[0].strip()\n-                        for p in params_part.split(',') if p.strip()\n+                        p.strip().split(\"=\")[0].strip()\n+                        for p in params_part.split(\",\")\n+                        if p.strip()\n                     ]\n                     for param in params:\n-                        context['defined_variables'].add(param)\n+                        context[\"defined_variables\"].add(param)\n \n             # D\u00e9tecter les classes d\u00e9finies\n-            if line.strip().startswith('class '):\n-                class_name = line.strip().split('class ')[1].split(\n-                    '(')[0].split(':')[0].strip()\n-                context['defined_variables'].add(class_name)\n+            if line.strip().startswith(\"class \"):\n+                class_name = (\n+                    line.strip().split(\"class \")[1].split(\"(\")[0].split(\":\")[0].strip()\n+                )\n+                context[\"defined_variables\"].add(class_name)\n \n             # D\u00e9tecter les variables utilis\u00e9es mais non d\u00e9finies\n-            words = re.findall(r'\\b[a-zA-Z_][a-zA-Z0-9_]*\\b', line)\n+            words = re.findall(r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\b\", line)\n             for word in words:\n-                if (\n-                    word not in context['defined_variables'] and word not in [\n-                        'def',\n-                        'class',\n-                        'import',\n-                        'from',\n-                        'if',\n-                        'else',\n-                        'for',\n-                        'while',\n-                        'try',\n-                        'except',\n-                        'finally',\n-                        'with',\n-                        'as',\n-                        'in',\n-                        'is',\n-                        'and',\n-                        'or',\n-                        'not',\n-                        'True',\n-                        'False',\n-                        'None',\n-                        'return',\n-                        'pass',\n-                        'break',\n-                        'continue',\n-                        'raise',\n-                        'yield',\n-                        'del',\n-                        'global',\n-                        'nonlocal',\n-                        'lambda',\n-                        'assert',\n-                        'async',\n-                        'await'\n-                    ]\n-                ):\n-                    context['undefined_variables'].add(word)\n+                if word not in context[\"defined_variables\"] and word not in [\n+                    \"def\",\n+                    \"class\",\n+                    \"import\",\n+                    \"from\",\n+                    \"if\",\n+                    \"else\",\n+                    \"for\",\n+                    \"while\",\n+                    \"try\",\n+                    \"except\",\n+                    \"finally\",\n+                    \"with\",\n+                    \"as\",\n+                    \"in\",\n+                    \"is\",\n+                    \"and\",\n+                    \"or\",\n+                    \"not\",\n+                    \"True\",\n+                    \"False\",\n+                    \"None\",\n+                    \"return\",\n+                    \"pass\",\n+                    \"break\",\n+                    \"continue\",\n+                    \"raise\",\n+                    \"yield\",\n+                    \"del\",\n+                    \"global\",\n+                    \"nonlocal\",\n+                    \"lambda\",\n+                    \"assert\",\n+                    \"async\",\n+                    \"await\",\n+                ]:\n+                    context[\"undefined_variables\"].add(word)\n \n         return context\n \n     def _validate_correction(self, content: str) -> bool:\n         \"\"\"Valide que la correction a fonctionn\u00e9\"\"\"\n@@ -583,82 +594,84 @@\n             return True\n         except SyntaxError:\n             return False\n \n     def _learn_from_correction(\n-            self,\n-            file_path: str,\n-            original: str,\n-            corrected: str,\n-            success: bool):\n+        self, file_path: str, original: str, corrected: str, success: bool\n+    ):\n         \"\"\"Apprend des corrections pour am\u00e9liorer les futures corrections\"\"\"\n         # Extraire des patterns de la correction\n         if success:\n             # Analyser les patterns qui ont fonctionn\u00e9\n             patterns = self._extract_patterns(original, corrected)\n             for pattern in patterns:\n                 self.success_patterns[pattern] += 1\n         else:\n             # Analyser les patterns qui ont \u00e9chou\u00e9\n-            patterns = self._extract_patterns(\n-                original, original)  # Pas de changement\n+            patterns = self._extract_patterns(original, original)  # Pas de changement\n             for pattern in patterns:\n                 self.failure_patterns[pattern] += 1\n \n         # Sauvegarder l'historique\n-        self.correction_history[file_path].append({\n-            'timestamp': time.time(),\n-            'success': success,\n-            'original_length': len(original),\n-            'corrected_length': len(corrected),\n-            'changes_count': len(corrected) - len(original)\n-        })\n+        self.correction_history[file_path].append(\n+            {\n+                \"timestamp\": time.time(),\n+                \"success\": success,\n+                \"original_length\": len(original),\n+                \"corrected_length\": len(corrected),\n+                \"changes_count\": len(corrected) - len(original),\n+            }\n+        )\n \n         # Garder seulement les 100 derniers historiques par fichier\n         if len(self.correction_history[file_path]) > 100:\n-            self.correction_history[file_path] = self.correction_history[file_path][-100:]\n+            self.correction_history[file_path] = self.correction_history[file_path][\n+                -100:\n+            ]\n \n     def _extract_patterns(self, original: str, corrected: str) -> List[str]:\n         \"\"\"Extrait des patterns de correction\"\"\"\n         patterns = []\n \n         # Patterns basiques\n-        if 'import' in corrected and 'import' not in original:\n-            patterns.append('add_import')\n-\n-        if 'def ' in corrected and 'def ' not in original:\n-            patterns.append('add_function')\n-\n-        if 'class ' in corrected and 'class ' not in original:\n-            patterns.append('add_class')\n-\n-        if '=' in corrected and '=' not in original:\n-            patterns.append('add_assignment')\n+        if \"import\" in corrected and \"import\" not in original:\n+            patterns.append(\"add_import\")\n+\n+        if \"def \" in corrected and \"def \" not in original:\n+            patterns.append(\"add_function\")\n+\n+        if \"class \" in corrected and \"class \" not in original:\n+            patterns.append(\"add_class\")\n+\n+        if \"=\" in corrected and \"=\" not in original:\n+            patterns.append(\"add_assignment\")\n \n         return patterns\n \n     def get_correction_stats(self) -> Dict[str, Any]:\n         \"\"\"R\u00e9cup\u00e8re les statistiques de correction\"\"\"\n-        total_corrections = sum(len(history)\n-                                for history in self.correction_history.values())\n+        total_corrections = sum(\n+            len(history) for history in self.correction_history.values()\n+        )\n         successful_corrections = sum(\n-            sum(1 for correction in history if correction['success'])\n+            sum(1 for correction in history if correction[\"success\"])\n             for history in self.correction_history.values()\n         )\n \n         success_rate = (\n-            successful_corrections\n-            / total_corrections\n-            * 100) if total_corrections > 0 else 0\n+            (successful_corrections / total_corrections * 100)\n+            if total_corrections > 0\n+            else 0\n+        )\n \n         return {\n-            'total_corrections': total_corrections,\n-            'successful_corrections': successful_corrections,\n-            'success_rate': success_rate,\n-            'success_patterns': dict(self.success_patterns),\n-            'failure_patterns': dict(self.failure_patterns),\n-            'files_corrected': len(self.correction_history)\n+            \"total_corrections\": total_corrections,\n+            \"successful_corrections\": successful_corrections,\n+            \"success_rate\": success_rate,\n+            \"success_patterns\": dict(self.success_patterns),\n+            \"failure_patterns\": dict(self.failure_patterns),\n+            \"files_corrected\": len(self.correction_history),\n         }\n \n \n # Instance globale\n correction_optimizer = CorrectionOptimizer()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_memory.py\t2025-07-29 17:56:27.830000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_memory.py\t2025-07-29 18:12:21.543759+00:00\n@@ -25,10 +25,11 @@\n \n \n @dataclass\n class LearningEvent:\n     \"\"\"\u00c9v\u00e9nement d'apprentissage\"\"\"\n+\n     event_type: str  # 'error', 'correction', 'pattern', 'duplicate'\n     description: str\n     code_snippet: str\n     location: str\n     timestamp: datetime\n@@ -39,10 +40,11 @@\n \n \n @dataclass\n class Prediction:\n     \"\"\"Pr\u00e9diction bas\u00e9e sur l'apprentissage\"\"\"\n+\n     prediction_type: str  # 'error', 'duplicate', 'optimization'\n     confidence: float  # 0.0 - 1.0\n     description: str\n     suggested_action: str\n     estimated_impact: str\n@@ -50,10 +52,11 @@\n \n \n @dataclass\n class CorrectionSuggestion:\n     \"\"\"Suggestion de correction\"\"\"\n+\n     original_code: str\n     suggested_code: str\n     reason: str\n     confidence: float\n     based_on_previous_corrections: List[str]\n@@ -84,11 +87,12 @@\n         \"\"\"Initialiser la base de donn\u00e9es de m\u00e9moire\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Table des \u00e9v\u00e9nements d'apprentissage\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS learning_events (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     event_type TEXT NOT NULL,\n                     description TEXT NOT NULL,\n                     code_snippet TEXT NOT NULL,\n@@ -98,28 +102,32 @@\n                     resolution TEXT,\n                     success BOOLEAN DEFAULT 1,\n                     context TEXT,\n                     pattern_hash TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des patterns appris\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS learned_patterns (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     pattern_hash TEXT UNIQUE NOT NULL,\n                     pattern_type TEXT NOT NULL,\n                     occurrences INTEGER DEFAULT 1,\n                     first_seen TEXT NOT NULL,\n                     last_seen TEXT NOT NULL,\n                     success_rate REAL DEFAULT 1.0,\n                     correction_history TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des pr\u00e9dictions\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS predictions (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     prediction_type TEXT NOT NULL,\n                     confidence REAL NOT NULL,\n                     description TEXT NOT NULL,\n@@ -128,14 +136,16 @@\n                     code_pattern TEXT NOT NULL,\n                     created_at TEXT NOT NULL,\n                     validated BOOLEAN DEFAULT 0,\n                     validation_result TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des corrections sugg\u00e9r\u00e9es\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS correction_suggestions (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     original_code TEXT NOT NULL,\n                     suggested_code TEXT NOT NULL,\n                     reason TEXT NOT NULL,\n@@ -144,133 +154,145 @@\n                     created_at TEXT NOT NULL,\n                     applied BOOLEAN DEFAULT 0,\n                     applied_at TEXT,\n                     success BOOLEAN\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des m\u00e9triques d'apprentissage\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS learning_metrics (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     metric_name TEXT NOT NULL,\n                     metric_value REAL NOT NULL,\n                     timestamp TEXT NOT NULL,\n                     context TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             conn.commit()\n \n     def learn_from_error(\n-        self, error_description: str, code_snippet: str,\n-        location: str, severity: str = \"medium\",\n-        context: Dict[str, Any] = None\n+        self,\n+        error_description: str,\n+        code_snippet: str,\n+        location: str,\n+        severity: str = \"medium\",\n+        context: Dict[str, Any] = None,\n     ) -> str:\n         \"\"\"Apprendre d'une erreur\"\"\"\n         event_id = self._record_learning_event(\n             event_type=\"error\",\n             description=error_description,\n             code_snippet=code_snippet,\n             location=location,\n             severity=severity,\n             success=False,\n-            context=context\n+            context=context,\n         )\n \n         # Analyser le pattern de l'erreur\n         pattern_hash = self._analyze_code_pattern(code_snippet)\n         self._update_pattern_learning(pattern_hash, \"error\", success=False)\n \n         # G\u00e9n\u00e9rer des pr\u00e9dictions bas\u00e9es sur cette erreur\n         self._generate_predictions_from_error(\n-            error_description, code_snippet, pattern_hash)\n+            error_description, code_snippet, pattern_hash\n+        )\n \n         logger.info(f\"\ud83d\udcda Apprentissage d'une erreur: {error_description}\")\n         return event_id\n \n     def learn_from_correction(\n-        self, original_code: str, corrected_code: str,\n-        reason: str, location: str, success: bool = True,\n-        context: Dict[str, Any] = None\n+        self,\n+        original_code: str,\n+        corrected_code: str,\n+        reason: str,\n+        location: str,\n+        success: bool = True,\n+        context: Dict[str, Any] = None,\n     ) -> str:\n         \"\"\"Apprendre d'une correction\"\"\"\n         event_id = self._record_learning_event(\n             event_type=\"correction\",\n             description=f\"Correction: {reason}\",\n             code_snippet=f\"Original: {original_code}\\nCorrected: {corrected_code}\",\n             location=location,\n             severity=\"medium\",\n             resolution=corrected_code,\n             success=success,\n-            context=context)\n+            context=context,\n+        )\n \n         # Analyser les patterns\n         original_pattern = self._analyze_code_pattern(original_code)\n         corrected_pattern = self._analyze_code_pattern(corrected_code)\n \n         # Mettre \u00e0 jour l'apprentissage des patterns\n-        self._update_pattern_learning(\n-            original_pattern, \"correction\", success=False)\n-        self._update_pattern_learning(\n-            corrected_pattern, \"correction\", success=success)\n+        self._update_pattern_learning(original_pattern, \"correction\", success=False)\n+        self._update_pattern_learning(corrected_pattern, \"correction\", success=success)\n \n         # Sauvegarder la suggestion de correction\n-        self._save_correction_suggestion(\n-            original_code, corrected_code, reason, success)\n+        self._save_correction_suggestion(original_code, corrected_code, reason, success)\n \n         logger.info(f\"\ud83d\udcda Apprentissage d'une correction: {reason}\")\n         return event_id\n \n     def learn_from_duplicate(\n-        self, duplicate_items: List[str], locations: List[str],\n-        similarity_score: float, context: Dict[str, Any] = None\n+        self,\n+        duplicate_items: List[str],\n+        locations: List[str],\n+        similarity_score: float,\n+        context: Dict[str, Any] = None,\n     ) -> str:\n         \"\"\"Apprendre d'un doublon d\u00e9tect\u00e9\"\"\"\n         event_id = self._record_learning_event(\n             event_type=\"duplicate\",\n             description=f\"Doublon d\u00e9tect\u00e9 (similarit\u00e9: {similarity_score:.2f})\",\n             code_snippet=f\"Items: {', '.join(duplicate_items)}\",\n             location=f\"Locations: {', '.join(locations)}\",\n             severity=\"high\" if similarity_score > 0.9 else \"medium\",\n             success=False,\n-            context=context)\n+            context=context,\n+        )\n \n         # Analyser le pattern du doublon\n         for item in duplicate_items:\n             pattern_hash = self._analyze_code_pattern(item)\n-            self._update_pattern_learning(\n-                pattern_hash, \"duplicate\", success=False)\n-\n-        logger.info(\n-            f\"\ud83d\udcda Apprentissage d'un doublon: {len(duplicate_items)} items\")\n+            self._update_pattern_learning(pattern_hash, \"duplicate\", success=False)\n+\n+        logger.info(f\"\ud83d\udcda Apprentissage d'un doublon: {len(duplicate_items)} items\")\n         return event_id\n \n-    def predict_issues(self,\n-                       code_snippet: str,\n-                       context: Dict[str,\n-                                     Any] = None) -> List[Prediction]:\n+    def predict_issues(\n+        self, code_snippet: str, context: Dict[str, Any] = None\n+    ) -> List[Prediction]:\n         \"\"\"Pr\u00e9dire les probl\u00e8mes potentiels\"\"\"\n         predictions = []\n \n         # Analyser le pattern du code\n         pattern_hash = self._analyze_code_pattern(code_snippet)\n \n         # Chercher des patterns similaires dans l'historique\n         similar_patterns = self._find_similar_patterns(pattern_hash)\n \n         for pattern in similar_patterns:\n-            if pattern['success_rate'] < 0.7:  # Pattern probl\u00e9matique\n+            if pattern[\"success_rate\"] < 0.7:  # Pattern probl\u00e9matique\n                 prediction = Prediction(\n                     prediction_type=\"error\",\n-                    confidence=1.0 - pattern['success_rate'],\n+                    confidence=1.0 - pattern[\"success_rate\"],\n                     description=(\n                         f\"Pattern similaire \u00e0 un probl\u00e8me pr\u00e9c\u00e9dent \"\n-                        f\"(taux de succ\u00e8s: {pattern['success_rate']:.2f})\"),\n+                        f\"(taux de succ\u00e8s: {pattern['success_rate']:.2f})\"\n+                    ),\n                     suggested_action=\"V\u00e9rifier la logique et consid\u00e9rer une refactorisation\",\n                     estimated_impact=\"Moyen\",\n-                    code_pattern=pattern_hash)\n+                    code_pattern=pattern_hash,\n+                )\n                 predictions.append(prediction)\n \n         # V\u00e9rifier les anti-patterns connus\n         antipattern_predictions = self._check_antipatterns(code_snippet)\n         predictions.extend(antipattern_predictions)\n@@ -280,43 +302,46 @@\n         predictions.extend(duplicate_predictions)\n \n         return predictions\n \n     def suggest_corrections(\n-        self, problematic_code: str,\n-        issue_description: str\n+        self, problematic_code: str, issue_description: str\n     ) -> List[CorrectionSuggestion]:\n         \"\"\"Sugg\u00e9rer des corrections bas\u00e9es sur l'apprentissage\"\"\"\n         suggestions = []\n \n         # Chercher des corrections similaires dans l'historique\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT original_code, suggested_code, reason, confidence, based_on_corrections\n                 FROM correction_suggestions\n                 WHERE applied = 1 AND success = 1\n                 ORDER BY confidence DESC\n                 LIMIT 5\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             rows = cursor.fetchall()\n \n             for row in rows:\n                 original, suggested, reason, confidence, based_on = row\n \n                 # Calculer la similarit\u00e9 avec le code probl\u00e9matique\n-                similarity = self._calculate_code_similarity(\n-                    problematic_code, original)\n+                similarity = self._calculate_code_similarity(problematic_code, original)\n \n                 if similarity > 0.7:  # Seuil de similarit\u00e9\n                     suggestion = CorrectionSuggestion(\n                         original_code=problematic_code,\n                         suggested_code=suggested,\n                         reason=f\"Bas\u00e9 sur une correction similaire: {reason}\",\n                         confidence=confidence * similarity,\n-                        based_on_previous_corrections=json.loads(based_on) if based_on else [])\n+                        based_on_previous_corrections=(\n+                            json.loads(based_on) if based_on else []\n+                        ),\n+                    )\n                     suggestions.append(suggestion)\n \n         return suggestions\n \n     def get_learning_insights(self) -> Dict[str, Any]:\n@@ -326,16 +351,14 @@\n \n             # Statistiques g\u00e9n\u00e9rales\n             cursor.execute(\"SELECT COUNT(*) FROM learning_events\")\n             total_events = cursor.fetchone()[0]\n \n-            cursor.execute(\n-                \"SELECT COUNT(*) FROM learning_events WHERE success = 0\")\n+            cursor.execute(\"SELECT COUNT(*) FROM learning_events WHERE success = 0\")\n             total_errors = cursor.fetchone()[0]\n \n-            cursor.execute(\n-                \"SELECT COUNT(*) FROM learning_events WHERE success = 1\")\n+            cursor.execute(\"SELECT COUNT(*) FROM learning_events WHERE success = 1\")\n             total_successes = cursor.fetchone()[0]\n \n             # Patterns appris\n             cursor.execute(\"SELECT COUNT(*) FROM learned_patterns\")\n             total_patterns = cursor.fetchone()[0]\n@@ -357,41 +380,49 @@\n                 \"total_successes\": total_successes,\n                 \"error_rate\": error_rate,\n                 \"total_patterns\": total_patterns,\n                 \"total_predictions\": total_predictions,\n                 \"total_corrections\": total_corrections,\n-                \"learning_progress\": \"Syst\u00e8me d'apprentissage actif\"\n+                \"learning_progress\": \"Syst\u00e8me d'apprentissage actif\",\n             }\n \n     def _record_learning_event(\n-        self, event_type: str, description: str,\n-        code_snippet: str, location: str, severity: str,\n-        success: bool = True, resolution: str = None,\n-        context: Dict[str, Any] = None\n+        self,\n+        event_type: str,\n+        description: str,\n+        code_snippet: str,\n+        location: str,\n+        severity: str,\n+        success: bool = True,\n+        resolution: str = None,\n+        context: Dict[str, Any] = None,\n     ) -> str:\n         \"\"\"Enregistrer un \u00e9v\u00e9nement d'apprentissage\"\"\"\n         pattern_hash = self._analyze_code_pattern(code_snippet)\n \n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 INSERT INTO learning_events\n                 (event_type, description, code_snippet, location, timestamp,\n                  severity, resolution, success, context, pattern_hash)\n                 VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n-            \"\"\", (\n-                event_type,\n-                description,\n-                code_snippet,\n-                location,\n-                datetime.now().isoformat(),\n-                severity,\n-                resolution,\n-                success,\n-                json.dumps(context) if context else None,\n-                pattern_hash\n-            ))\n+            \"\"\",\n+                (\n+                    event_type,\n+                    description,\n+                    code_snippet,\n+                    location,\n+                    datetime.now().isoformat(),\n+                    severity,\n+                    resolution,\n+                    success,\n+                    json.dumps(context) if context else None,\n+                    pattern_hash,\n+                ),\n+            )\n \n             event_id = cursor.lastrowid\n             conn.commit()\n \n             return str(event_id)\n@@ -405,202 +436,230 @@\n         return hashlib.md5(normalized.encode()).hexdigest()\n \n     def _normalize_code(self, code: str) -> str:\n         \"\"\"Normaliser le code pour la comparaison\"\"\"\n         # Supprimer les commentaires\n-        code = re.sub(r'#.*$', '', code, flags=re.MULTILINE)\n+        code = re.sub(r\"#.*$\", \"\", code, flags=re.MULTILINE)\n \n         # Supprimer les docstrings\n-        code = re.sub(r'\"\"\".*?\"\"\"', '', code, flags=re.DOTALL)\n-        code = re.sub(r\"'''.*?'''\", '', code, flags=re.DOTALL)\n+        code = re.sub(r'\"\"\".*?\"\"\"', \"\", code, flags=re.DOTALL)\n+        code = re.sub(r\"'''.*?'''\", \"\", code, flags=re.DOTALL)\n \n         # Supprimer les espaces en d\u00e9but de ligne\n-        lines = [line.strip() for line in code.split('\\n')]\n+        lines = [line.strip() for line in code.split(\"\\n\")]\n \n         # Supprimer les lignes vides\n         lines = [line for line in lines if line]\n \n-        return '\\n'.join(lines)\n+        return \"\\n\".join(lines)\n \n     def _update_pattern_learning(\n-            self,\n-            pattern_hash: str,\n-            pattern_type: str,\n-            success: bool):\n+        self, pattern_hash: str, pattern_type: str, success: bool\n+    ):\n         \"\"\"Mettre \u00e0 jour l'apprentissage d'un pattern\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # V\u00e9rifier si le pattern existe d\u00e9j\u00e0\n             cursor.execute(\n-                \"SELECT * FROM learned_patterns WHERE pattern_hash = ?\", (pattern_hash,))\n+                \"SELECT * FROM learned_patterns WHERE pattern_hash = ?\", (pattern_hash,)\n+            )\n             existing = cursor.fetchone()\n \n             if existing:\n                 # Mettre \u00e0 jour le pattern existant\n                 occurrences = existing[3] + 1\n-                success_count = int(\n-                    float(existing[6]) * existing[3]) + (1 if success else 0)\n+                success_count = int(float(existing[6]) * existing[3]) + (\n+                    1 if success else 0\n+                )\n                 success_rate = success_count / occurrences\n \n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     UPDATE learned_patterns\n                     SET occurrences = ?, last_seen = ?, success_rate = ?\n                     WHERE pattern_hash = ?\n-                \"\"\", (\n-                    occurrences,\n-                    datetime.now().isoformat(),\n-                    success_rate,\n-                    pattern_hash\n-                ))\n+                \"\"\",\n+                    (\n+                        occurrences,\n+                        datetime.now().isoformat(),\n+                        success_rate,\n+                        pattern_hash,\n+                    ),\n+                )\n             else:\n                 # Cr\u00e9er un nouveau pattern\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO learned_patterns\n                     (pattern_hash, pattern_type, occurrences, first_seen, last_seen, success_rate)\n                     VALUES (?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    pattern_hash,\n-                    pattern_type,\n-                    1,\n-                    datetime.now().isoformat(),\n-                    datetime.now().isoformat(),\n-                    1.0 if success else 0.0\n-                ))\n+                \"\"\",\n+                    (\n+                        pattern_hash,\n+                        pattern_type,\n+                        1,\n+                        datetime.now().isoformat(),\n+                        datetime.now().isoformat(),\n+                        1.0 if success else 0.0,\n+                    ),\n+                )\n \n             conn.commit()\n \n     def _generate_predictions_from_error(\n-        self, error_description: str,\n-        code_snippet: str, pattern_hash: str\n+        self, error_description: str, code_snippet: str, pattern_hash: str\n     ):\n         \"\"\"G\u00e9n\u00e9rer des pr\u00e9dictions bas\u00e9es sur une erreur\"\"\"\n         # Analyser le type d'erreur et g\u00e9n\u00e9rer des pr\u00e9dictions\n         if \"duplicate\" in error_description.lower():\n             prediction = {\n                 \"prediction_type\": \"duplicate\",\n                 \"confidence\": 0.8,\n                 \"description\": \"Risque de duplication de code\",\n-                \"suggested_action\": \"V\u00e9rifier s'il existe d\u00e9j\u00e0 une fonction/module similaire\",\n+                \"suggested_action\": (\n+                    \"V\u00e9rifier s'il existe d\u00e9j\u00e0 une fonction/module similaire\"\n+                ),\n                 \"estimated_impact\": \"Moyen\",\n-                \"code_pattern\": pattern_hash}\n+                \"code_pattern\": pattern_hash,\n+            }\n         elif \"complexity\" in error_description.lower():\n             prediction = {\n                 \"prediction_type\": \"complexity\",\n                 \"confidence\": 0.7,\n                 \"description\": \"Code potentiellement trop complexe\",\n-                \"suggested_action\": \"Consid\u00e9rer la refactorisation en fonctions plus petites\",\n+                \"suggested_action\": (\n+                    \"Consid\u00e9rer la refactorisation en fonctions plus petites\"\n+                ),\n                 \"estimated_impact\": \"\u00c9lev\u00e9\",\n-                \"code_pattern\": pattern_hash}\n+                \"code_pattern\": pattern_hash,\n+            }\n         else:\n             prediction = {\n                 \"prediction_type\": \"error\",\n                 \"confidence\": 0.6,\n                 \"description\": \"Pattern similaire \u00e0 une erreur pr\u00e9c\u00e9dente\",\n                 \"suggested_action\": \"V\u00e9rifier la logique et les bonnes pratiques\",\n                 \"estimated_impact\": \"Moyen\",\n-                \"code_pattern\": pattern_hash}\n+                \"code_pattern\": pattern_hash,\n+            }\n \n         # Sauvegarder la pr\u00e9diction\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 INSERT INTO predictions\n                 (prediction_type, confidence, description, suggested_action,\n                  estimated_impact, code_pattern, created_at)\n                 VALUES (?, ?, ?, ?, ?, ?, ?)\n-            \"\"\", (\n-                prediction[\"prediction_type\"],\n-                prediction[\"confidence\"],\n-                prediction[\"description\"],\n-                prediction[\"suggested_action\"],\n-                prediction[\"estimated_impact\"],\n-                prediction[\"code_pattern\"],\n-                datetime.now().isoformat()\n-            ))\n+            \"\"\",\n+                (\n+                    prediction[\"prediction_type\"],\n+                    prediction[\"confidence\"],\n+                    prediction[\"description\"],\n+                    prediction[\"suggested_action\"],\n+                    prediction[\"estimated_impact\"],\n+                    prediction[\"code_pattern\"],\n+                    datetime.now().isoformat(),\n+                ),\n+            )\n             conn.commit()\n \n-    def _find_similar_patterns(\n-            self, pattern_hash: str) -> List[Dict[str, Any]]:\n+    def _find_similar_patterns(self, pattern_hash: str) -> List[Dict[str, Any]]:\n         \"\"\"Trouver des patterns similaires\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 SELECT pattern_hash, pattern_type, occurrences, success_rate, last_seen\n                 FROM learned_patterns\n                 WHERE pattern_hash != ?\n                 ORDER BY success_rate ASC\n                 LIMIT 10\n-            \"\"\", (pattern_hash,))\n+            \"\"\",\n+                (pattern_hash,),\n+            )\n \n             rows = cursor.fetchall()\n             return [\n                 {\n                     \"pattern_hash\": row[0],\n                     \"pattern_type\": row[1],\n                     \"occurrences\": row[2],\n                     \"success_rate\": row[3],\n-                    \"last_seen\": row[4]\n+                    \"last_seen\": row[4],\n                 }\n                 for row in rows\n             ]\n \n     def _check_antipatterns(self, code_snippet: str) -> List[Prediction]:\n         \"\"\"V\u00e9rifier les anti-patterns connus\"\"\"\n         predictions = []\n \n         # Anti-patterns \u00e0 v\u00e9rifier\n         antipatterns = [\n-            (r'eval\\s*\\(',\n-             \"Utilisation de eval() - Risque de s\u00e9curit\u00e9\",\n-             \"Remplacer par une alternative s\u00e9curis\u00e9e\"),\n-            (r'exec\\s*\\(',\n-             \"Utilisation de exec() - Risque de s\u00e9curit\u00e9\",\n-             \"Remplacer par une alternative s\u00e9curis\u00e9e\"),\n-            (r'for\\s+\\w+\\s+in\\s+\\w+:\\s*\\n\\s*for\\s+\\w+\\s+in\\s+\\w+:',\n-             \"Boucles imbriqu\u00e9es - Complexit\u00e9 \u00e9lev\u00e9e\",\n-             \"Consid\u00e9rer l'utilisation de list comprehensions ou itertools\"),\n-            (r'if\\s+\\w+:\\s*\\n\\s*if\\s+\\w+:\\s*\\n\\s*if\\s+\\w+:',\n-             \"Conditions imbriqu\u00e9es - Complexit\u00e9 \u00e9lev\u00e9e\",\n-             \"Refactoriser en utilisant des early returns ou des guard clauses\"),\n-            (r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n-             \"Mot de passe en dur\",\n-             \"Utiliser des variables d'environnement ou un gestionnaire de secrets\")]\n+            (\n+                r\"eval\\s*\\(\",\n+                \"Utilisation de eval() - Risque de s\u00e9curit\u00e9\",\n+                \"Remplacer par une alternative s\u00e9curis\u00e9e\",\n+            ),\n+            (\n+                r\"exec\\s*\\(\",\n+                \"Utilisation de exec() - Risque de s\u00e9curit\u00e9\",\n+                \"Remplacer par une alternative s\u00e9curis\u00e9e\",\n+            ),\n+            (\n+                r\"for\\s+\\w+\\s+in\\s+\\w+:\\s*\\n\\s*for\\s+\\w+\\s+in\\s+\\w+:\",\n+                \"Boucles imbriqu\u00e9es - Complexit\u00e9 \u00e9lev\u00e9e\",\n+                \"Consid\u00e9rer l'utilisation de list comprehensions ou itertools\",\n+            ),\n+            (\n+                r\"if\\s+\\w+:\\s*\\n\\s*if\\s+\\w+:\\s*\\n\\s*if\\s+\\w+:\",\n+                \"Conditions imbriqu\u00e9es - Complexit\u00e9 \u00e9lev\u00e9e\",\n+                \"Refactoriser en utilisant des early returns ou des guard clauses\",\n+            ),\n+            (\n+                r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']',\n+                \"Mot de passe en dur\",\n+                \"Utiliser des variables d'environnement ou un gestionnaire de secrets\",\n+            ),\n+        ]\n \n         for pattern, description, suggestion in antipatterns:\n             if re.search(pattern, code_snippet):\n                 prediction = Prediction(\n                     prediction_type=\"antipattern\",\n                     confidence=0.9,\n                     description=description,\n                     suggested_action=suggestion,\n                     estimated_impact=\"\u00c9lev\u00e9\",\n-                    code_pattern=self._analyze_code_pattern(code_snippet)\n+                    code_pattern=self._analyze_code_pattern(code_snippet),\n                 )\n                 predictions.append(prediction)\n \n         return predictions\n \n-    def _check_potential_duplicates(\n-            self, code_snippet: str) -> List[Prediction]:\n+    def _check_potential_duplicates(self, code_snippet: str) -> List[Prediction]:\n         \"\"\"V\u00e9rifier les doublons potentiels\"\"\"\n         predictions = []\n \n         # Chercher des patterns similaires dans l'historique\n         pattern_hash = self._analyze_code_pattern(code_snippet)\n         similar_patterns = self._find_similar_patterns(pattern_hash)\n \n         for pattern in similar_patterns:\n-            if pattern['occurrences'] > 2:  # Pattern r\u00e9p\u00e9t\u00e9\n+            if pattern[\"occurrences\"] > 2:  # Pattern r\u00e9p\u00e9t\u00e9\n                 prediction = Prediction(\n                     prediction_type=\"duplicate\",\n                     confidence=0.7,\n                     description=f\"Pattern similaire d\u00e9tect\u00e9 {pattern['occurrences']} fois\",\n-                    suggested_action=(\"Consid\u00e9rer l'extraction en fonction/\"\n-                                      \"module commun\"),\n+                    suggested_action=(\n+                        \"Consid\u00e9rer l'extraction en fonction/\" \"module commun\"\n+                    ),\n                     estimated_impact=\"Moyen\",\n-                    code_pattern=pattern_hash\n+                    code_pattern=pattern_hash,\n                 )\n                 predictions.append(prediction)\n \n         return predictions\n \n@@ -613,33 +672,33 @@\n         # Utiliser difflib pour calculer la similarit\u00e9\n         similarity = difflib.SequenceMatcher(None, norm1, norm2).ratio()\n         return similarity\n \n     def _save_correction_suggestion(\n-            self,\n-            original_code: str,\n-            corrected_code: str,\n-            reason: str,\n-            success: bool):\n+        self, original_code: str, corrected_code: str, reason: str, success: bool\n+    ):\n         \"\"\"Sauvegarder une suggestion de correction\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 INSERT INTO correction_suggestions\n                 (original_code, suggested_code, reason, confidence,\n                  based_on_corrections, created_at, applied, success)\n                 VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n-            \"\"\", (\n-                original_code,\n-                corrected_code,\n-                reason,\n-                0.8 if success else 0.3,\n-                json.dumps([]),  # Pour l'instant, pas de corrections bas\u00e9es\n-                datetime.now().isoformat(),\n-                True,\n-                success\n-            ))\n+            \"\"\",\n+                (\n+                    original_code,\n+                    corrected_code,\n+                    reason,\n+                    0.8 if success else 0.3,\n+                    json.dumps([]),  # Pour l'instant, pas de corrections bas\u00e9es\n+                    datetime.now().isoformat(),\n+                    True,\n+                    success,\n+                ),\n+            )\n             conn.commit()\n \n \n def main():\n     \"\"\"Test du syst\u00e8me de m\u00e9moire intelligente\"\"\"\n@@ -648,21 +707,24 @@\n     # Simuler l'apprentissage d'une erreur\n     error_id = memory.learn_from_error(\n         error_description=\"Fonction trop longue d\u00e9tect\u00e9e\",\n         code_snippet=\"def very_long_function():\\n    # 100 lignes de code...\",\n         location=\"test_file.py:10\",\n-        severity=\"medium\"\n+        severity=\"medium\",\n     )\n \n     # Simuler l'apprentissage d'une correction\n     correction_id = memory.learn_from_correction(\n         original_code=\"def very_long_function():\\n    # 100 lignes...\",\n-        corrected_code=(\"def short_function1():\\n    # 30 lignes...\\n\\n\"\n-                        \"def short_function2():\\n    # 30 lignes...\"),\n+        corrected_code=(\n+            \"def short_function1():\\n    # 30 lignes...\\n\\n\"\n+            \"def short_function2():\\n    # 30 lignes...\"\n+        ),\n         reason=\"Division en fonctions plus petites\",\n         location=\"test_file.py:10\",\n-        success=True)\n+        success=True,\n+    )\n \n     # Tester les pr\u00e9dictions\n     predictions = memory.predict_issues(\n         \"def another_long_function():\\n    # 80 lignes de code...\"\n     )\n@@ -679,11 +741,10 @@\n     insights = memory.get_learning_insights()\n     print(\"\\n\ud83d\udcca Insights d'apprentissage:\")\n     print(f\"  \u2022 \u00c9v\u00e9nements totaux: {insights['total_events']}\")\n     print(f\"  \u2022 Taux d'erreur: {insights['error_rate']:.2f}\")\n     print(f\"  \u2022 Patterns appris: {insights['total_patterns']}\")\n-    print(\n-        f\"  \u2022 Pr\u00e9cision des pr\u00e9dictions: {insights['prediction_accuracy']:.2f}\")\n+    print(f\"  \u2022 Pr\u00e9cision des pr\u00e9dictions: {insights['prediction_accuracy']:.2f}\")\n \n \n if __name__ == \"__main__\":\n     main()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/performance_analyzer.py\t2025-07-29 17:56:29.960000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/performance_analyzer.py\t2025-07-29 18:12:21.549452+00:00\n@@ -23,10 +23,11 @@\n \n \n @dataclass\n class PerformanceMetric:\n     \"\"\"M\u00e9trique de performance\"\"\"\n+\n     metric_type: str\n     value: float\n     unit: str\n     location: str\n     threshold: float\n@@ -34,10 +35,11 @@\n \n \n @dataclass\n class PerformanceIssue:\n     \"\"\"Probl\u00e8me de performance d\u00e9tect\u00e9\"\"\"\n+\n     issue_type: str\n     location: str\n     description: str\n     impact: str  # 'low', 'medium', 'high', 'critical'\n     suggestion: str\n@@ -45,10 +47,11 @@\n \n \n @dataclass\n class PerformanceReport:\n     \"\"\"Rapport de performance complet\"\"\"\n+\n     overall_score: float\n     metrics: List[PerformanceMetric]\n     issues: List[PerformanceIssue]\n     recommendations: List[str]\n     optimization_opportunities: List[str]\n@@ -75,36 +78,39 @@\n             \"complexity\": 10,\n             \"function_size\": 50,\n             \"class_size\": 200,\n             \"imports\": 30,\n             \"nested_depth\": 5,\n-            \"loop_complexity\": 3\n+            \"loop_complexity\": 3,\n         }\n \n         logger.info(f\"\u26a1 Performance Analyzer initialis\u00e9 dans {self.root_path}\")\n \n     def _init_database(self):\n         \"\"\"Initialiser la base de donn\u00e9es de performance\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Table des m\u00e9triques de performance\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS performance_metrics (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     metric_type TEXT NOT NULL,\n                     value REAL NOT NULL,\n                     unit TEXT NOT NULL,\n                     location TEXT NOT NULL,\n                     threshold REAL NOT NULL,\n                     status TEXT NOT NULL,\n                     measured_at TEXT NOT NULL\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des probl\u00e8mes de performance\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS performance_issues (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     issue_type TEXT NOT NULL,\n                     location TEXT NOT NULL,\n                     description TEXT NOT NULL,\n@@ -112,37 +118,41 @@\n                     suggestion TEXT NOT NULL,\n                     estimated_improvement REAL,\n                     detected_at TEXT NOT NULL,\n                     resolved_at TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des profils de performance\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS performance_profiles (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     function_name TEXT NOT NULL,\n                     execution_time REAL,\n                     call_count INTEGER,\n                     memory_usage REAL,\n                     profile_data TEXT,\n                     profiled_at TEXT NOT NULL\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             conn.commit()\n \n     def analyze_project_performance(\n-            self, project_path: str = None) -> PerformanceReport:\n+        self, project_path: str = None\n+    ) -> PerformanceReport:\n         \"\"\"Analyser les performances d'un projet complet\"\"\"\n         project_path = Path(project_path or self.root_path)\n-        logger.info(\n-            f\"\u26a1 Analyse des performances du projet: {project_path.name}\")\n+        logger.info(f\"\u26a1 Analyse des performances du projet: {project_path.name}\")\n \n         # Analyser tous les fichiers Python (ignorer les fichiers cach\u00e9s)\n-        python_files = [f for f in project_path.rglob(\n-            \"*.py\") if not f.name.startswith('._')]\n+        python_files = [\n+            f for f in project_path.rglob(\"*.py\") if not f.name.startswith(\"._\")\n+        ]\n         logger.info(f\"\ud83d\udcc1 {len(python_files)} fichiers Python analys\u00e9s\")\n \n         # Limiter le nombre de fichiers pour les performances\n         if len(python_files) > 50:\n             python_files = python_files[:50]\n@@ -153,49 +163,48 @@\n \n         for py_file in python_files:\n             try:\n                 file_analysis = self.ast_analyzer.analyze_file(py_file)\n                 if file_analysis:\n-                    file_metrics = self._analyze_file_performance(\n-                        file_analysis)\n-                    file_issues = self._detect_performance_issues(\n-                        file_analysis)\n+                    file_metrics = self._analyze_file_performance(file_analysis)\n+                    file_issues = self._detect_performance_issues(file_analysis)\n \n                     all_metrics.extend(file_metrics)\n                     all_issues.extend(file_issues)\n             except Exception as e:\n                 logger.warning(\n-                    f\"Erreur lors de l'analyse de performance de \"\n-                    f\"{py_file}: {e}\")\n+                    f\"Erreur lors de l'analyse de performance de \" f\"{py_file}: {e}\"\n+                )\n \n         # Calculer le score global\n         overall_score = self._calculate_overall_score(all_metrics)\n \n         # G\u00e9n\u00e9rer les recommandations\n-        recommendations = self._generate_performance_recommendations(\n-            all_issues)\n+        recommendations = self._generate_performance_recommendations(all_issues)\n \n         # Identifier les opportunit\u00e9s d'optimisation\n         optimization_opportunities = self._identify_optimization_opportunities(\n-            all_issues)\n+            all_issues\n+        )\n \n         # Cr\u00e9er le rapport\n         report = PerformanceReport(\n             overall_score=overall_score,\n             metrics=all_metrics,\n             issues=all_issues,\n             recommendations=recommendations,\n-            optimization_opportunities=optimization_opportunities\n+            optimization_opportunities=optimization_opportunities,\n         )\n \n         # Sauvegarder le rapport\n         self._save_performance_report(report)\n \n         return report\n \n     def _analyze_file_performance(\n-            self, file_analysis: FileAnalysis) -> List[PerformanceMetric]:\n+        self, file_analysis: FileAnalysis\n+    ) -> List[PerformanceMetric]:\n         \"\"\"Analyser les performances d'un fichier\"\"\"\n         metrics = []\n \n         # M\u00e9trique de complexit\u00e9\n         complexity_metric = PerformanceMetric(\n@@ -203,37 +212,38 @@\n             value=file_analysis.complexity_score,\n             unit=\"cyclomatic\",\n             location=str(file_analysis.file_path),\n             threshold=self.thresholds[\"complexity\"],\n             status=self._get_metric_status(\n-                file_analysis.complexity_score,\n-                self.thresholds[\"complexity\"]))\n+                file_analysis.complexity_score, self.thresholds[\"complexity\"]\n+            ),\n+        )\n         metrics.append(complexity_metric)\n \n         # M\u00e9trique de taille\n         size_metric = PerformanceMetric(\n             metric_type=\"file_size\",\n             value=file_analysis.total_lines,\n             unit=\"lines\",\n             location=str(file_analysis.file_path),\n             threshold=500,\n             status=self._get_metric_status(\n-                file_analysis.total_lines,\n-                500,\n-                reverse=True))\n+                file_analysis.total_lines, 500, reverse=True\n+            ),\n+        )\n         metrics.append(size_metric)\n \n         # M\u00e9trique d'imports\n         imports_metric = PerformanceMetric(\n             metric_type=\"imports\",\n             value=len(file_analysis.imports),\n             unit=\"count\",\n             location=str(file_analysis.file_path),\n             threshold=self.thresholds[\"imports\"],\n             status=self._get_metric_status(\n-                len(file_analysis.imports),\n-                self.thresholds[\"imports\"])\n+                len(file_analysis.imports), self.thresholds[\"imports\"]\n+            ),\n         )\n         metrics.append(imports_metric)\n \n         # M\u00e9triques des fonctions\n         for func in file_analysis.functions:\n@@ -242,18 +252,20 @@\n                 value=func.complexity,\n                 unit=\"cyclomatic\",\n                 location=f\"{file_analysis.file_path}:{func.line_number}\",\n                 threshold=self.thresholds[\"function_size\"],\n                 status=self._get_metric_status(\n-                    func.complexity,\n-                    self.thresholds[\"function_size\"]))\n+                    func.complexity, self.thresholds[\"function_size\"]\n+                ),\n+            )\n             metrics.append(func_metric)\n \n         return metrics\n \n     def _detect_performance_issues(\n-            self, file_analysis: FileAnalysis) -> List[PerformanceIssue]:\n+        self, file_analysis: FileAnalysis\n+    ) -> List[PerformanceIssue]:\n         \"\"\"D\u00e9tecter les probl\u00e8mes de performance dans un fichier\"\"\"\n         issues = []\n \n         # V\u00e9rifier la complexit\u00e9 globale\n         if file_analysis.complexity_score > 8:  # Seuil plus bas\n@@ -261,22 +273,24 @@\n                 issue_type=\"high_complexity\",\n                 location=str(file_analysis.file_path),\n                 description=f\"Complexit\u00e9 \u00e9lev\u00e9e: {file_analysis.complexity_score:.1f}\",\n                 impact=\"medium\" if file_analysis.complexity_score < 15 else \"high\",\n                 suggestion=\"Refactoriser en modules plus petits\",\n-                estimated_improvement=20.0)\n+                estimated_improvement=20.0,\n+            )\n             issues.append(issue)\n \n         # V\u00e9rifier la taille du fichier\n         if file_analysis.total_lines > 300:  # Seuil plus bas\n             issue = PerformanceIssue(\n                 issue_type=\"large_file\",\n                 location=str(file_analysis.file_path),\n                 description=f\"Fichier tr\u00e8s long: {file_analysis.total_lines} lignes\",\n                 impact=\"medium\",\n                 suggestion=\"Diviser en fichiers plus petits\",\n-                estimated_improvement=15.0)\n+                estimated_improvement=15.0,\n+            )\n             issues.append(issue)\n \n         # V\u00e9rifier les fonctions complexes\n         for func in file_analysis.functions:\n             if func.complexity > 8:  # Seuil plus bas\n@@ -285,22 +299,23 @@\n                     location=f\"{file_analysis.file_path}:{func.line_number}\",\n                     description=f\"Fonction {func.name} tr\u00e8s complexe: \"\n                     f\"{func.complexity}\",\n                     impact=\"high\",\n                     suggestion=\"Diviser en sous-fonctions\",\n-                    estimated_improvement=25.0)\n+                    estimated_improvement=25.0,\n+                )\n                 issues.append(issue)\n \n         # V\u00e9rifier les imports excessifs\n         if len(file_analysis.imports) > 20:  # Seuil plus bas\n             issue = PerformanceIssue(\n                 issue_type=\"excessive_imports\",\n                 location=str(file_analysis.file_path),\n                 description=f\"Trop d'imports: {len(file_analysis.imports)}\",\n                 impact=\"low\",\n                 suggestion=\"Nettoyer les imports inutilis\u00e9s\",\n-                estimated_improvement=5.0\n+                estimated_improvement=5.0,\n             )\n             issues.append(issue)\n \n         # V\u00e9rifier les classes complexes\n         for cls in file_analysis.classes:\n@@ -310,20 +325,19 @@\n                     location=f\"{file_analysis.file_path}:{cls.line_number}\",\n                     description=f\"Classe {cls.name} tr\u00e8s complexe: \"\n                     f\"{cls.complexity}\",\n                     impact=\"high\",\n                     suggestion=\"Diviser en classes plus petites\",\n-                    estimated_improvement=30.0)\n+                    estimated_improvement=30.0,\n+                )\n                 issues.append(issue)\n \n         return issues\n \n     def _get_metric_status(\n-            self,\n-            value: float,\n-            threshold: float,\n-            reverse: bool = False) -> str:\n+        self, value: float, threshold: float, reverse: bool = False\n+    ) -> str:\n         \"\"\"D\u00e9terminer le statut d'une m\u00e9trique\"\"\"\n         if reverse:\n             if value > threshold * 1.5:\n                 return \"critical\"\n             elif value > threshold:\n@@ -336,12 +350,11 @@\n             elif value > threshold:\n                 return \"warning\"\n             else:\n                 return \"good\"\n \n-    def _calculate_overall_score(\n-            self, metrics: List[PerformanceMetric]) -> float:\n+    def _calculate_overall_score(self, metrics: List[PerformanceMetric]) -> float:\n         \"\"\"Calculer le score de performance global\"\"\"\n         if not metrics:\n             return 100.0\n \n         total_score = 0\n@@ -360,11 +373,11 @@\n         \"\"\"Obtenir le poids d'une m\u00e9trique\"\"\"\n         weights = {\n             \"complexity\": 3.0,\n             \"function_complexity\": 2.5,\n             \"file_size\": 1.5,\n-            \"imports\": 1.0\n+            \"imports\": 1.0,\n         }\n         return weights.get(metric_type, 1.0)\n \n     def _calculate_metric_score(self, metric: PerformanceMetric) -> float:\n         \"\"\"Calculer le score d'une m\u00e9trique\"\"\"\n@@ -374,11 +387,12 @@\n             return 70.0\n         else:  # critical\n             return 30.0\n \n     def _generate_performance_recommendations(\n-            self, issues: List[PerformanceIssue]) -> List[str]:\n+        self, issues: List[PerformanceIssue]\n+    ) -> List[str]:\n         \"\"\"G\u00e9n\u00e9rer des recommandations de performance\"\"\"\n         recommendations = []\n \n         # Grouper les probl\u00e8mes par type\n         issue_types = {}\n@@ -389,45 +403,48 @@\n \n         # Recommandations par type de probl\u00e8me\n         if \"high_complexity\" in issue_types:\n             count = len(issue_types[\"high_complexity\"])\n             recommendations.append(\n-                f\"\ud83e\udde0 {count} modules tr\u00e8s complexes - priorit\u00e9 au refactoring\")\n+                f\"\ud83e\udde0 {count} modules tr\u00e8s complexes - priorit\u00e9 au refactoring\"\n+            )\n \n         if \"complex_function\" in issue_types:\n             count = len(issue_types[\"complex_function\"])\n             recommendations.append(\n-                f\"\u2699\ufe0f {count} fonctions complexes - diviser en sous-fonctions\")\n+                f\"\u2699\ufe0f {count} fonctions complexes - diviser en sous-fonctions\"\n+            )\n \n         if \"large_file\" in issue_types:\n             count = len(issue_types[\"large_file\"])\n             recommendations.append(\n-                f\"\ud83d\udcc4 {count} fichiers tr\u00e8s longs - diviser en modules\")\n+                f\"\ud83d\udcc4 {count} fichiers tr\u00e8s longs - diviser en modules\"\n+            )\n \n         if \"excessive_imports\" in issue_types:\n             count = len(issue_types[\"excessive_imports\"])\n             recommendations.append(\n-                f\"\ud83d\udce6 {count} fichiers avec trop d'imports - nettoyer\")\n+                f\"\ud83d\udce6 {count} fichiers avec trop d'imports - nettoyer\"\n+            )\n \n         # Recommandations g\u00e9n\u00e9rales\n         if issues:\n-            total_improvement = sum(\n-                issue.estimated_improvement for issue in issues)\n+            total_improvement = sum(issue.estimated_improvement for issue in issues)\n             recommendations.append(\n-                f\"\ud83d\udcc8 Am\u00e9lioration potentielle estim\u00e9e: {total_improvement:.1f}%\")\n+                f\"\ud83d\udcc8 Am\u00e9lioration potentielle estim\u00e9e: {total_improvement:.1f}%\"\n+            )\n \n         return recommendations\n \n     def _identify_optimization_opportunities(\n-            self, issues: List[PerformanceIssue]) -> List[str]:\n+        self, issues: List[PerformanceIssue]\n+    ) -> List[str]:\n         \"\"\"Identifier les opportunit\u00e9s d'optimisation\"\"\"\n         opportunities = []\n \n         # Opportunit\u00e9s par impact\n-        high_impact_issues = [\n-            i for i in issues if i.impact in [\n-                \"high\", \"critical\"]]\n+        high_impact_issues = [i for i in issues if i.impact in [\"high\", \"critical\"]]\n         if high_impact_issues:\n             opportunities.append(\"\ud83d\ude80 Optimisations critiques disponibles\")\n \n         medium_impact_issues = [i for i in issues if i.impact == \"medium\"]\n         if medium_impact_issues:\n@@ -437,11 +454,12 @@\n         complexity_issues = [i for i in issues if \"complexity\" in i.issue_type]\n         if complexity_issues:\n             opportunities.append(\"\ud83e\udde9 Refactoring de complexit\u00e9\")\n \n         size_issues = [\n-            i for i in issues if \"size\" in i.issue_type or \"large\" in i.issue_type]\n+            i for i in issues if \"size\" in i.issue_type or \"large\" in i.issue_type\n+        ]\n         if size_issues:\n             opportunities.append(\"\ud83d\udce6 Division de modules\")\n \n         return opportunities\n \n@@ -450,56 +468,59 @@\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Sauvegarder les m\u00e9triques\n             for metric in report.metrics:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO performance_metrics\n                     (metric_type, value, unit, location, threshold, status,\n                      measured_at)\n                     VALUES (?, ?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    metric.metric_type,\n-                    metric.value,\n-                    metric.unit,\n-                    metric.location,\n-                    metric.threshold,\n-                    metric.status,\n-                    datetime.now().isoformat()\n-                ))\n+                \"\"\",\n+                    (\n+                        metric.metric_type,\n+                        metric.value,\n+                        metric.unit,\n+                        metric.location,\n+                        metric.threshold,\n+                        metric.status,\n+                        datetime.now().isoformat(),\n+                    ),\n+                )\n \n             # Sauvegarder les probl\u00e8mes\n             for issue in report.issues:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO performance_issues\n                     (issue_type, location, description, impact, suggestion,\n                      estimated_improvement, detected_at)\n                     VALUES (?, ?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    issue.issue_type,\n-                    issue.location,\n-                    issue.description,\n-                    issue.impact,\n-                    issue.suggestion,\n-                    issue.estimated_improvement,\n-                    datetime.now().isoformat()\n-                ))\n+                \"\"\",\n+                    (\n+                        issue.issue_type,\n+                        issue.location,\n+                        issue.description,\n+                        issue.impact,\n+                        issue.suggestion,\n+                        issue.estimated_improvement,\n+                        datetime.now().isoformat(),\n+                    ),\n+                )\n \n             conn.commit()\n \n-    def profile_function(self,\n-                         function_path: str,\n-                         function_name: str,\n-                         *args,\n-                         **kwargs) -> Dict[str,\n-                                           Any]:\n+    def profile_function(\n+        self, function_path: str, function_name: str, *args, **kwargs\n+    ) -> Dict[str, Any]:\n         \"\"\"Profiler une fonction sp\u00e9cifique\"\"\"\n         try:\n             # Importer et ex\u00e9cuter la fonction\n             import importlib.util\n-            spec = importlib.util.spec_from_file_location(\n-                \"module\", function_path)\n+\n+            spec = importlib.util.spec_from_file_location(\"module\", function_path)\n             module = importlib.util.module_from_spec(spec)\n             spec.loader.exec_module(module)\n \n             func = getattr(module, function_name)\n \n@@ -513,18 +534,18 @@\n \n             profiler.disable()\n \n             # Analyser les statistiques\n             s = io.StringIO()\n-            stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')\n+            stats = pstats.Stats(profiler, stream=s).sort_stats(\"cumulative\")\n             stats.print_stats()\n \n             return {\n                 \"function_name\": function_name,\n                 \"execution_time\": execution_time,\n                 \"profile_data\": s.getvalue(),\n-                \"result\": result\n+                \"result\": result,\n             }\n \n         except Exception as e:\n             logger.error(f\"Erreur lors du profilage de {function_name}: {e}\")\n             return None\n@@ -535,25 +556,25 @@\n             cursor = conn.cursor()\n \n             # Statistiques globales\n             cursor.execute(\n                 \"SELECT AVG(value) FROM performance_metrics \"\n-                \"WHERE metric_type = 'complexity'\")\n+                \"WHERE metric_type = 'complexity'\"\n+            )\n             avg_complexity = cursor.fetchone()[0] or 0\n \n             cursor.execute(\n-                \"SELECT COUNT(*) FROM performance_issues \"\n-                \"WHERE resolved_at IS NULL\")\n+                \"SELECT COUNT(*) FROM performance_issues \" \"WHERE resolved_at IS NULL\"\n+            )\n             unresolved_issues = cursor.fetchone()[0]\n \n-            cursor.execute(\n-                \"SELECT AVG(estimated_improvement) FROM performance_issues\")\n+            cursor.execute(\"SELECT AVG(estimated_improvement) FROM performance_issues\")\n             avg_improvement = cursor.fetchone()[0] or 0\n \n             return {\n                 \"average_complexity\": avg_complexity,\n                 \"unresolved_issues\": unresolved_issues,\n                 \"average_improvement_potential\": avg_improvement,\n                 \"performance_health\": max(\n                     0, 100 - (avg_complexity * 2 + unresolved_issues * 5)\n-                )\n+                ),\n             }\n--- /Volumes/T7/athalia-dev-setup/athalia_core/project_importer.py\t2025-07-29 18:02:53.620000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/project_importer.py\t2025-07-29 18:12:21.564380+00:00\n@@ -19,17 +19,17 @@\n \n \n class ProjectImporter:\n     def __init__(self):\n         self.project_types = {\n-            'game': ['pygame', 'arcade', 'kivyame'],\n-            'api': ['flask', 'fastapi', 'djangoapi'],\n-            'ai': ['tensorflow', 'pytorch', 'sklearn', 'ai', 'ml'],\n-            'web': ['html', 'css', 'js', 'web', 'frontend'],\n-            'data': ['pandas', 'numpy', 'matplotlib', 'data'],\n-            'mobile': ['kivy', 'flutter', 'mobileapp'],\n-            'iot': ['gpio', 'sensor', 'arduino', 'iot']\n+            \"game\": [\"pygame\", \"arcade\", \"kivyame\"],\n+            \"api\": [\"flask\", \"fastapi\", \"djangoapi\"],\n+            \"ai\": [\"tensorflow\", \"pytorch\", \"sklearn\", \"ai\", \"ml\"],\n+            \"web\": [\"html\", \"css\", \"js\", \"web\", \"frontend\"],\n+            \"data\": [\"pandas\", \"numpy\", \"matplotlib\", \"data\"],\n+            \"mobile\": [\"kivy\", \"flutter\", \"mobileapp\"],\n+            \"iot\": [\"gpio\", \"sensor\", \"arduino\", \"iot\"],\n         }\n \n     def import_project(self, project_path: str) -> Dict[str, Any]:\n         \"\"\"Importe et analyse un projet existant.\"\"\"\n         if not os.path.exists(project_path):\n@@ -50,58 +50,56 @@\n         correction_blueprint = self._generate_correction_blueprint(\n             project_path, structure, project_type, quality_analysis\n         )\n \n         return {\n-            'project_path': project_path,\n-            'structure': structure,\n-            'project_type': project_type,\n-            'quality_analysis': quality_analysis,\n-            'correction_blueprint': correction_blueprint,\n-            'import_timestamp': datetime.now().isoformat()\n+            \"project_path\": project_path,\n+            \"structure\": structure,\n+            \"project_type\": project_type,\n+            \"quality_analysis\": quality_analysis,\n+            \"correction_blueprint\": correction_blueprint,\n+            \"import_timestamp\": datetime.now().isoformat(),\n         }\n \n     def _scan_structure(self, project_path: str) -> Dict[str, Any]:\n         \"\"\"Analyse la structure du projet.\"\"\"\n         structure: Dict[str, Any] = {\n-            'files': [],\n-            'directories': [],\n-            'python_files': [],\n-            'config_files': [],\n-            'test_files': []\n+            \"files\": [],\n+            \"directories\": [],\n+            \"python_files\": [],\n+            \"config_files\": [],\n+            \"test_files\": [],\n         }\n \n         for root, dirs, files in os.walk(project_path):\n             rel_root = os.path.relpath(root, project_path)\n \n             for dir_name in dirs:\n-                if not dir_name.startswith('.'):\n-                    structure['directories'].append(\n-                        os.path.join(rel_root, dir_name))\n+                if not dir_name.startswith(\".\"):\n+                    structure[\"directories\"].append(os.path.join(rel_root, dir_name))\n \n             for file_name in files:\n                 file_path = os.path.join(rel_root, file_name)\n-                structure['files'].append(file_path)\n-\n-                if file_name.endswith('.py'):\n-                    structure['python_files'].append(file_path)\n-                elif file_name in ['requirements.txt', 'setup.py']:\n-                    structure['config_files'].append(file_path)\n-                elif 'test' in file_name.lower():\n-                    structure['test_files'].append(file_path)\n+                structure[\"files\"].append(file_path)\n+\n+                if file_name.endswith(\".py\"):\n+                    structure[\"python_files\"].append(file_path)\n+                elif file_name in [\"requirements.txt\", \"setup.py\"]:\n+                    structure[\"config_files\"].append(file_path)\n+                elif \"test\" in file_name.lower():\n+                    structure[\"test_files\"].append(file_path)\n \n         return structure\n \n-    def _detect_project_type(self, project_path: str,\n-                             structure: Dict[str, Any]) -> str:\n+    def _detect_project_type(self, project_path: str, structure: Dict[str, Any]) -> str:\n         \"\"\"D\u00e9tecte automatiquement le type de projet.\"\"\"\n         # Analyser les fichiers Python pour d\u00e9tecter les imports\n         imports = []\n-        for py_file in structure['python_files']:\n+        for py_file in structure[\"python_files\"]:\n             full_path = os.path.join(project_path, py_file)\n             try:\n-                with open(full_path, 'r', encoding='utf-8') as file_handle:\n+                with open(full_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     content = file_handle.read()\n                     tree = ast.parse(content)\n                     for node in ast.walk(tree):\n                         if isinstance(node, ast.Import):\n                             for alias in node.names:\n@@ -111,12 +109,11 @@\n                                 imports.append(node.module)\n             except Exception:\n                 continue\n \n         # Analyser les noms de fichiers et dossiers\n-        file_names = (file_handle.lower()\n-                      for file_handle in structure['files'])\n+        file_names = (file_handle.lower() for file_handle in structure[\"files\"])\n \n         # Calculer les scores par type\n         scores = {}\n         for project_type, keywords in self.project_types.items():\n             score = 0\n@@ -128,161 +125,160 @@\n             scores[project_type] = int(score)\n \n         # Retourner le type avec le score le plus \u00e9lev\u00e9\n         if scores and any(v > 0 for v in scores.values()):\n             return max(scores, key=scores.get)\n-        return 'generic'\n+        return \"generic\"\n \n     def _analyze_code_quality(self, project_path: str) -> Dict[str, Any]:\n         \"\"\"Analyse la qualit\u00e9 du code.\"\"\"\n         analysis = {\n-            'has_tests': False,\n-            'has_docs': False,\n-            'has_requirements': False,\n-            'has_readme': False,\n-            'python_files_count': 0,\n-            'test_files_count': 0,\n-            'issues': []\n+            \"has_tests\": False,\n+            \"has_docs\": False,\n+            \"has_requirements\": False,\n+            \"has_readme\": False,\n+            \"python_files_count\": 0,\n+            \"test_files_count\": 0,\n+            \"issues\": [],\n         }\n \n         files = os.listdir(project_path)\n \n         # V\u00e9rifications de base\n-        analysis['has_tests'] = any(\n-            file_handle.lower().endswith('.py') for file_handle in files)\n-        analysis['has_docs'] = any(file_handle.endswith('.md')\n-                                   for file_handle in files)\n-        analysis['has_requirements'] = 'requirements.txt' in files\n-        analysis['has_readme'] = 'README.md' in files\n+        analysis[\"has_tests\"] = any(\n+            file_handle.lower().endswith(\".py\") for file_handle in files\n+        )\n+        analysis[\"has_docs\"] = any(file_handle.endswith(\".md\") for file_handle in files)\n+        analysis[\"has_requirements\"] = \"requirements.txt\" in files\n+        analysis[\"has_readme\"] = \"README.md\" in files\n \n         # Compter les fichiers Python\n         for root, dirs, files in os.walk(project_path):\n-            analysis['python_files_count'] += len(\n-                [file_handle for file_handle in files if file_handle.endswith('.py')])\n-            analysis['test_files_count'] += len(\n-                [file_handle for file_handle in files if 'test' in file_handle.lower()])\n+            analysis[\"python_files_count\"] += len(\n+                [file_handle for file_handle in files if file_handle.endswith(\".py\")]\n+            )\n+            analysis[\"test_files_count\"] += len(\n+                [file_handle for file_handle in files if \"test\" in file_handle.lower()]\n+            )\n \n         # D\u00e9tecter les probl\u00e8mes\n-        if not analysis['has_tests']:\n-            analysis['issues'].append(\"Aucun test\")\n-        if not analysis['has_docs']:\n-            analysis['issues'].append(\"Documentation\")\n-        if not analysis['has_requirements']:\n-            analysis['issues'].append(\"Fichier requirements.txt\")\n-        if not analysis['has_readme']:\n-            analysis['issues'].append(\"README.md\")\n+        if not analysis[\"has_tests\"]:\n+            analysis[\"issues\"].append(\"Aucun test\")\n+        if not analysis[\"has_docs\"]:\n+            analysis[\"issues\"].append(\"Documentation\")\n+        if not analysis[\"has_requirements\"]:\n+            analysis[\"issues\"].append(\"Fichier requirements.txt\")\n+        if not analysis[\"has_readme\"]:\n+            analysis[\"issues\"].append(\"README.md\")\n \n         return analysis\n \n-    def _generate_correction_blueprint(self,\n-                                       project_path: str,\n-                                       structure: Dict[str,\n-                                                       Any],\n-                                       project_type: str,\n-                                       quality_analysis: Dict[str,\n-                                                              Any]) -> Dict[str,\n-                                                                            Any]:\n+    def _generate_correction_blueprint(\n+        self,\n+        project_path: str,\n+        structure: Dict[str, Any],\n+        project_type: str,\n+        quality_analysis: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n         \"\"\"G\u00e9n\u00e8re un blueprint de correction pour le projet.\"\"\"\n         project_name = os.path.basename(project_path)\n \n         blueprint = {\n-            'project_name': project_name,\n-            'description': f\"Version am\u00e9lior\u00e9e de {project_name}\",\n-            'project_type': project_type,\n-            'modules': self._suggest_modules(project_type),\n-            'structure': self._suggest_structure(structure),\n-            'dependencies': self._suggest_dependencies(project_type),\n-            'prompts': self._suggest_prompts(project_type),\n-            'booster_ia': True,\n-            'docker': False,\n-            'corrections_needed': quality_analysis['issues'],\n-            'enhancements': self._suggest_enhancements(\n-                project_type,\n-                quality_analysis)\n+            \"project_name\": project_name,\n+            \"description\": f\"Version am\u00e9lior\u00e9e de {project_name}\",\n+            \"project_type\": project_type,\n+            \"modules\": self._suggest_modules(project_type),\n+            \"structure\": self._suggest_structure(structure),\n+            \"dependencies\": self._suggest_dependencies(project_type),\n+            \"prompts\": self._suggest_prompts(project_type),\n+            \"booster_ia\": True,\n+            \"docker\": False,\n+            \"corrections_needed\": quality_analysis[\"issues\"],\n+            \"enhancements\": self._suggest_enhancements(project_type, quality_analysis),\n         }\n \n         return blueprint\n \n     def _suggest_modules(self, project_type: str) -> List[str]:\n         \"\"\"Sugg\u00e8re des modules selon le type de projet.\"\"\"\n         suggestions = {\n-            'game': ['game_engine', 'physics', 'ui', 'audio'],\n-            'api': ['database', 'auth', 'ocs'],\n-            'ai': ['ai_engine', 'data_processing', 'modelapi'],\n-            'web': ['frontend', 'backend', 'database', 'auth'],\n-            'data': ['data_processing', 'ml', 'visualization', 'api'],\n-            'mobile': ['api', 'e', 'notifications'],\n-            'iot': ['sensors', 'communication', 'data_logging', 'api']\n-        }\n-        return suggestions.get(project_type, ['api', 'core', 'utils'])\n+            \"game\": [\"game_engine\", \"physics\", \"ui\", \"audio\"],\n+            \"api\": [\"database\", \"auth\", \"ocs\"],\n+            \"ai\": [\"ai_engine\", \"data_processing\", \"modelapi\"],\n+            \"web\": [\"frontend\", \"backend\", \"database\", \"auth\"],\n+            \"data\": [\"data_processing\", \"ml\", \"visualization\", \"api\"],\n+            \"mobile\": [\"api\", \"e\", \"notifications\"],\n+            \"iot\": [\"sensors\", \"communication\", \"data_logging\", \"api\"],\n+        }\n+        return suggestions.get(project_type, [\"api\", \"core\", \"utils\"])\n \n     def _suggest_structure(self, structure: Dict[str, Any]) -> List[str]:\n         \"\"\"G\u00e8re une structure am\u00e9lior\u00e9e.\"\"\"\n-        base_structure = ['src/', 'tests/', 'docs/', 'config/']\n+        base_structure = [\"src/\", \"tests/\", \"docs/\", \"config/\"]\n \n         # Ajouter des dossiers sp\u00e9cifiques selon les fichiers existants\n-        if structure['python_files']:\n-            base_structure.append('src/')\n-        if not structure['test_files']:\n-            base_structure.append('tests/')\n+        if structure[\"python_files\"]:\n+            base_structure.append(\"src/\")\n+        if not structure[\"test_files\"]:\n+            base_structure.append(\"tests/\")\n         # Correction du NameError : remplacer 'doc' par 'docname'\n-        docnames = ['readme', 'doc', 'documentation']\n-        if not any(any(docname in file_handle.lower() for docname in docnames)\n-                   for file_handle in structure['files']):\n-            base_structure.append('docs/')\n+        docnames = [\"readme\", \"doc\", \"documentation\"]\n+        if not any(\n+            any(docname in file_handle.lower() for docname in docnames)\n+            for file_handle in structure[\"files\"]\n+        ):\n+            base_structure.append(\"docs/\")\n \n         return base_structure\n \n     def _suggest_dependencies(self, project_type: str) -> List[str]:\n         \"\"\"Sugg\u00e8re des d\u00e9pendances selon le type de projet.\"\"\"\n         suggestions = {\n-            'game': ['pygame', 'pymunk'],\n-            'api': ['fastapi', 'sqlalchemy', 'pydantic'],\n-            'ai': ['tensorflow', 'pandas', 'scikit-learn'],\n-            'web': ['flask', 'jinja2', 'emy'],\n-            'data': ['pandas', 'numpy', 'lib'],\n-            'mobile': ['kivy', 'requests'],\n-            'iot': ['pyserial', 'requests']\n-        }\n-        return suggestions.get(project_type, ['flask', 'requests'])\n+            \"game\": [\"pygame\", \"pymunk\"],\n+            \"api\": [\"fastapi\", \"sqlalchemy\", \"pydantic\"],\n+            \"ai\": [\"tensorflow\", \"pandas\", \"scikit-learn\"],\n+            \"web\": [\"flask\", \"jinja2\", \"emy\"],\n+            \"data\": [\"pandas\", \"numpy\", \"lib\"],\n+            \"mobile\": [\"kivy\", \"requests\"],\n+            \"iot\": [\"pyserial\", \"requests\"],\n+        }\n+        return suggestions.get(project_type, [\"flask\", \"requests\"])\n \n     def _suggest_prompts(self, project_type: str) -> List[str]:\n         \"\"\"Sugg\u00e8re des prompts selon le type de projet.\"\"\"\n         suggestions = {\n-            'game': ['game_mechanics.md', 'level_design.md'],\n-            'api': ['api_design.md', 'security_audit.md'],\n-            'ai': ['ml_pipeline.md', 'model_evaluation.md'],\n-            'web': ['web_design.md', 'responsive_ui.md'],\n-            'data': ['data_analysis.md', 'visualization.md'],\n-            'mobile': ['mobile_ui.md', 'performance.md'],\n-            'iot': ['iot_architecture.md', 'sensor_integration.md']\n-        }\n-        return suggestions.get(\n-            project_type, ['prompt.md'])\n-\n-    def _suggest_enhancements(self,\n-                              project_type: str,\n-                              quality_analysis: Dict[str,\n-                                                     Any]) -> List[str]:\n+            \"game\": [\"game_mechanics.md\", \"level_design.md\"],\n+            \"api\": [\"api_design.md\", \"security_audit.md\"],\n+            \"ai\": [\"ml_pipeline.md\", \"model_evaluation.md\"],\n+            \"web\": [\"web_design.md\", \"responsive_ui.md\"],\n+            \"data\": [\"data_analysis.md\", \"visualization.md\"],\n+            \"mobile\": [\"mobile_ui.md\", \"performance.md\"],\n+            \"iot\": [\"iot_architecture.md\", \"sensor_integration.md\"],\n+        }\n+        return suggestions.get(project_type, [\"prompt.md\"])\n+\n+    def _suggest_enhancements(\n+        self, project_type: str, quality_analysis: Dict[str, Any]\n+    ) -> List[str]:\n         \"\"\"Sugg\u00e8re des am\u00e9liorations sp\u00e9cifiques.\"\"\"\n         enhancements = []\n-        if not quality_analysis['has_tests']:\n+        if not quality_analysis[\"has_tests\"]:\n             enhancements.append(\"Ajouter une suite de tests\")\n-        if not quality_analysis['has_docs']:\n+        if not quality_analysis[\"has_docs\"]:\n             enhancements.append(\"G\u00e9n\u00e9rer une documentation\")\n-        if not quality_analysis['has_requirements']:\n+        if not quality_analysis[\"has_requirements\"]:\n             enhancements.append(\"Cr\u00e9er un fichier requirements.txt\")\n-        if not quality_analysis['has_readme']:\n+        if not quality_analysis[\"has_readme\"]:\n             enhancements.append(\"Cr\u00e9er un README.md\")\n         # Am\u00e9liorations sp\u00e9cifiques au type\n-        if project_type == 'api':\n+        if project_type == \"api\":\n             enhancements.append(\"Ajouter une authentification\")\n             enhancements.append(\"Impl\u00e9menter une validation des\")\n-        elif project_type == 'game':\n+        elif project_type == \"game\":\n             enhancements.append(\"Ajouter un syst\u00e8me de f\")\n             enhancements.append(\"Impl\u00e9menter des effets f\")\n-        elif project_type == 'ai':\n+        elif project_type == \"ai\":\n             enhancements.append(\"Ajouter un syst\u00e8me de f\")\n             enhancements.append(\"Impl\u00e9menter un monitoring des f\")\n         return enhancements\n \n \n@@ -290,12 +286,11 @@\n project_importer = ProjectImporter()\n \n \n if __name__ == \"__main__\":\n     if len(sys.argv) < 2:\n-        logger.info(\n-            \"Usage: python -m athalia_core.project_importer <chemin_du_projet>\")\n+        logger.info(\"Usage: python -m athalia_core.project_importer <chemin_du_projet>\")\n         sys.exit(1)\n     project_path = sys.argv[1]\n     importer = ProjectImporter()\n     report = importer.import_project(project_path)\n     pprint.pprint(report)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/logger_advanced.py\t2025-07-29 17:56:29.330000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/logger_advanced.py\t2025-07-29 18:12:21.575775+00:00\n@@ -36,347 +36,341 @@\n \n         # Initialiser les loggers\n         self._setup_loggers()\n \n         # Thread de nettoyage automatique\n-        self.cleanup_thread = threading.Thread(\n-            target=self._cleanup_worker, daemon=True)\n+        self.cleanup_thread = threading.Thread(target=self._cleanup_worker, daemon=True)\n         self.cleanup_thread.start()\n \n     def _setup_loggers(self):\n         \"\"\"Configure tous les loggers\"\"\"\n \n         # Logger principal\n-        self.loggers['main'] = self._create_logger(\n-            'athalia',\n-            self.log_dir / 'athalia.log',\n-            logging.INFO\n+        self.loggers[\"main\"] = self._create_logger(\n+            \"athalia\", self.log_dir / \"athalia.log\", logging.INFO\n         )\n \n         # Logger de validation\n-        self.loggers['validation'] = self._create_logger(\n-            'validation',\n-            self.log_dir / 'validation.log',\n-            logging.DEBUG\n+        self.loggers[\"validation\"] = self._create_logger(\n+            \"validation\", self.log_dir / \"validation.log\", logging.DEBUG\n         )\n \n         # Logger de correction\n-        self.loggers['correction'] = self._create_logger(\n-            'correction',\n-            self.log_dir / 'correction.log',\n-            logging.DEBUG\n+        self.loggers[\"correction\"] = self._create_logger(\n+            \"correction\", self.log_dir / \"correction.log\", logging.DEBUG\n         )\n \n         # Logger de performance\n-        self.loggers['performance'] = self._create_logger(\n-            'performance',\n-            self.log_dir / 'performance.log',\n-            logging.DEBUG\n+        self.loggers[\"performance\"] = self._create_logger(\n+            \"performance\", self.log_dir / \"performance.log\", logging.DEBUG\n         )\n \n         # Logger d'erreurs\n-        self.loggers['errors'] = self._create_logger(\n-            'errors',\n-            self.log_dir / 'errors.log',\n-            logging.ERROR\n-        )\n-\n-    def _create_logger(\n-            self,\n-            name: str,\n-            log_file: Path,\n-            level: int) -> logging.Logger:\n+        self.loggers[\"errors\"] = self._create_logger(\n+            \"errors\", self.log_dir / \"errors.log\", logging.ERROR\n+        )\n+\n+    def _create_logger(self, name: str, log_file: Path, level: int) -> logging.Logger:\n         \"\"\"Cr\u00e9e un logger avec rotation et compression\"\"\"\n \n-        logger = logging.getLogger(f'athalia.{name}')\n+        logger = logging.getLogger(f\"athalia.{name}\")\n         logger.setLevel(level)\n \n         # \u00c9viter les doublons\n         if logger.handlers:\n             return logger\n \n         # Handler pour fichier avec rotation\n         file_handler = logging.handlers.RotatingFileHandler(\n-            log_file,\n-            maxBytes=10 * 1024 * 1024,  # 10MB\n-            backupCount=5,\n-            encoding='utf-8'\n+            log_file, maxBytes=10 * 1024 * 1024, backupCount=5, encoding=\"utf-8\"  # 10MB\n         )\n \n         # Format personnalis\u00e9\n         formatter = logging.Formatter(\n-            '%(asctime)s | %(name)s | %(levelname)s | %(message)s',\n-            datefmt='%Y-%m-%d %H:%M:%S'\n+            \"%(asctime)s | %(name)s | %(levelname)s | %(message)s\",\n+            datefmt=\"%Y-%m-%d %H:%M:%S\",\n         )\n         file_handler.setFormatter(formatter)\n \n         # Handler pour console (seulement pour les erreurs)\n-        if name == 'errors':\n+        if name == \"errors\":\n             console_handler = logging.StreamHandler()\n             console_handler.setLevel(logging.ERROR)\n             console_handler.setFormatter(formatter)\n             logger.addHandler(console_handler)\n \n         logger.addHandler(file_handler)\n         return logger\n \n-    def log_main(self, message: str, level: str = 'INFO', **kwargs):\n+    def log_main(self, message: str, level: str = \"INFO\", **kwargs):\n         \"\"\"Log dans le logger principal\"\"\"\n-        logger = self.loggers['main']\n+        logger = self.loggers[\"main\"]\n         extra_data = f\" | {json.dumps(kwargs)}\" if kwargs else \"\"\n         getattr(logger, level.lower())(f\"{message}{extra_data}\")\n \n-    def log_validation(self, test_name: str,\n-                       result: Dict[str, Any], duration: float):\n+    def log_validation(self, test_name: str, result: Dict[str, Any], duration: float):\n         \"\"\"Log des r\u00e9sultats de validation\"\"\"\n-        logger = self.loggers['validation']\n+        logger = self.loggers[\"validation\"]\n \n         # M\u00e9triques de validation\n-        self.metrics['validation'].append({\n-            'timestamp': datetime.now().isoformat(),\n-            'test_name': test_name,\n-            'success': result.get('success', False),\n-            'duration': duration,\n-            'details': result\n-        })\n+        self.metrics[\"validation\"].append(\n+            {\n+                \"timestamp\": datetime.now().isoformat(),\n+                \"test_name\": test_name,\n+                \"success\": result.get(\"success\", False),\n+                \"duration\": duration,\n+                \"details\": result,\n+            }\n+        )\n \n         # Garder seulement les 1000 derni\u00e8res m\u00e9triques\n-        if len(self.metrics['validation']) > 1000:\n-            self.metrics['validation'].popleft()\n+        if len(self.metrics[\"validation\"]) > 1000:\n+            self.metrics[\"validation\"].popleft()\n \n         logger.info(\n             f\"VALIDATION | {test_name} | {result.get('succes', False)} | {duration:.2f}s | {result}\"\n         )\n \n     def log_correction(\n-            self,\n-            file_path: str,\n-            correction_type: str,\n-            success: bool,\n-            old_content: str,\n-            new_content: str,\n-            duration: float):\n+        self,\n+        file_path: str,\n+        correction_type: str,\n+        success: bool,\n+        old_content: str,\n+        new_content: str,\n+        duration: float,\n+    ):\n         \"\"\"Log des corrections automatiques\"\"\"\n-        logger = self.loggers['correction']\n+        logger = self.loggers[\"correction\"]\n \n         # M\u00e9triques de correction\n-        self.metrics['correction'].append({\n-            'timestamp': datetime.now().isoformat(),\n-            'file_path': file_path,\n-            'type': correction_type,\n-            'success': success,\n-            'duration': duration,\n-            'changes': len(new_content) - len(old_content)\n-        })\n+        self.metrics[\"correction\"].append(\n+            {\n+                \"timestamp\": datetime.now().isoformat(),\n+                \"file_path\": file_path,\n+                \"type\": correction_type,\n+                \"success\": success,\n+                \"duration\": duration,\n+                \"changes\": len(new_content) - len(old_content),\n+            }\n+        )\n \n         # Garder seulement les 1000 derni\u00e8res m\u00e9triques\n-        if len(self.metrics['correction']) > 1000:\n-            self.metrics['correction'].popleft()\n+        if len(self.metrics[\"correction\"]) > 1000:\n+            self.metrics[\"correction\"].popleft()\n \n         logger.info(\n-            f\"CORRECTION | {file_path} | {correction_type} | {success} | {duration:.2f}s\")\n+            f\"CORRECTION | {file_path} | {correction_type} | {success} | {duration:.2f}s\"\n+        )\n \n         if not success:\n-            logger.warning(\n-                f\"CORRECTION_FAILED | {file_path} | {correction_type}\")\n+            logger.warning(f\"CORRECTION_FAILED | {file_path} | {correction_type}\")\n \n     def log_performance(\n-            self,\n-            operation: str,\n-            duration: float,\n-            memory_mb: Optional[float] = None,\n-            cpu_percent: Optional[float] = None,\n-            **kwargs):\n+        self,\n+        operation: str,\n+        duration: float,\n+        memory_mb: Optional[float] = None,\n+        cpu_percent: Optional[float] = None,\n+        **kwargs,\n+    ):\n         \"\"\"Log des m\u00e9triques de performance\"\"\"\n-        logger = self.loggers['performance']\n+        logger = self.loggers[\"performance\"]\n \n         # M\u00e9triques de performance\n         perf_data = {\n-            'timestamp': datetime.now().isoformat(),\n-            'operation': operation,\n-            'duration': duration,\n-            'memory_mb': memory_mb,\n-            'cpu_percent': cpu_percent,\n-            **kwargs\n-        }\n-\n-        self.metrics['performance'].append(perf_data)\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"operation\": operation,\n+            \"duration\": duration,\n+            \"memory_mb\": memory_mb,\n+            \"cpu_percent\": cpu_percent,\n+            **kwargs,\n+        }\n+\n+        self.metrics[\"performance\"].append(perf_data)\n \n         # Garder seulement les 1000 derni\u00e8res m\u00e9triques\n-        if len(self.metrics['performance']) > 1000:\n-            self.metrics['performance'].popleft()\n+        if len(self.metrics[\"performance\"]) > 1000:\n+            self.metrics[\"performance\"].popleft()\n \n         logger.info(\n-            f\"PERFORMANCE | {operation} | {duration:.2f}s | {memory_mb}MB | {cpu_percent}%\")\n+            f\"PERFORMANCE | {operation} | {duration:.2f}s | {memory_mb}MB | {cpu_percent}%\"\n+        )\n \n     def log_error(self, error: Exception, context: str = \"\", **kwargs):\n         \"\"\"Log des erreurs\"\"\"\n-        logger = self.loggers['errors']\n+        logger = self.loggers[\"errors\"]\n \n         error_data = {\n-            'timestamp': datetime.now().isoformat(),\n-            'error_type': type(error).__name__,\n-            'error_message': str(error),\n-            'context': context,\n-            **kwargs\n-        }\n-\n-        self.metrics['errors'].append(error_data)\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"error_type\": type(error).__name__,\n+            \"error_message\": str(error),\n+            \"context\": context,\n+            **kwargs,\n+        }\n+\n+        self.metrics[\"errors\"].append(error_data)\n \n         # Garder seulement les 100 derni\u00e8res erreurs\n-        if len(self.metrics['errors']) > 100:\n-            self.metrics['errors'].popleft()\n+        if len(self.metrics[\"errors\"]) > 100:\n+            self.metrics[\"errors\"].popleft()\n \n         logger.error(\n-            f\"ERROR | {context} | {type(error).__name__} | {str(error)} | {kwargs}\")\n+            f\"ERROR | {context} | {type(error).__name__} | {str(error)} | {kwargs}\"\n+        )\n \n     def get_validation_stats(self, hours: int = 24) -> Dict[str, Any]:\n         \"\"\"R\u00e9cup\u00e8re les statistiques de validation\"\"\"\n         cutoff = datetime.now() - timedelta(hours=hours)\n \n         recent_metrics = [\n-            m for m in self.metrics['validation']\n-            if datetime.fromisoformat(m['timestamp']) > cutoff\n+            m\n+            for m in self.metrics[\"validation\"]\n+            if datetime.fromisoformat(m[\"timestamp\"]) > cutoff\n         ]\n \n         if not recent_metrics:\n-            return {'total': 0, 'success_rate': 0, 'avg_duration': 0}\n+            return {\"total\": 0, \"success_rate\": 0, \"avg_duration\": 0}\n \n         total = len(recent_metrics)\n-        success_count = sum(1 for m in recent_metrics if m['success'])\n+        success_count = sum(1 for m in recent_metrics if m[\"success\"])\n         success_rate = (success_count / total) * 100\n-        avg_duration = sum(m['duration'] for m in recent_metrics) / total\n+        avg_duration = sum(m[\"duration\"] for m in recent_metrics) / total\n \n         return {\n-            'total': total,\n-            'success_rate': success_rate,\n-            'avg_duration': avg_duration,\n-            'recent_tests': recent_metrics[-10:]  # 10 derniers tests\n+            \"total\": total,\n+            \"success_rate\": success_rate,\n+            \"avg_duration\": avg_duration,\n+            \"recent_tests\": recent_metrics[-10:],  # 10 derniers tests\n         }\n \n     def get_correction_stats(self, hours: int = 24) -> Dict[str, Any]:\n         \"\"\"R\u00e9cup\u00e8re les statistiques de correction\"\"\"\n         cutoff = datetime.now() - timedelta(hours=hours)\n \n         recent_metrics = [\n-            m for m in self.metrics['correction']\n-            if datetime.fromisoformat(m['timestamp']) > cutoff\n+            m\n+            for m in self.metrics[\"correction\"]\n+            if datetime.fromisoformat(m[\"timestamp\"]) > cutoff\n         ]\n \n         if not recent_metrics:\n-            return {'total': 0, 'success_rate': 0, 'avg_duration': 0}\n+            return {\"total\": 0, \"success_rate\": 0, \"avg_duration\": 0}\n \n         total = len(recent_metrics)\n-        success_count = sum(1 for m in recent_metrics if m['success'])\n+        success_count = sum(1 for m in recent_metrics if m[\"success\"])\n         success_rate = (success_count / total) * 100\n-        avg_duration = sum(m['duration'] for m in recent_metrics) / total\n+        avg_duration = sum(m[\"duration\"] for m in recent_metrics) / total\n \n         # Statistiques par type de correction\n         correction_types = defaultdict(list)\n         for m in recent_metrics:\n-            correction_types[m['type']].append(m)\n+            correction_types[m[\"type\"]].append(m)\n \n         type_stats = {}\n         for corr_type, metrics in correction_types.items():\n-            type_success = sum(1 for m in metrics if m['success'])\n+            type_success = sum(1 for m in metrics if m[\"success\"])\n             type_stats[corr_type] = {\n-                'count': len(metrics),\n-                'success_rate': (type_success / len(metrics)) * 100\n+                \"count\": len(metrics),\n+                \"success_rate\": (type_success / len(metrics)) * 100,\n             }\n \n         return {\n-            'total': total,\n-            'success_rate': success_rate,\n-            'avg_duration': avg_duration,\n-            'type_stats': type_stats,\n+            \"total\": total,\n+            \"success_rate\": success_rate,\n+            \"avg_duration\": avg_duration,\n+            \"type_stats\": type_stats,\n             # 10 derni\u00e8res corrections\n-            'recent_corrections': recent_metrics[-10:]\n+            \"recent_corrections\": recent_metrics[-10:],\n         }\n \n     def get_performance_stats(self, hours: int = 24) -> Dict[str, Any]:\n         \"\"\"R\u00e9cup\u00e8re les statistiques de performance\"\"\"\n         cutoff = datetime.now() - timedelta(hours=hours)\n \n         recent_metrics = [\n-            m for m in self.metrics['performance']\n-            if datetime.fromisoformat(m['timestamp']) > cutoff\n+            m\n+            for m in self.metrics[\"performance\"]\n+            if datetime.fromisoformat(m[\"timestamp\"]) > cutoff\n         ]\n \n         if not recent_metrics:\n-            return {'total': 0, 'avg_duration': 0}\n+            return {\"total\": 0, \"avg_duration\": 0}\n \n         # Statistiques par op\u00e9ration\n         operations = defaultdict(list)\n         for m in recent_metrics:\n-            operations[m['operation']].append(m)\n+            operations[m[\"operation\"]].append(m)\n \n         op_stats = {}\n         for op, metrics in operations.items():\n-            durations = [m['duration'] for m in metrics]\n+            durations = [m[\"duration\"] for m in metrics]\n             op_stats[op] = {\n-                'count': len(metrics),\n-                'avg_duration': sum(durations) / len(durations),\n-                'min_duration': min(durations),\n-                'max_duration': max(durations)\n+                \"count\": len(metrics),\n+                \"avg_duration\": sum(durations) / len(durations),\n+                \"min_duration\": min(durations),\n+                \"max_duration\": max(durations),\n             }\n \n-        all_durations = [m['duration'] for m in recent_metrics]\n+        all_durations = [m[\"duration\"] for m in recent_metrics]\n \n         return {\n-            'total': len(recent_metrics),\n-            'avg_duration': sum(all_durations) / len(all_durations),\n-            'min_duration': min(all_durations),\n-            'max_duration': max(all_durations),\n-            'operation_stats': op_stats\n+            \"total\": len(recent_metrics),\n+            \"avg_duration\": sum(all_durations) / len(all_durations),\n+            \"min_duration\": min(all_durations),\n+            \"max_duration\": max(all_durations),\n+            \"operation_stats\": op_stats,\n         }\n \n     def get_error_stats(self, hours: int = 24) -> Dict[str, Any]:\n         \"\"\"R\u00e9cup\u00e8re les statistiques d'erreurs\"\"\"\n         cutoff = datetime.now() - timedelta(hours=hours)\n \n         recent_errors = [\n-            m for m in self.metrics['errors']\n-            if datetime.fromisoformat(m['timestamp']) > cutoff\n+            m\n+            for m in self.metrics[\"errors\"]\n+            if datetime.fromisoformat(m[\"timestamp\"]) > cutoff\n         ]\n \n         if not recent_errors:\n-            return {'total': 0, 'error_types': {}}\n+            return {\"total\": 0, \"error_types\": {}}\n \n         # Statistiques par type d'erreur\n         error_types = defaultdict(int)\n         for error in recent_errors:\n-            error_types[error['error_type']] += 1\n+            error_types[error[\"error_type\"]] += 1\n \n         return {\n-            'total': len(recent_errors),\n-            'error_types': dict(error_types),\n-            'recent_errors': recent_errors[-10:]  # 10 derni\u00e8res erreurs\n+            \"total\": len(recent_errors),\n+            \"error_types\": dict(error_types),\n+            \"recent_errors\": recent_errors[-10:],  # 10 derni\u00e8res erreurs\n         }\n \n     def _cleanup_worker(self):\n         \"\"\"Thread de nettoyage automatique des logs\"\"\"\n-        while hasattr(self, '_cleanup_active') and self._cleanup_active:\n+        while hasattr(self, \"_cleanup_active\") and self._cleanup_active:\n             try:\n                 self._cleanup_old_logs()\n                 self._compress_old_logs()\n                 time.sleep(3600)  # Nettoyer toutes les heures\n             except Exception as e:\n                 self.log_error(e, \"cleanup_worker\")\n                 time.sleep(300)  # Attendre 5 minutes en cas d'erreur\n \n     def start_cleanup_worker(self):\n         \"\"\"D\u00e9marre le thread de nettoyage\"\"\"\n-        if not hasattr(self, '_cleanup_active') or not self._cleanup_active:\n+        if not hasattr(self, \"_cleanup_active\") or not self._cleanup_active:\n             self._cleanup_active = True\n             self._cleanup_thread = threading.Thread(\n-                target=self._cleanup_worker, daemon=True)\n+                target=self._cleanup_worker, daemon=True\n+            )\n             self._cleanup_thread.start()\n             self.log_main(\"\ud83e\uddf9 Thread de nettoyage d\u00e9marr\u00e9\")\n \n     def stop_cleanup_worker(self):\n         \"\"\"Arr\u00eate le thread de nettoyage\"\"\"\n-        if hasattr(self, '_cleanup_active'):\n+        if hasattr(self, \"_cleanup_active\"):\n             self._cleanup_active = False\n             self.log_main(\"\ud83d\uded1 Thread de nettoyage arr\u00eat\u00e9\")\n \n     def _cleanup_old_logs(self):\n         \"\"\"Nettoie les anciens logs\"\"\"\n@@ -394,46 +388,43 @@\n     def _compress_old_logs(self):\n         \"\"\"Compresse les logs anciens\"\"\"\n         cutoff = datetime.now() - timedelta(days=7)\n \n         for log_file in self.log_dir.glob(\"*.log.*\"):\n-            if log_file.suffix == '.gz':\n+            if log_file.suffix == \".gz\":\n                 continue\n \n             try:\n                 file_time = datetime.fromtimestamp(log_file.stat().st_mtime)\n                 if file_time < cutoff:\n                     # Compresser le fichier\n-                    with open(log_file, 'rb') as f_in:\n-                        with gzip.open(f\"{log_file}.gz\", 'wb') as f_out:\n+                    with open(log_file, \"rb\") as f_in:\n+                        with gzip.open(f\"{log_file}.gz\", \"wb\") as f_out:\n                             shutil.copyfileobj(f_in, f_out)\n \n                     # Supprimer l'original\n                     log_file.unlink()\n                     self.log_main(f\"Compress\u00e9 log: {log_file.name}\")\n             except Exception as e:\n                 self.log_error(e, f\"compress_logs_{log_file.name}\")\n \n-    def export_metrics(\n-            self, output_file: Optional[str] = None) -> Dict[str, Any]:\n+    def export_metrics(self, output_file: Optional[str] = None) -> Dict[str, Any]:\n         \"\"\"Exporte toutes les m\u00e9triques\"\"\"\n         if output_file is None:\n             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n-            output_file = str(\n-                self.log_dir\n-                / f\"metrics_export_{timestamp}.json\")\n+            output_file = str(self.log_dir / f\"metrics_export_{timestamp}.json\")\n \n         export_data = {\n-            'export_timestamp': datetime.now().isoformat(),\n-            'validation_stats': self.get_validation_stats(),\n-            'correction_stats': self.get_correction_stats(),\n-            'performance_stats': self.get_performance_stats(),\n-            'error_stats': self.get_error_stats(),\n-            'raw_metrics': dict(self.metrics)\n-        }\n-\n-        with open(output_file, 'w', encoding='utf-8') as f:\n+            \"export_timestamp\": datetime.now().isoformat(),\n+            \"validation_stats\": self.get_validation_stats(),\n+            \"correction_stats\": self.get_correction_stats(),\n+            \"performance_stats\": self.get_performance_stats(),\n+            \"error_stats\": self.get_error_stats(),\n+            \"raw_metrics\": dict(self.metrics),\n+        }\n+\n+        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n             json.dump(export_data, f, indent=2, ensure_ascii=False)\n \n         self.log_main(f\"M\u00e9triques export\u00e9es: {output_file}\")\n         return export_data\n \n@@ -441,40 +432,45 @@\n # Instance globale du logger\n athalia_logger = AthaliaLogger()\n \n \n # Fonctions de convenance\n-def log_main(message: str, level: str = 'INFO', **kwargs):\n+def log_main(message: str, level: str = \"INFO\", **kwargs):\n     \"\"\"Log dans le logger principal\"\"\"\n     athalia_logger.log_main(message, level, **kwargs)\n \n \n def log_validation(test_name: str, result: Dict[str, Any], duration: float):\n     \"\"\"Log des r\u00e9sultats de validation\"\"\"\n     athalia_logger.log_validation(test_name, result, duration)\n \n \n-def log_correction(file_path: str, correction_type: str, success: bool,\n-                   old_content: str, new_content: str, duration: float):\n+def log_correction(\n+    file_path: str,\n+    correction_type: str,\n+    success: bool,\n+    old_content: str,\n+    new_content: str,\n+    duration: float,\n+):\n     \"\"\"Log des corrections automatiques\"\"\"\n-    athalia_logger.log_correction(file_path, correction_type, success,\n-                                  old_content, new_content, duration)\n+    athalia_logger.log_correction(\n+        file_path, correction_type, success, old_content, new_content, duration\n+    )\n \n \n def log_performance(\n-        operation: str,\n-        duration: float,\n-        memory_mb: Optional[float] = None,\n-        cpu_percent: Optional[float] = None,\n-        **kwargs):\n+    operation: str,\n+    duration: float,\n+    memory_mb: Optional[float] = None,\n+    cpu_percent: Optional[float] = None,\n+    **kwargs,\n+):\n     \"\"\"Log des m\u00e9triques de performance\"\"\"\n     athalia_logger.log_performance(\n-        operation,\n-        duration,\n-        memory_mb,\n-        cpu_percent,\n-        **kwargs)\n+        operation, duration, memory_mb, cpu_percent, **kwargs\n+    )\n \n \n def log_error(error: Exception, context: str = \"\", **kwargs):\n     \"\"\"Log des erreurs\"\"\"\n     athalia_logger.log_error(error, context, **kwargs)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/docker_robotics.py\t2025-07-29 17:56:31.470000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/docker_robotics.py\t2025-07-29 18:12:21.592606+00:00\n@@ -21,10 +21,11 @@\n \n \n @dataclass\n class DockerServiceConfig:\n     \"\"\"Configuration d'un service Docker\"\"\"\n+\n     name: str\n     image: str\n     environment: Dict[str, str]\n     volumes: List[str]\n     ports: List[str]\n@@ -33,10 +34,11 @@\n \n \n @dataclass\n class DockerValidationResult:\n     \"\"\"R\u00e9sultat de validation Docker\"\"\"\n+\n     compose_valid: bool\n     services: List[DockerServiceConfig]\n     issues: List[str]\n     recommendations: List[str]\n     ready_to_run: bool\n@@ -60,121 +62,126 @@\n \n         # V\u00e9rifier docker-compose.yaml\n         compose_file = self.docker_path / \"compose.yaml\"\n         if compose_file.exists():\n             try:\n-                with open(compose_file, 'r') as f:\n+                with open(compose_file, \"r\") as f:\n                     compose_data = yaml.safe_load(f)\n \n-                if 'services' in compose_data:\n-                    for service_name, service_config in compose_data['services'].items(\n-                    ):\n+                if \"services\" in compose_data:\n+                    for service_name, service_config in compose_data[\n+                        \"services\"\n+                    ].items():\n                         service = self._parse_service_config(\n-                            service_name, service_config)\n+                            service_name, service_config\n+                        )\n                         if service:\n                             services.append(service)\n \n                 # V\u00e9rifications sp\u00e9cifiques Reachy\n                 reachy_service = next(\n-                    (s for s in services if 'reachy' in s.name.lower()), None)\n+                    (s for s in services if \"reachy\" in s.name.lower()), None\n+                )\n                 if reachy_service:\n                     self._validate_reachy_service(\n-                        reachy_service, issues, recommendations)\n+                        reachy_service, issues, recommendations\n+                    )\n                 else:\n                     recommendations.append(\n-                        \"Ajouter un service 'reachy_2023' pour la robotique\")\n+                        \"Ajouter un service 'reachy_2023' pour la robotique\"\n+                    )\n \n             except Exception as e:\n                 issues.append(f\"Erreur parsing docker-compose.yaml: {e}\")\n         else:\n             issues.append(\"docker-compose.yaml manquant\")\n-            recommendations.append(\n-                \"Cr\u00e9er docker-compose.yaml pour le d\u00e9ploiement\")\n+            recommendations.append(\"Cr\u00e9er docker-compose.yaml pour le d\u00e9ploiement\")\n \n         # V\u00e9rifier Dockerfile\n         dockerfile = self.docker_path / \"Dockerfile\"\n         if not dockerfile.exists():\n-            recommendations.append(\n-                \"Ajouter Dockerfile pour la containerisation\")\n+            recommendations.append(\"Ajouter Dockerfile pour la containerisation\")\n \n         # V\u00e9rifier .dockerignore\n         dockerignore = self.project_path / \".dockerignore\"\n         if not dockerignore.exists():\n-            recommendations.append(\n-                \"Ajouter .dockerignore pour optimiser les builds\")\n+            recommendations.append(\"Ajouter .dockerignore pour optimiser les builds\")\n \n         compose_valid = len(issues) == 0\n         ready_to_run = compose_valid and any(\n-            'reachy' in s.name.lower() for s in services)\n+            \"reachy\" in s.name.lower() for s in services\n+        )\n \n         return DockerValidationResult(\n             compose_valid=compose_valid,\n             services=services,\n             issues=issues,\n             recommendations=recommendations,\n-            ready_to_run=ready_to_run\n+            ready_to_run=ready_to_run,\n         )\n \n     def _parse_service_config(\n-            self,\n-            name: str,\n-            config: Dict) -> Optional[DockerServiceConfig]:\n+        self, name: str, config: Dict\n+    ) -> Optional[DockerServiceConfig]:\n         \"\"\"Parser la configuration d'un service\"\"\"\n         try:\n             return DockerServiceConfig(\n                 name=name,\n-                image=config.get('image', ''),\n-                environment=config.get('environment', {}),\n-                volumes=config.get('volumes', []),\n-                ports=config.get('ports', []),\n-                depends_on=config.get('depends_on', []),\n-                network_mode=config.get('network_mode')\n+                image=config.get(\"image\", \"\"),\n+                environment=config.get(\"environment\", {}),\n+                volumes=config.get(\"volumes\", []),\n+                ports=config.get(\"ports\", []),\n+                depends_on=config.get(\"depends_on\", []),\n+                network_mode=config.get(\"network_mode\"),\n             )\n         except Exception as e:\n             self.logger.error(f\"Erreur parsing service {name}: {e}\")\n             return None\n \n     def _validate_reachy_service(\n-            self,\n-            service: DockerServiceConfig,\n-            issues: List[str],\n-            recommendations: List[str]):\n+        self,\n+        service: DockerServiceConfig,\n+        issues: List[str],\n+        recommendations: List[str],\n+    ):\n         \"\"\"Valider sp\u00e9cifiquement le service Reachy\"\"\"\n \n         # V\u00e9rifier image\n         if not service.image:\n             issues.append(\"Image Docker manquante pour le service Reachy\")\n-        elif 'pollenrobotics/reachy' not in service.image:\n+        elif \"pollenrobotics/reachy\" not in service.image:\n             recommendations.append(\n-                \"Utiliser l'image officielle pollenrobotics/reachy_2023\")\n+                \"Utiliser l'image officielle pollenrobotics/reachy_2023\"\n+            )\n \n         # V\u00e9rifier variables d'environnement ROS\n         if isinstance(service.environment, list):\n             env_vars = [str(v) for v in service.environment]\n         else:\n             env_vars = [str(v) for v in service.environment.values()]\n \n-        if not any('ROS_DOMAIN_ID' in var for var in env_vars):\n+        if not any(\"ROS_DOMAIN_ID\" in var for var in env_vars):\n             recommendations.append(\n-                \"Ajouter ROS_DOMAIN_ID dans les variables d'environnement\")\n-\n-        if not any('DISPLAY' in var for var in env_vars):\n+                \"Ajouter ROS_DOMAIN_ID dans les variables d'environnement\"\n+            )\n+\n+        if not any(\"DISPLAY\" in var for var in env_vars):\n             recommendations.append(\"Ajouter DISPLAY pour la visualisation\")\n \n         # V\u00e9rifier volumes\n         if not service.volumes:\n             recommendations.append(\n-                \"Configurer les volumes pour la persistance des donn\u00e9es\")\n+                \"Configurer les volumes pour la persistance des donn\u00e9es\"\n+            )\n         else:\n             # V\u00e9rifier volume source code\n-            source_volumes = [v for v in service.volumes if 'src' in v.lower()]\n+            source_volumes = [v for v in service.volumes if \"src\" in v.lower()]\n             if not source_volumes:\n-                recommendations.append(\n-                    \"Monter le code source dans le container\")\n+                recommendations.append(\"Monter le code source dans le container\")\n \n         # V\u00e9rifier network mode\n-        if service.network_mode != 'host':\n+        if service.network_mode != \"host\":\n             recommendations.append(\"Utiliser network_mode: host pour ROS2\")\n \n     def create_reachy_compose_template(self) -> str:\n         \"\"\"Cr\u00e9er un template docker-compose.yaml pour Reachy\"\"\"\n         template = \"\"\"services:\n@@ -284,44 +291,46 @@\n             self.docker_path.mkdir(exist_ok=True)\n \n             # Cr\u00e9er docker-compose.yaml\n             compose_file = self.docker_path / \"compose.yaml\"\n             if not compose_file.exists():\n-                with open(compose_file, 'w') as f:\n+                with open(compose_file, \"w\") as f:\n                     f.write(self.create_reachy_compose_template())\n                 self.logger.info(\"\u2705 docker-compose.yaml cr\u00e9\u00e9\")\n \n             # Cr\u00e9er Dockerfile\n             dockerfile = self.docker_path / \"Dockerfile\"\n             if not dockerfile.exists():\n-                with open(dockerfile, 'w') as f:\n+                with open(dockerfile, \"w\") as f:\n                     f.write(self.create_dockerfile_template())\n                 self.logger.info(\"\u2705 Dockerfile cr\u00e9\u00e9\")\n \n             # Cr\u00e9er script de d\u00e9marrage\n             start_script = self.docker_path / \"start.sh\"\n             if not start_script.exists():\n-                with open(start_script, 'w') as f:\n+                with open(start_script, \"w\") as f:\n                     f.write(self.create_start_script_template())\n                 os.chmod(start_script, 0o755)\n                 self.logger.info(\"\u2705 start.sh cr\u00e9\u00e9\")\n \n             # Cr\u00e9er .dockerignore\n             dockerignore = self.project_path / \".dockerignore\"\n             if not dockerignore.exists():\n-                with open(dockerignore, 'w') as f:\n-                    f.write(\"\"\"# Fichiers \u00e0 ignorer pour Docker\n+                with open(dockerignore, \"w\") as f:\n+                    f.write(\n+                        \"\"\"# Fichiers \u00e0 ignorer pour Docker\n .git\n .gitignore\n README.md\n *.log\n __pycache__\n *.pyc\n .coverage\n htmlcov/\n .pytest_cache/\n-\"\"\")\n+\"\"\"\n+                    )\n                 self.logger.info(\"\u2705 .dockerignore cr\u00e9\u00e9\")\n \n             return True\n \n         except Exception as e:\n@@ -339,14 +348,12 @@\n             cmd = [\"docker-compose\", \"-f\", str(compose_file), \"up\", \"-d\"]\n             if service:\n                 cmd.append(service)\n \n             result = subprocess.run(\n-                cmd,\n-                cwd=self.project_path,\n-                capture_output=True,\n-                text=True)\n+                cmd, cwd=self.project_path, capture_output=True, text=True\n+            )\n \n             if result.returncode == 0:\n                 self.logger.info(\"\u2705 Docker Compose lanc\u00e9 avec succ\u00e8s\")\n                 return True\n             else:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/reachy_auditor.py\t2025-07-29 18:02:53.620000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/reachy_auditor.py\t2025-07-29 18:12:21.601397+00:00\n@@ -22,10 +22,11 @@\n \n \n @dataclass\n class ReachyAuditResult:\n     \"\"\"R\u00e9sultat d'audit Reachy\"\"\"\n+\n     project_path: str\n     timestamp: datetime\n     ros2_valid: bool\n     docker_valid: bool\n     rust_valid: bool\n@@ -70,11 +71,13 @@\n             score -= 20\n             issues.extend(rust_issues)\n         recommendations.extend(rust_recommendations)\n \n         # Audit structure\n-        structure_valid, structure_issues, structure_recommendations = self._audit_structure()\n+        structure_valid, structure_issues, structure_recommendations = (\n+            self._audit_structure()\n+        )\n         if not structure_valid:\n             score -= 25\n             issues.extend(structure_issues)\n         recommendations.extend(structure_recommendations)\n \n@@ -85,11 +88,11 @@\n             docker_valid=docker_valid,\n             rust_valid=rust_valid,\n             structure_valid=structure_valid,\n             issues=issues,\n             recommendations=recommendations,\n-            score=max(0, score)\n+            score=max(0, score),\n         )\n \n         self.logger.info(f\"\u2705 Audit termin\u00e9 - Score: {score:.1f}/100\")\n         return result\n \n@@ -113,19 +116,19 @@\n             self.logger.info(f\"\ud83d\udce6 {len(packages)} packages ROS2 d\u00e9tect\u00e9s\")\n \n         # V\u00e9rifier launch files\n         launch_files = list(self.project_path.rglob(\"*.launch.py\"))\n         if not launch_files:\n-            recommendations.append(\n-                \"Ajouter des fichiers launch.py pour le d\u00e9ploiement\")\n+            recommendations.append(\"Ajouter des fichiers launch.py pour le d\u00e9ploiement\")\n \n         # V\u00e9rifier URDF/XACRO\n         urdf_files = list(self.project_path.rglob(\"*.urdf\"))\n         xacro_files = list(self.project_path.rglob(\"*.xacro\"))\n         if not urdf_files and not xacro_files:\n             recommendations.append(\n-                \"Ajouter des fichiers URDF/XACRO pour la description du robot\")\n+                \"Ajouter des fichiers URDF/XACRO pour la description du robot\"\n+            )\n \n         return len(issues) == 0, issues, recommendations\n \n     def _audit_docker(self) -> Tuple[bool, List[str], List[str]]:\n         \"\"\"Audit Docker/Containers\"\"\"\n@@ -134,44 +137,45 @@\n \n         # V\u00e9rifier docker-compose.yaml\n         compose_file = self.project_path / \"docker\" / \"compose.yaml\"\n         if compose_file.exists():\n             try:\n-                with open(compose_file, 'r') as f:\n+                with open(compose_file, \"r\") as f:\n                     compose_data = yaml.safe_load(f)\n \n                 # V\u00e9rifier service reachy_2023\n-                if 'services' in compose_data and 'reachy_2023' in compose_data['services']:\n-                    service = compose_data['services']['reachy_2023']\n+                if (\n+                    \"services\" in compose_data\n+                    and \"reachy_2023\" in compose_data[\"services\"]\n+                ):\n+                    service = compose_data[\"services\"][\"reachy_2023\"]\n \n                     # V\u00e9rifier variables d'environnement ROS\n-                    if 'environment' in service:\n-                        env_vars = service['environment']\n-                        if any('ROS_DOMAIN_ID' in str(var)\n-                               for var in env_vars):\n+                    if \"environment\" in service:\n+                        env_vars = service[\"environment\"]\n+                        if any(\"ROS_DOMAIN_ID\" in str(var) for var in env_vars):\n                             self.logger.info(\"\u2705 ROS_DOMAIN_ID configur\u00e9\")\n                         else:\n                             recommendations.append(\n-                                \"Ajouter ROS_DOMAIN_ID dans docker-compose\")\n+                                \"Ajouter ROS_DOMAIN_ID dans docker-compose\"\n+                            )\n \n                     # V\u00e9rifier volumes\n-                    if 'volumes' in service:\n+                    if \"volumes\" in service:\n                         self.logger.info(\"\u2705 Volumes Docker configur\u00e9s\")\n                     else:\n                         recommendations.append(\"Configurer les volumes Docker\")\n \n             except Exception as e:\n                 issues.append(f\"Erreur parsing docker-compose.yaml: {e}\")\n         else:\n-            recommendations.append(\n-                \"Ajouter docker-compose.yaml pour le d\u00e9ploiement\")\n+            recommendations.append(\"Ajouter docker-compose.yaml pour le d\u00e9ploiement\")\n \n         # V\u00e9rifier Dockerfile\n         dockerfile = self.project_path / \"docker\" / \"Dockerfile\"\n         if not dockerfile.exists():\n-            recommendations.append(\n-                \"Ajouter Dockerfile pour la containerisation\")\n+            recommendations.append(\"Ajouter Dockerfile pour la containerisation\")\n \n         return len(issues) == 0, issues, recommendations\n \n     def _audit_rust(self) -> Tuple[bool, List[str], List[str]]:\n         \"\"\"Audit Rust/Cargo\"\"\"\n@@ -184,24 +188,25 @@\n             self.logger.info(f\"\ud83d\udd27 {len(cargo_files)} projets Rust d\u00e9tect\u00e9s\")\n \n             for cargo_file in cargo_files:\n                 try:\n                     # V\u00e9rifier d\u00e9pendances critiques\n-                    with open(cargo_file, 'r') as f:\n+                    with open(cargo_file, \"r\") as f:\n                         content = f.read()\n \n-                    if 'ros2' in content.lower():\n+                    if \"ros2\" in content.lower():\n                         self.logger.info(\"\u2705 D\u00e9pendances ROS2 Rust d\u00e9tect\u00e9es\")\n \n-                    if 'dynamixel' in content.lower():\n+                    if \"dynamixel\" in content.lower():\n                         self.logger.info(\"\u2705 Support Dynamixel d\u00e9tect\u00e9\")\n \n                 except Exception as e:\n                     issues.append(f\"Erreur lecture {cargo_file}: {e}\")\n         else:\n             recommendations.append(\n-                \"Consid\u00e9rer l'ajout de composants Rust pour les performances\")\n+                \"Consid\u00e9rer l'ajout de composants Rust pour les performances\"\n+            )\n \n         return len(issues) == 0, issues, recommendations\n \n     def _audit_structure(self) -> Tuple[bool, List[str], List[str]]:\n         \"\"\"Audit structure g\u00e9n\u00e9rale du projet\"\"\"\n@@ -219,20 +224,16 @@\n         gitignore = self.project_path / \".gitignore\"\n         if not gitignore.exists():\n             recommendations.append(\"Ajouter .gitignore pour ROS2/Rust\")\n \n         # V\u00e9rifier structure typique Reachy\n-        expected_dirs = [\n-            \"reachy_controllers\",\n-            \"reachy_description\",\n-            \"reachy_gazebo\"]\n+        expected_dirs = [\"reachy_controllers\", \"reachy_description\", \"reachy_gazebo\"]\n         for dir_name in expected_dirs:\n             if (self.project_path / dir_name).exists():\n                 self.logger.info(f\"\u2705 Module {dir_name} pr\u00e9sent\")\n             else:\n-                recommendations.append(\n-                    f\"Consid\u00e9rer l'ajout du module {dir_name}\")\n+                recommendations.append(f\"Consid\u00e9rer l'ajout du module {dir_name}\")\n \n         # V\u00e9rifier tests\n         test_files = list(self.project_path.rglob(\"test_*.py\"))\n         if not test_files:\n             recommendations.append(\"Ajouter des tests unitaires\")\n@@ -277,22 +278,21 @@\n \"\"\"\n \n         return report\n \n     def save_report(\n-            self,\n-            result: ReachyAuditResult,\n-            output_path: Optional[str] = None) -> str:\n+        self, result: ReachyAuditResult, output_path: Optional[str] = None\n+    ) -> str:\n         \"\"\"Sauvegarder le rapport\"\"\"\n         if output_path is None:\n             output_path = (\n                 f\"data/reports/audits/reachy_audit_\"\n                 f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n             )\n \n         report = self.generate_report(result)\n \n-        with open(output_path, 'w', encoding='utf-8') as f:\n+        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n             f.write(report)\n \n         self.logger.info(f\"\ud83d\udcc4 Rapport sauvegard\u00e9: {output_path}\")\n         return output_path\n--- /Volumes/T7/athalia-dev-setup/athalia_core/security.py\t2025-07-29 18:02:53.620000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/security.py\t2025-07-29 18:12:21.605785+00:00\n@@ -9,41 +9,36 @@\n from pathlib import Path\n \n \n def security_audit_project(project_path):\n     \"\"\"Audit de s\u00e9curit\u00e9 dun projet\"\"\"\n-    audit_path = Path(project_path) / 'security_audit.txt'\n+    audit_path = Path(project_path) / \"security_audit.txt\"\n     issues = []\n     patterns = [\n         (r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', \"Mot de passe en clair\"),\n-        (r'sk\\s*-?\\s*[a-zA-Z0-9]{10,}', \"Cl\u00e9 API trouv\u00e9e\"),\n-        (r'os\\.system\\(', \"Appel syst\u00e8me potentiellement dangereux\"),\n+        (r\"sk\\s*-?\\s*[a-zA-Z0-9]{10,}\", \"Cl\u00e9 API trouv\u00e9e\"),\n+        (r\"os\\.system\\(\", \"Appel syst\u00e8me potentiellement dangereux\"),\n     ]\n \n     for root, dirs, files in os.walk(project_path):\n         for file in files:\n-            if file.endswith('.py') or file.endswith('.f(f'):\n+            if file.endswith(\".py\") or file.endswith(\".f(f\"):\n                 file_path = os.path.join(root, file)\n                 try:\n-                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n+                    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                         content = f.read()\n                         for pattern, message in patterns:\n                             if re.search(pattern, content, re.IGNORECASE):\n                                 if message == \"Cl\u00e9 API trouv\u00e9e\":\n-                                    issues.append(\n-                                        f\"Cl\u00e9 API trouv\u00e9e dans {file}\")\n+                                    issues.append(f\"Cl\u00e9 API trouv\u00e9e dans {file}\")\n                                 else:\n                                     issues.append(f\"{message} dans {file}\")\n                 except Exception as e:\n                     issues.append(f\"Erreur lecture {file}: {e}\")\n \n-    with open(audit_path, 'w', encoding='utf-8') as f:\n+    with open(audit_path, \"w\", encoding=\"utf-8\") as f:\n         for issue in issues:\n-            f.write(issue + '\\n')\n+            f.write(issue + \"\\n\")\n \n     score = 100 if not issues else max(0, 100 - 20 * len(issues))\n     is_secure = len(issues) == 0\n-    return {\n-        'secure': is_secure,\n-        'f': is_secure,\n-        'issues': issues,\n-        'score': score}\n+    return {\"secure\": is_secure, \"f\": is_secure, \"issues\": issues, \"score\": score}\n--- /Volumes/T7/athalia-dev-setup/athalia_core/templates/__init__.py\t2025-07-29 17:56:32.840000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/templates/__init__.py\t2025-07-29 18:12:21.606264+00:00\n@@ -5,8 +5,6 @@\n from .base_templates import get_base_templates\n \n # Fichier dinitialisation du sous - package templates dict_data'athalia_core\n \n \n-__all__ = [\n-    'get_base_templates'\n-]\n+__all__ = [\"get_base_templates\"]\n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/ros2_validator.py\t2025-07-29 17:56:31.990000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/ros2_validator.py\t2025-07-29 18:12:21.606221+00:00\n@@ -22,10 +22,11 @@\n \n \n @dataclass\n class ROS2PackageInfo:\n     \"\"\"Informations sur un package ROS2\"\"\"\n+\n     name: str\n     path: Path\n     package_type: str  # ament_cmake, ament_python, etc.\n     dependencies: List[str]\n     has_launch: bool\n@@ -34,10 +35,11 @@\n \n \n @dataclass\n class ROS2ValidationResult:\n     \"\"\"R\u00e9sultat de validation ROS2\"\"\"\n+\n     workspace_valid: bool\n     packages: List[ROS2PackageInfo]\n     issues: List[str]\n     recommendations: List[str]\n     build_ready: bool\n@@ -59,18 +61,17 @@\n         recommendations = []\n         packages = []\n \n         # V\u00e9rifier structure workspace\n         if not self.src_path.exists():\n-            issues.append(\n-                \"Dossier 'src' manquant - structure workspace ROS2 invalide\")\n+            issues.append(\"Dossier 'src' manquant - structure workspace ROS2 invalide\")\n             return ROS2ValidationResult(\n                 workspace_valid=False,\n                 packages=[],\n                 issues=issues,\n                 recommendations=recommendations,\n-                build_ready=False\n+                build_ready=False,\n             )\n \n         # Analyser packages\n         package_dirs = [d for d in self.src_path.iterdir() if d.is_dir()]\n \n@@ -90,28 +91,28 @@\n             recommendations.append(\"Configurer colcon build\")\n \n         # V\u00e9rifier launch files\n         launch_files = list(self.workspace_path.rglob(\"*.launch.py\"))\n         if not launch_files:\n-            recommendations.append(\n-                \"Ajouter des fichiers launch.py pour le d\u00e9ploiement\")\n+            recommendations.append(\"Ajouter des fichiers launch.py pour le d\u00e9ploiement\")\n \n         # V\u00e9rifier URDF/XACRO\n         urdf_files = list(self.workspace_path.rglob(\"*.urdf\"))\n         xacro_files = list(self.workspace_path.rglob(\"*.xacro\"))\n         if not urdf_files and not xacro_files:\n             recommendations.append(\n-                \"Ajouter des fichiers URDF/XACRO pour la description du robot\")\n+                \"Ajouter des fichiers URDF/XACRO pour la description du robot\"\n+            )\n \n         workspace_valid = len(issues) == 0\n \n         return ROS2ValidationResult(\n             workspace_valid=workspace_valid,\n             packages=packages,\n             issues=issues,\n             recommendations=recommendations,\n-            build_ready=build_ready\n+            build_ready=build_ready,\n         )\n \n     def _analyze_package(self, package_dir: Path) -> Optional[ROS2PackageInfo]:\n         \"\"\"Analyser un package ROS2\"\"\"\n         package_xml = package_dir / \"package.xml\"\n@@ -123,43 +124,41 @@\n         try:\n             tree = ET.parse(package_xml)\n             root = tree.getroot()\n \n             # Extraire nom du package\n-            name = root.get('name', package_dir.name)\n+            name = root.get(\"name\", package_dir.name)\n \n             # D\u00e9terminer type de package\n             package_type = self._detect_package_type(package_dir)\n \n             # Extraire d\u00e9pendances\n             dependencies = []\n-            for dep in root.findall('.//depend'):\n+            for dep in root.findall(\".//depend\"):\n                 dependencies.append(dep.text)\n \n             # V\u00e9rifier pr\u00e9sence de fichiers importants\n-            has_launch = (\n-                (package_dir / \"launch\").exists()\n-                or list(package_dir.glob(\"*.launch.py\"))\n+            has_launch = (package_dir / \"launch\").exists() or list(\n+                package_dir.glob(\"*.launch.py\")\n             )\n             has_urdf = (\n                 (package_dir / \"urdf\").exists()\n                 or list(package_dir.glob(\"*.urdf\"))\n                 or list(package_dir.glob(\"*.xacro\"))\n             )\n-            has_tests = (\n-                (package_dir / \"test\").exists()\n-                or list(package_dir.glob(\"test_*.py\"))\n+            has_tests = (package_dir / \"test\").exists() or list(\n+                package_dir.glob(\"test_*.py\")\n             )\n \n             return ROS2PackageInfo(\n                 name=name,\n                 path=package_dir,\n                 package_type=package_type,\n                 dependencies=dependencies,\n                 has_launch=bool(has_launch),\n                 has_urdf=bool(has_urdf),\n-                has_tests=bool(has_tests)\n+                has_tests=bool(has_tests),\n             )\n \n         except Exception as e:\n             self.logger.error(f\"Erreur analyse package {package_dir}: {e}\")\n             return None\n@@ -175,12 +174,11 @@\n \n     def _check_build_system(self) -> bool:\n         \"\"\"V\u00e9rifier si le build system est configur\u00e9\"\"\"\n         try:\n             result = subprocess.run(\n-                ['colcon', '--version'],\n-                capture_output=True, text=True, timeout=10\n+                [\"colcon\", \"--version\"], capture_output=True, text=True, timeout=10\n             )\n             return result.returncode == 0\n         except BaseException:\n             return False\n \n@@ -190,32 +188,32 @@\n         results = []\n \n         for launch_file in launch_files:\n             try:\n                 # V\u00e9rifier syntaxe Python\n-                with open(launch_file, 'r') as f:\n+                with open(launch_file, \"r\") as f:\n                     ast.parse(f.read())\n \n                 # V\u00e9rifier imports ROS2\n-                with open(launch_file, 'r') as f:\n+                with open(launch_file, \"r\") as f:\n                     content = f.read()\n-                    has_launch_import = 'from launch import' in content\n-                    has_launch_ros_import = 'from launch_ros' in content\n-\n-                results.append({\n-                    'file': str(launch_file),\n-                    'valid': True,\n-                    'has_launch_import': has_launch_import,\n-                    'has_launch_ros_import': has_launch_ros_import\n-                })\n+                    has_launch_import = \"from launch import\" in content\n+                    has_launch_ros_import = \"from launch_ros\" in content\n+\n+                results.append(\n+                    {\n+                        \"file\": str(launch_file),\n+                        \"valid\": True,\n+                        \"has_launch_import\": has_launch_import,\n+                        \"has_launch_ros_import\": has_launch_ros_import,\n+                    }\n+                )\n \n             except Exception as e:\n-                results.append({\n-                    'file': str(launch_file),\n-                    'valid': False,\n-                    'error': str(e)\n-                })\n+                results.append(\n+                    {\"file\": str(launch_file), \"valid\": False, \"error\": str(e)}\n+                )\n \n         return results\n \n     def validate_urdf_files(self) -> List[Dict]:\n         \"\"\"Valider les fichiers URDF/XACRO\"\"\"\n@@ -223,32 +221,32 @@\n         xacro_files = list(self.workspace_path.rglob(\"*.xacro\"))\n         results = []\n \n         for urdf_file in urdf_files + xacro_files:\n             try:\n-                with open(urdf_file, 'r') as f:\n+                with open(urdf_file, \"r\") as f:\n                     content = f.read()\n \n                 # V\u00e9rifications basiques\n-                has_robot_tag = '<robot' in content\n-                has_link_tags = '<link' in content\n-                has_joint_tags = '<joint' in content\n-\n-                results.append({\n-                    'file': str(urdf_file),\n-                    'valid': True,\n-                    'has_robot_tag': has_robot_tag,\n-                    'has_link_tags': has_link_tags,\n-                    'has_joint_tags': has_joint_tags\n-                })\n+                has_robot_tag = \"<robot\" in content\n+                has_link_tags = \"<link\" in content\n+                has_joint_tags = \"<joint\" in content\n+\n+                results.append(\n+                    {\n+                        \"file\": str(urdf_file),\n+                        \"valid\": True,\n+                        \"has_robot_tag\": has_robot_tag,\n+                        \"has_link_tags\": has_link_tags,\n+                        \"has_joint_tags\": has_joint_tags,\n+                    }\n+                )\n \n             except Exception as e:\n-                results.append({\n-                    'file': str(urdf_file),\n-                    'valid': False,\n-                    'error': str(e)\n-                })\n+                results.append(\n+                    {\"file\": str(urdf_file), \"valid\": False, \"error\": str(e)}\n+                )\n \n         return results\n \n     def generate_validation_report(self, result: ROS2ValidationResult) -> str:\n         \"\"\"G\u00e9n\u00e9rer rapport de validation\"\"\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/pattern_detector.py\t2025-07-29 17:56:29.750000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/pattern_detector.py\t2025-07-29 18:12:21.610799+00:00\n@@ -21,22 +21,24 @@\n \n \n @dataclass\n class CodePattern:\n     \"\"\"Pattern de code d\u00e9tect\u00e9\"\"\"\n+\n     pattern_type: str  # 'function', 'class', 'logic', 'structure'\n-    signature: str     # Signature unique du pattern\n+    signature: str  # Signature unique du pattern\n     locations: List[str]  # Fichiers o\u00f9 il appara\u00eet\n     similarity_score: float  # Score de similarit\u00e9 (0-1)\n-    complexity: int    # Complexit\u00e9 du pattern\n+    complexity: int  # Complexit\u00e9 du pattern\n     last_seen: datetime\n     correction_history: List[str] = None\n \n \n @dataclass\n class DuplicateAnalysis:\n     \"\"\"Analyse de doublons\"\"\"\n+\n     duplicate_type: str\n     items: List[str]\n     locations: List[str]\n     severity: str\n     similarity_score: float\n@@ -45,10 +47,11 @@\n \n \n @dataclass\n class AntiPattern:\n     \"\"\"Anti-pattern d\u00e9tect\u00e9\"\"\"\n+\n     pattern_name: str\n     description: str\n     locations: List[str]\n     impact: str  # 'low', 'medium', 'high', 'critical'\n     suggestion: str\n@@ -85,11 +88,12 @@\n         \"\"\"Initialiser la base de donn\u00e9es\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Table des patterns de code\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS code_patterns (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     pattern_type TEXT NOT NULL,\n                     signature TEXT UNIQUE NOT NULL,\n                     locations TEXT NOT NULL,\n@@ -97,14 +101,16 @@\n                     complexity INTEGER DEFAULT 1,\n                     last_seen TEXT NOT NULL,\n                     correction_history TEXT,\n                     usage_count INTEGER DEFAULT 1\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des doublons d\u00e9tect\u00e9s\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS duplicates (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     duplicate_type TEXT NOT NULL,\n                     items TEXT NOT NULL,\n                     locations TEXT NOT NULL,\n@@ -114,14 +120,16 @@\n                     estimated_effort TEXT,\n                     detected_at TEXT NOT NULL,\n                     resolved_at TEXT,\n                     resolution_method TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             # Table des anti-patterns\n-            cursor.execute(\"\"\"\n+            cursor.execute(\n+                \"\"\"\n                 CREATE TABLE IF NOT EXISTS antipatterns (\n                     id INTEGER PRIMARY KEY AUTOINCREMENT,\n                     pattern_name TEXT NOT NULL,\n                     description TEXT NOT NULL,\n                     locations TEXT NOT NULL,\n@@ -129,11 +137,12 @@\n                     suggestion TEXT NOT NULL,\n                     previous_corrections TEXT,\n                     detected_at TEXT NOT NULL,\n                     resolved_at TEXT\n                 )\n-            \"\"\")\n+            \"\"\"\n+            )\n \n             conn.commit()\n \n     def _load_patterns(self):\n         \"\"\"Charger les patterns depuis la base de donn\u00e9es\"\"\"\n@@ -148,23 +157,23 @@\n                     signature=row[2],\n                     locations=json.loads(row[3]),\n                     similarity_score=row[4],\n                     complexity=row[5],\n                     last_seen=datetime.fromisoformat(row[6]),\n-                    correction_history=json.loads(row[7]) if row[7] else []\n+                    correction_history=json.loads(row[7]) if row[7] else [],\n                 )\n                 self._pattern_cache[pattern.signature] = pattern\n \n-    def analyze_project_patterns(\n-            self, project_path: str = None) -> Dict[str, Any]:\n+    def analyze_project_patterns(self, project_path: str = None) -> Dict[str, Any]:\n         \"\"\"Analyser les patterns d'un projet complet\"\"\"\n         project_path = Path(project_path or self.root_path)\n         logger.info(f\"\ud83d\udd0d Analyse des patterns du projet: {project_path.name}\")\n \n         # Analyser tous les fichiers Python (ignorer les fichiers cach\u00e9s)\n-        python_files = [f for f in project_path.rglob(\n-            \"*.py\") if not f.name.startswith('._')]\n+        python_files = [\n+            f for f in project_path.rglob(\"*.py\") if not f.name.startswith(\"._\")\n+        ]\n         logger.info(f\"\ud83d\udcc1 {len(python_files)} fichiers Python trouv\u00e9s\")\n \n         # Limiter le nombre de fichiers pour les tests\n         if len(python_files) > 50:\n             python_files = python_files[:50]\n@@ -177,12 +186,11 @@\n \n         for py_file in python_files:\n             try:\n                 file_analysis = self.ast_analyzer.analyze_file(py_file)\n                 if file_analysis:\n-                    file_patterns = self._extract_patterns_from_file(\n-                        file_analysis)\n+                    file_patterns = self._extract_patterns_from_file(file_analysis)\n                     all_patterns.extend(file_patterns)\n             except Exception as e:\n                 logger.warning(f\"Erreur lors de l'analyse de {py_file}: {e}\")\n \n         # D\u00e9tecter les doublons\n@@ -192,32 +200,33 @@\n         # D\u00e9tecter les anti-patterns\n         antipatterns = self._detect_antipatterns(all_patterns)\n         all_antipatterns.extend(antipatterns)\n \n         # Sauvegarder les r\u00e9sultats\n-        self._save_analysis_results(\n-            all_patterns, all_duplicates, all_antipatterns)\n+        self._save_analysis_results(all_patterns, all_duplicates, all_antipatterns)\n \n         # G\u00e9n\u00e9rer les recommandations\n         recommendations = self._generate_recommendations(\n-            all_duplicates, all_antipatterns)\n+            all_duplicates, all_antipatterns\n+        )\n \n         return {\n             \"patterns\": all_patterns,\n             \"duplicates\": all_duplicates,\n             \"antipatterns\": all_antipatterns,\n             \"recommendations\": recommendations,\n             \"summary\": {\n                 \"total_patterns\": len(all_patterns),\n                 \"total_duplicates\": len(all_duplicates),\n                 \"total_antipatterns\": len(all_antipatterns),\n-                \"files_analyzed\": len(python_files)\n-            }\n+                \"files_analyzed\": len(python_files),\n+            },\n         }\n \n     def _extract_patterns_from_file(\n-            self, file_analysis: FileAnalysis) -> List[CodePattern]:\n+        self, file_analysis: FileAnalysis\n+    ) -> List[CodePattern]:\n         \"\"\"Extraire les patterns d'un fichier analys\u00e9\"\"\"\n         patterns = []\n \n         # Patterns de fonctions\n         for func in file_analysis.functions:\n@@ -226,11 +235,11 @@\n                 signature=func.signature,\n                 locations=[str(file_analysis.file_path)],\n                 similarity_score=1.0,\n                 complexity=func.complexity,\n                 last_seen=file_analysis.last_modified,\n-                correction_history=[]\n+                correction_history=[],\n             )\n             patterns.append(pattern)\n \n         # Patterns de classes\n         for cls in file_analysis.classes:\n@@ -239,11 +248,11 @@\n                 signature=cls.signature,\n                 locations=[str(file_analysis.file_path)],\n                 similarity_score=1.0,\n                 complexity=cls.complexity,\n                 last_seen=file_analysis.last_modified,\n-                correction_history=[]\n+                correction_history=[],\n             )\n             patterns.append(pattern)\n \n         # Patterns de conditions\n         for cond in file_analysis.conditionals:\n@@ -252,11 +261,11 @@\n                 signature=cond.signature,\n                 locations=[str(file_analysis.file_path)],\n                 similarity_score=1.0,\n                 complexity=cond.complexity,\n                 last_seen=file_analysis.last_modified,\n-                correction_history=[]\n+                correction_history=[],\n             )\n             patterns.append(pattern)\n \n         # Patterns de boucles\n         for loop in file_analysis.loops:\n@@ -265,19 +274,19 @@\n                 signature=loop.signature,\n                 locations=[str(file_analysis.file_path)],\n                 similarity_score=1.0,\n                 complexity=loop.complexity,\n                 last_seen=file_analysis.last_modified,\n-                correction_history=[]\n+                correction_history=[],\n             )\n             patterns.append(pattern)\n \n         return patterns\n \n     def _detect_duplicates(\n-            self,\n-            patterns: List[CodePattern]) -> List[DuplicateAnalysis]:\n+        self, patterns: List[CodePattern]\n+    ) -> List[DuplicateAnalysis]:\n         \"\"\"D\u00e9tecter les doublons parmi les patterns\"\"\"\n         duplicates = []\n         processed = set()\n \n         # Grouper les patterns par type pour une comparaison plus efficace\n@@ -293,11 +302,11 @@\n                 if pattern1.signature in processed:\n                     continue\n \n                 similar_patterns = [pattern1]\n \n-                for j, pattern2 in enumerate(type_patterns[i + 1:], i + 1):\n+                for j, pattern2 in enumerate(type_patterns[i + 1 :], i + 1):\n                     if pattern2.signature in processed:\n                         continue\n \n                     similarity = self._calculate_similarity(pattern1, pattern2)\n                     if similarity > 0.7:  # Seuil de similarit\u00e9 plus bas\n@@ -322,69 +331,78 @@\n                         effort = \"medium\"\n \n                     duplicate = DuplicateAnalysis(\n                         duplicate_type=pattern1.pattern_type,\n                         items=[p.signature for p in similar_patterns],\n-                        locations=list(set([loc for p in similar_patterns for loc in p.locations])),\n+                        locations=list(\n+                            set([loc for p in similar_patterns for loc in p.locations])\n+                        ),\n                         severity=severity,\n                         similarity_score=similarity,\n-                        suggested_action=\"merge\" if severity in [\"medium\", \"high\"] else \"review\",\n-                        estimated_effort=effort\n+                        suggested_action=(\n+                            \"merge\" if severity in [\"medium\", \"high\"] else \"review\"\n+                        ),\n+                        estimated_effort=effort,\n                     )\n                     duplicates.append(duplicate)\n \n         return duplicates\n \n     def _calculate_similarity(\n-            self,\n-            pattern1: CodePattern,\n-            pattern2: CodePattern) -> float:\n+        self, pattern1: CodePattern, pattern2: CodePattern\n+    ) -> float:\n         \"\"\"Calculer la similarit\u00e9 entre deux patterns\"\"\"\n         # Comparer les signatures\n         return difflib.SequenceMatcher(\n-            None, pattern1.signature, pattern2.signature).ratio()\n-\n-    def _detect_antipatterns(\n-            self,\n-            patterns: List[CodePattern]) -> List[AntiPattern]:\n+            None, pattern1.signature, pattern2.signature\n+        ).ratio()\n+\n+    def _detect_antipatterns(self, patterns: List[CodePattern]) -> List[AntiPattern]:\n         \"\"\"D\u00e9tecter les anti-patterns\"\"\"\n         antipatterns = []\n \n         # Anti-pattern: fonctions trop complexes\n         for pattern in patterns:\n-            if pattern.pattern_type == \"function\" and pattern.complexity > 10:  # Seuil plus bas\n+            if (\n+                pattern.pattern_type == \"function\" and pattern.complexity > 10\n+            ):  # Seuil plus bas\n                 antipattern = AntiPattern(\n                     pattern_name=\"high_complexity_function\",\n                     description=f\"Fonction trop complexe (complexit\u00e9: {pattern.complexity})\",\n                     locations=pattern.locations,\n                     impact=\"medium\" if pattern.complexity < 20 else \"high\",\n                     suggestion=\"Refactoriser en sous-fonctions plus petites\",\n-                    previous_corrections=[])\n+                    previous_corrections=[],\n+                )\n                 antipatterns.append(antipattern)\n \n         # Anti-pattern: classes trop grandes\n         for pattern in patterns:\n-            if pattern.pattern_type == \"class\" and pattern.complexity > 15:  # Seuil plus bas\n+            if (\n+                pattern.pattern_type == \"class\" and pattern.complexity > 15\n+            ):  # Seuil plus bas\n                 antipattern = AntiPattern(\n                     pattern_name=\"large_class\",\n                     description=f\"Classe trop grande (complexit\u00e9: {pattern.complexity})\",\n                     locations=pattern.locations,\n                     impact=\"medium\" if pattern.complexity < 25 else \"high\",\n                     suggestion=\"Diviser en classes plus petites\",\n-                    previous_corrections=[])\n+                    previous_corrections=[],\n+                )\n                 antipatterns.append(antipattern)\n \n         # Anti-pattern: patterns trop similaires (doublons potentiels)\n         for pattern in patterns:\n-            if pattern.pattern_type in [\"function\",\n-                                        \"class\"] and pattern.complexity > 5:\n+            if pattern.pattern_type in [\"function\", \"class\"] and pattern.complexity > 5:\n                 # Chercher des patterns similaires\n                 similar_count = 0\n                 for other_pattern in patterns:\n-                    if (other_pattern != pattern\n+                    if (\n+                        other_pattern != pattern\n                         and other_pattern.pattern_type == pattern.pattern_type\n-                            and self._calculate_similarity(pattern, other_pattern) > 0.6):\n+                        and self._calculate_similarity(pattern, other_pattern) > 0.6\n+                    ):\n                         similar_count += 1\n \n                 if similar_count >= 2:\n                     antipattern = AntiPattern(\n                         pattern_name=\"potential_duplicate\",\n@@ -392,83 +410,92 @@\n                             f\"Pattern potentiellement dupliqu\u00e9 ({similar_count} similaires)\"\n                         ),\n                         locations=pattern.locations,\n                         impact=\"medium\",\n                         suggestion=\"Consid\u00e9rer la fusion ou l'abstraction\",\n-                        previous_corrections=[])\n+                        previous_corrections=[],\n+                    )\n                     antipatterns.append(antipattern)\n \n         return antipatterns\n \n     def _save_analysis_results(\n-            self,\n-            patterns: List[CodePattern],\n-            duplicates: List[DuplicateAnalysis],\n-            antipatterns: List[AntiPattern]\n+        self,\n+        patterns: List[CodePattern],\n+        duplicates: List[DuplicateAnalysis],\n+        antipatterns: List[AntiPattern],\n     ):\n         \"\"\"Sauvegarder les r\u00e9sultats d'analyse\"\"\"\n         with sqlite3.connect(self.db_path) as conn:\n             cursor = conn.cursor()\n \n             # Sauvegarder les patterns\n             for pattern in patterns:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT OR REPLACE INTO code_patterns\n                     (pattern_type, signature, locations, similarity_score,\n                      complexity, last_seen, correction_history)\n                     VALUES (?, ?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    pattern.pattern_type,\n-                    pattern.signature,\n-                    json.dumps(pattern.locations),\n-                    pattern.similarity_score,\n-                    pattern.complexity,\n-                    pattern.last_seen.isoformat(),\n-                    json.dumps(pattern.correction_history or [])\n-                ))\n+                \"\"\",\n+                    (\n+                        pattern.pattern_type,\n+                        pattern.signature,\n+                        json.dumps(pattern.locations),\n+                        pattern.similarity_score,\n+                        pattern.complexity,\n+                        pattern.last_seen.isoformat(),\n+                        json.dumps(pattern.correction_history or []),\n+                    ),\n+                )\n \n             # Sauvegarder les doublons\n             for duplicate in duplicates:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO duplicates\n                     (duplicate_type, items, locations, severity,\n                      similarity_score, suggested_action, estimated_effort, detected_at)\n                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    duplicate.duplicate_type,\n-                    json.dumps(duplicate.items),\n-                    json.dumps(duplicate.locations),\n-                    duplicate.severity,\n-                    duplicate.similarity_score,\n-                    duplicate.suggested_action,\n-                    duplicate.estimated_effort,\n-                    datetime.now().isoformat()\n-                ))\n+                \"\"\",\n+                    (\n+                        duplicate.duplicate_type,\n+                        json.dumps(duplicate.items),\n+                        json.dumps(duplicate.locations),\n+                        duplicate.severity,\n+                        duplicate.similarity_score,\n+                        duplicate.suggested_action,\n+                        duplicate.estimated_effort,\n+                        datetime.now().isoformat(),\n+                    ),\n+                )\n \n             # Sauvegarder les anti-patterns\n             for antipattern in antipatterns:\n-                cursor.execute(\"\"\"\n+                cursor.execute(\n+                    \"\"\"\n                     INSERT INTO antipatterns\n                     (pattern_name, description, locations, impact,\n                      suggestion, previous_corrections, detected_at)\n                     VALUES (?, ?, ?, ?, ?, ?, ?)\n-                \"\"\", (\n-                    antipattern.pattern_name,\n-                    antipattern.description,\n-                    json.dumps(antipattern.locations),\n-                    antipattern.impact,\n-                    antipattern.suggestion,\n-                    json.dumps(antipattern.previous_corrections or []),\n-                    datetime.now().isoformat()\n-                ))\n+                \"\"\",\n+                    (\n+                        antipattern.pattern_name,\n+                        antipattern.description,\n+                        json.dumps(antipattern.locations),\n+                        antipattern.impact,\n+                        antipattern.suggestion,\n+                        json.dumps(antipattern.previous_corrections or []),\n+                        datetime.now().isoformat(),\n+                    ),\n+                )\n \n             conn.commit()\n \n     def _generate_recommendations(\n-            self,\n-            duplicates: List[DuplicateAnalysis],\n-            antipatterns: List[AntiPattern]) -> List[str]:\n+        self, duplicates: List[DuplicateAnalysis], antipatterns: List[AntiPattern]\n+    ) -> List[str]:\n         \"\"\"G\u00e9n\u00e9rer des recommandations bas\u00e9es sur l'analyse\"\"\"\n         recommendations = []\n \n         # Recommandations pour les doublons\n         high_severity_duplicates = [\n@@ -495,13 +522,11 @@\n             recommendations.append(\n                 \"\ud83d\udcca Cr\u00e9er un module utilitaire pour les patterns communs\"\n             )\n \n         if antipatterns:\n-            recommendations.append(\n-                \"\ud83d\udcda Revoir les bonnes pratiques de complexit\u00e9\"\n-            )\n+            recommendations.append(\"\ud83d\udcda Revoir les bonnes pratiques de complexit\u00e9\")\n \n         return recommendations\n \n     def get_learning_insights(self) -> Dict[str, Any]:\n         \"\"\"Obtenir des insights d'apprentissage\"\"\"\n@@ -510,31 +535,33 @@\n \n             # Statistiques des patterns\n             cursor.execute(\"SELECT COUNT(*) FROM code_patterns\")\n             total_patterns = cursor.fetchone()[0]\n \n+            cursor.execute(\"SELECT COUNT(*) FROM duplicates WHERE resolved_at IS NULL\")\n+            unresolved_duplicates = cursor.fetchone()[0]\n+\n             cursor.execute(\n-                \"SELECT COUNT(*) FROM duplicates WHERE resolved_at IS NULL\")\n-            unresolved_duplicates = cursor.fetchone()[0]\n-\n+                \"SELECT COUNT(*) FROM antipatterns WHERE resolved_at IS NULL\"\n+            )\n+            unresolved_antipatterns = cursor.fetchone()[0]\n+\n+            # Patterns les plus utilis\u00e9s\n             cursor.execute(\n-                \"SELECT COUNT(*) FROM antipatterns WHERE resolved_at IS NULL\")\n-            unresolved_antipatterns = cursor.fetchone()[0]\n-\n-            # Patterns les plus utilis\u00e9s\n-            cursor.execute(\"\"\"\n+                \"\"\"\n                 SELECT pattern_type, COUNT(*) as count\n                 FROM code_patterns\n                 GROUP BY pattern_type\n                 ORDER BY count DESC\n-            \"\"\")\n+            \"\"\"\n+            )\n             pattern_distribution = dict(cursor.fetchall())\n \n             return {\n                 \"total_patterns\": total_patterns,\n                 \"unresolved_duplicates\": unresolved_duplicates,\n                 \"unresolved_antipatterns\": unresolved_antipatterns,\n                 \"pattern_distribution\": pattern_distribution,\n                 \"learning_score\": max(\n                     0, 100 - (unresolved_duplicates + unresolved_antipatterns) * 10\n-                )\n+                ),\n             }\n--- /Volumes/T7/athalia-dev-setup/athalia_core/templates/artistic_templates.py\t2025-07-29 17:56:32.950000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/templates/artistic_templates.py\t2025-07-29 18:12:21.622707+00:00\n@@ -8,11 +8,12 @@\n \n def get_artistic_templates() -> Dict[str, str]:\n     \"\"\"Retourne les templates de code pour projets artistiques.\"\"\"\n \n     return {\n-        \"animation/main.py\": '''\"\"\"\n+        \"animation/main.py\": (\n+            '''\"\"\"\n Module d'animation pour projet artistique.\n G\u00e8re les animations et les mouvements.\n \"\"\"\n \n import pygame\n@@ -86,13 +87,14 @@\n \n     pygame.quit()\n \n if __name__ == \"__main__\":\n     main()\n-''',\n-\n-        \"audio/main.py\": '''\"\"\"\n+'''\n+        ),\n+        \"audio/main.py\": (\n+            '''\"\"\"\n Module audio pour projet artistique.\n G\u00e8re la musique et la synchronisation.\n \"\"\"\n \n import pygame\n@@ -153,13 +155,14 @@\n     logger.info(\"Module audio artistique initialis\u00e9\")\n     logger.info(\"Fonctionnalit\u00e9s: chargement musique, synchronisation rythmique\")\n \n if __name__ == \"__main__\":\n     main()\n-''',\n-\n-        \"visualization/main.py\": '''\"\"\"\n+'''\n+        ),\n+        \"visualization/main.py\": (\n+            '''\"\"\"\n Module de visualisation pour projet artistique.\n G\u00e8re les effets visuels et particules.\n \"\"\"\n \n import pygame\n@@ -252,6 +255,7 @@\n     pygame.quit()\n \n if __name__ == \"__main__\":\n     main()\n '''\n+        ),\n     }\n--- /Volumes/T7/athalia-dev-setup/athalia_core/templates/base_templates.py\t2025-07-29 17:56:33.090000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/templates/base_templates.py\t2025-07-29 18:12:21.625205+00:00\n@@ -8,11 +8,12 @@\n \n def get_base_templates() -> Dict[str, str]:\n     \"\"\"Retourne les templates de base pour tous les projets.\"\"\"\n \n     return {\n-        \"api/main.py\": '''\"\"\"\n+        \"api/main.py\": (\n+            '''\"\"\"\n API principale du projet.\n \"\"\"\n \n import logging\n import json\n@@ -75,13 +76,14 @@\n     return jsonify({'error': 'Erreur interne du serveur'}), 500\n \n if __name__ == '__main__':\n     logger.info(\"D\u00e9marrage de l'API {{ project_name }}\")\n     app.run(debug=True, host='0.0.0.0', port=5000)\n-''',\n-\n-        \"tts/tts.py\": '''\"\"\"\n+'''\n+        ),\n+        \"tts/tts.py\": (\n+            '''\"\"\"\n Module de synth\u00e8se vocale.\n \"\"\"\n \n import logging\n from typing import Dict, Any, Optional\n@@ -156,13 +158,14 @@\n     result = tts_manager.synthesize_speech(test_text, 'fr')\n     logger.info(f\"R\u00e9sultat TTS: {result}\")\n \n if __name__ == \"__main__\":\n     main()\n-''',\n-\n-        \"memory/memory.py\": '''\"\"\"\n+'''\n+        ),\n+        \"memory/memory.py\": (\n+            '''\"\"\"\n Module de gestion m\u00e9moire et stockage.\n \"\"\"\n \n import logging\n import json\n@@ -318,6 +321,7 @@\n     logger.info(f\"R\u00e9cup\u00e9ration: {result}\")\n \n if __name__ == \"__main__\":\n     main()\n '''\n+        ),\n     }\n--- /Volumes/T7/athalia-dev-setup/bin/ath-audit.py\t2025-07-29 17:49:47.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-audit.py\t2025-07-29 18:12:21.630019+00:00\n@@ -3,14 +3,23 @@\n import sys\n import argparse\n \n \n def main():\n-    parser = argparse.ArgumentParser(description=\"Audit intelligent d\u2019un projet Athalia/Arkalia\")\n-    parser.add_argument('--project', type=str, default='.', help='Chemin du projet \u00e0 auditer (d\u00e9faut: .)')\n+    parser = argparse.ArgumentParser(\n+        description=\"Audit intelligent d\u2019un projet Athalia/Arkalia\"\n+    )\n+    parser.add_argument(\n+        \"--project\",\n+        type=str,\n+        default=\".\",\n+        help=\"Chemin du projet \u00e0 auditer (d\u00e9faut: .)\",\n+    )\n     args = parser.parse_args()\n-    result = subprocess.run([\"python3\", \"-m\", \"athalia_core.cli\", \"audit\", args.project], check=False)\n+    result = subprocess.run(\n+        [\"python3\", \"-m\", \"athalia_core.cli\", \"audit\", args.project], check=False\n+    )\n     sys.exit(result.returncode)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/robotics_ci.py\t2025-07-29 17:56:31.780000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/robotics_ci.py\t2025-07-29 18:12:21.634575+00:00\n@@ -21,10 +21,11 @@\n \n \n @dataclass\n class CIConfig:\n     \"\"\"Configuration CI/CD\"\"\"\n+\n     ros2_enabled: bool\n     docker_enabled: bool\n     rust_enabled: bool\n     test_enabled: bool\n     deploy_enabled: bool\n@@ -32,10 +33,11 @@\n \n \n @dataclass\n class CIResult:\n     \"\"\"R\u00e9sultat d'ex\u00e9cution CI/CD\"\"\"\n+\n     success: bool\n     stages: Dict[str, bool]\n     logs: Dict[str, str]\n     artifacts: List[str]\n     duration: float\n@@ -208,11 +210,11 @@\n                 return CIResult(\n                     success=False,\n                     stages=stages,\n                     logs=logs,\n                     artifacts=artifacts,\n-                    duration=time.time() - start_time\n+                    duration=time.time() - start_time,\n                 )\n \n         # Stage 2: Build Docker\n         if config.docker_enabled:\n             self.logger.info(\"\ud83d\udc33 Stage 2: Build Docker\")\n@@ -261,11 +263,11 @@\n         return CIResult(\n             success=overall_success,\n             stages=stages,\n             logs=logs,\n             artifacts=artifacts,\n-            duration=duration\n+            duration=duration,\n         )\n \n     def _run_ros2_validation(self) -> Tuple[bool, str]:\n         \"\"\"Ex\u00e9cuter validation ROS2\"\"\"\n         try:\n@@ -283,11 +285,11 @@\n             result = subprocess.run(\n                 [\"colcon\", \"build\", \"--symlink-install\"],\n                 cwd=self.project_path,\n                 capture_output=True,\n                 text=True,\n-                timeout=300\n+                timeout=300,\n             )\n \n             if result.returncode == 0:\n                 return True, f\"Build r\u00e9ussi: {len(packages)} packages\"\n             else:\n@@ -302,15 +304,23 @@\n             dockerfile = self.project_path / \"docker\" / \"Dockerfile\"\n             if not dockerfile.exists():\n                 return False, \"Dockerfile non trouv\u00e9\"\n \n             result = subprocess.run(\n-                [\"docker\", \"build\", \"-f\", str(dockerfile), \"-t\", \"reachy-robotics\", \".\"],\n+                [\n+                    \"docker\",\n+                    \"build\",\n+                    \"-f\",\n+                    str(dockerfile),\n+                    \"-t\",\n+                    \"reachy-robotics\",\n+                    \".\",\n+                ],\n                 cwd=self.project_path,\n                 capture_output=True,\n                 text=True,\n-                timeout=600\n+                timeout=600,\n             )\n \n             if result.returncode == 0:\n                 return True, \"Build Docker r\u00e9ussi\"\n             else:\n@@ -331,15 +341,18 @@\n                 result = subprocess.run(\n                     [\"cargo\", \"build\", \"--release\"],\n                     cwd=project_dir,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n \n                 if result.returncode != 0:\n-                    return False, f\"Build Rust \u00e9chou\u00e9 dans {project_dir}: {result.stderr}\"\n+                    return (\n+                        False,\n+                        f\"Build Rust \u00e9chou\u00e9 dans {project_dir}: {result.stderr}\",\n+                    )\n \n             return True, f\"Build Rust r\u00e9ussi: {len(cargo_files)} projets\"\n \n         except Exception as e:\n             return False, f\"Erreur build Rust: {e}\"\n@@ -351,11 +364,11 @@\n             result = subprocess.run(\n                 [\"colcon\", \"test\", \"--event-handlers\", \"console_direct+\"],\n                 cwd=self.project_path,\n                 capture_output=True,\n                 text=True,\n-                timeout=300\n+                timeout=300,\n             )\n \n             if result.returncode != 0:\n                 return False, f\"Tests ROS2 \u00e9chou\u00e9s: {result.stderr}\"\n \n@@ -365,11 +378,11 @@\n                 result = subprocess.run(\n                     [\"python\", \"-m\", \"pytest\", \"tests/\", \"-v\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n \n                 if result.returncode != 0:\n                     return False, f\"Tests Python \u00e9chou\u00e9s: {result.stderr}\"\n \n@@ -443,16 +456,16 @@\n             workflows_dir = self.project_path / \".github\" / \"workflows\"\n             workflows_dir.mkdir(parents=True, exist_ok=True)\n \n             # Cr\u00e9er workflow\n             workflow_file = workflows_dir / \"robotics-ci.yml\"\n-            with open(workflow_file, 'w') as f:\n+            with open(workflow_file, \"w\") as f:\n                 f.write(self.create_github_workflow())\n \n             # Cr\u00e9er docker-compose.ci.yml\n             compose_file = self.project_path / \"docker-compose.ci.yml\"\n-            with open(compose_file, 'w') as f:\n+            with open(compose_file, \"w\") as f:\n                 f.write(self.create_docker_compose_ci())\n \n             self.logger.info(\"\u2705 Environnement CI configur\u00e9\")\n             return True\n \n--- /Volumes/T7/athalia-dev-setup/bin/ath-build.py\t2025-07-29 17:49:47.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-build.py\t2025-07-29 18:12:21.635919+00:00\n@@ -7,6 +7,6 @@\n     result = subprocess.run([\"python3\", \"-m\", \"athalia_core.main\"], check=False)\n     sys.exit(result.returncode)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_auditor.py\t2025-07-29 18:02:53.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_auditor.py\t2025-07-29 18:12:21.631309+00:00\n@@ -42,15 +42,14 @@\n             \"performance\": {},\n             \"documentation\": {},\n             \"testing\": {},\n             \"structure\": {},\n             \"recommendations\": [],\n-            \"score\": 0\n-        }\n-\n-        logger.info(\n-            f\"\ud83d\udd0d Audit intelligent en cours pour : {self.project_path.name}\")\n+            \"score\": 0,\n+        }\n+\n+        logger.info(f\"\ud83d\udd0d Audit intelligent en cours pour : {self.project_path.name}\")\n \n         # Analyses parall\u00e8les\n         self._analyze_project_info()\n         self._analyze_code_quality()\n         self._analyze_security()\n@@ -73,11 +72,11 @@\n             \"name\": self.project_path.name,\n             \"type\": self._detect_project_type(),\n             \"size\": self._calculate_project_size(),\n             \"languages\": self._detect_languages(),\n             \"dependencies\": self._detect_dependencies(),\n-            \"last_modified\": self._get_last_modified()\n+            \"last_modified\": self._get_last_modified(),\n         }\n         self.audit_results[\"info\"] = info\n \n     def _detect_project_type(self) -> str:\n         \"\"\"D\u00e9tection automatique du type de projet\"\"\"\n@@ -104,79 +103,95 @@\n         code_files = 0\n         for file_path in self.project_path.rglob(\"*\"):\n             if file_path.is_file():\n                 total_files += 1\n                 try:\n-                    with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                    with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                         lines = len(file_handle.readlines())\n                         total_lines += lines\n                         if self._is_code_file(file_path):\n                             code_files += 1\n                 except Exception:\n                     pass\n         return {\n             \"total_files\": total_files,\n             \"total_lines\": total_lines,\n-            \"code_files\": code_files\n+            \"code_files\": code_files,\n         }\n \n     def _is_code_file(self, file_path: Path) -> bool:\n         \"\"\"D\u00e9termine si un fichier est un fichier de code\"\"\"\n         code_extensions = {\n-            '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h',\n-            '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala', '.cs'\n+            \".py\",\n+            \".js\",\n+            \".ts\",\n+            \".jsx\",\n+            \".tsx\",\n+            \".java\",\n+            \".cpp\",\n+            \".c\",\n+            \".h\",\n+            \".go\",\n+            \".rs\",\n+            \".php\",\n+            \".rb\",\n+            \".swift\",\n+            \".kt\",\n+            \".scala\",\n+            \".cs\",\n         }\n         return file_path.suffix.lower() in code_extensions\n \n     def _detect_languages(self) -> List[str]:\n         \"\"\"D\u00e9tection des langages du projet\"\"\"\n         languages = set()\n         for file_path in self.project_path.rglob(\"*\"):\n             if file_path.is_file():\n                 ext = file_path.suffix.lower()\n-                if ext == '.py':\n-                    languages.add('Python')\n-                elif ext in ['.js', '.jsx']:\n-                    languages.add('JavaScript')\n-                elif ext in ['.ts', '.tsx']:\n-                    languages.add('TypeScript')\n-                elif ext == '.java':\n-                    languages.add('Java')\n-                elif ext == '.go':\n-                    languages.add('Go')\n-                elif ext == '.rs':\n-                    languages.add('Rust')\n-                elif ext == '.php':\n-                    languages.add('PHP')\n-                elif ext == '.rb':\n-                    languages.add('Ruby')\n+                if ext == \".py\":\n+                    languages.add(\"Python\")\n+                elif ext in [\".js\", \".jsx\"]:\n+                    languages.add(\"JavaScript\")\n+                elif ext in [\".ts\", \".tsx\"]:\n+                    languages.add(\"TypeScript\")\n+                elif ext == \".java\":\n+                    languages.add(\"Java\")\n+                elif ext == \".go\":\n+                    languages.add(\"Go\")\n+                elif ext == \".rs\":\n+                    languages.add(\"Rust\")\n+                elif ext == \".php\":\n+                    languages.add(\"PHP\")\n+                elif ext == \".rb\":\n+                    languages.add(\"Ruby\")\n         return list(languages)\n \n     def _detect_dependencies(self) -> Dict[str, List[str]]:\n         \"\"\"D\u00e9tection des d\u00e9pendances du projet\"\"\"\n         dependencies = {}\n         # Python\n         req_file = self.project_path / \"requirements.txt\"\n         if req_file.exists():\n             try:\n-                with open(req_file, 'r') as file_handle:\n+                with open(req_file, \"r\") as file_handle:\n                     deps = [\n-                        line.strip() for line in file_handle\n-                        if line.strip() and not line.startswith('#')\n+                        line.strip()\n+                        for line in file_handle\n+                        if line.strip() and not line.startswith(\"#\")\n                     ]\n-                    dependencies['python'] = deps\n+                    dependencies[\"python\"] = deps\n             except Exception:\n                 pass\n         # Node.js\n         package_file = self.project_path / \"package.json\"\n         if package_file.exists():\n             try:\n-                with open(package_file, 'r') as file_handle:\n+                with open(package_file, \"r\") as file_handle:\n                     data = json.load(file_handle)\n-                    deps = list(data.get('dependencies', {}).keys())\n-                    dev_deps = list(data.get('devDependencies', {}).keys())\n-                    dependencies['nodejs'] = deps + dev_deps\n+                    deps = list(data.get(\"dependencies\", {}).keys())\n+                    dev_deps = list(data.get(\"devDependencies\", {}).keys())\n+                    dependencies[\"nodejs\"] = deps + dev_deps\n             except Exception:\n                 pass\n         return dependencies\n \n     def _get_last_modified(self) -> str:\n@@ -193,21 +208,21 @@\n         \"\"\"Analyse de la qualit\u00e9 du code\"\"\"\n         quality = {\n             \"complexity\": self._analyze_complexity(),\n             \"style\": self._analyze_style(),\n             \"documentation\": self._analyze_code_documentation(),\n-            \"naming\": self._analyze_naming_conventions()\n+            \"naming\": self._analyze_naming_conventions(),\n         }\n         self.audit_results[\"code_quality\"] = quality\n \n     def _analyze_complexity(self) -> Dict[str, Any]:\n         \"\"\"Analyse de la complexit\u00e9 du code\"\"\"\n         complexity_scores = []\n \n         for file_path in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     tree = ast.parse(file_handle.read())\n                     complexity = self._calculate_cyclomatic_complexity(tree)\n                     complexity_scores.append(complexity)\n             except Exception:\n                 pass\n@@ -219,11 +234,11 @@\n             avg_complexity = max_complexity = 0\n \n         return {\n             \"avg_complexity\": avg_complexity,\n             \"max_complexity\": max_complexity,\n-            \"status\": \"\u2705\" if avg_complexity < 10 else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if avg_complexity < 10 else \"\u26a0\ufe0f\",\n         }\n \n     def _calculate_cyclomatic_complexity(self, tree: ast.AST) -> int:\n         \"\"\"Calcul de la complexit\u00e9 cyclomatique dun fichier\"\"\"\n         complexity = 1  # Base complexity\n@@ -242,93 +257,97 @@\n         \"\"\"Analyse du style du code\"\"\"\n         style_issues = []\n \n         for file_path in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     lines = file_handle.readlines()\n                     for index, line in enumerate(lines, 1):\n                         if len(line.rstrip()) > 120:\n                             style_issues.append(\n                                 f\"Ligne trop longue: {file_path.name}:{index}\"\n                             )\n-                        if line.strip() and not line.startswith('#'):\n-                            if not line.startswith(\n-                                    (' ', '\\t')) and line.strip():\n+                        if line.strip() and not line.startswith(\"#\"):\n+                            if not line.startswith((\" \", \"\\t\")) and line.strip():\n                                 if not any(\n-                                    keyword in line for keyword in [\n-                                        'class ', 'def ', 'import ', 'from ']):\n+                                    keyword in line\n+                                    for keyword in [\n+                                        \"class \",\n+                                        \"def \",\n+                                        \"import \",\n+                                        \"from \",\n+                                    ]\n+                                ):\n                                     style_issues.append(\n-                                        f\"Indentation: {file_path.name}:{index}\")\n+                                        f\"Indentation: {file_path.name}:{index}\"\n+                                    )\n             except Exception:\n                 pass\n \n         return {\n             \"issues\": style_issues,\n-            \"status\": \"\u2705\" if len(style_issues) < 10 else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if len(style_issues) < 10 else \"\u26a0\ufe0f\",\n         }\n \n     def _analyze_code_documentation(self) -> Dict[str, Any]:\n         \"\"\"Analyse de la documentation du code\"\"\"\n         documented_functions = 0\n         total_functions = 0\n \n         for file_path in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     tree = ast.parse(file_handle.read())\n                     for node in ast.walk(tree):\n                         if isinstance(node, ast.FunctionDef):\n                             total_functions += 1\n                             if ast.get_docstring(node):\n                                 documented_functions += 1\n             except Exception:\n                 pass\n \n         coverage = (\n-            documented_functions\n-            / total_functions\n-            * 100) if total_functions > 0 else 0\n+            (documented_functions / total_functions * 100) if total_functions > 0 else 0\n+        )\n \n         return {\n             \"coverage\": coverage,\n             \"documented_functions\": documented_functions,\n             \"total_functions\": total_functions,\n-            \"status\": \"\u2705\" if coverage > 80 else \"\u26a0\ufe0f\" if coverage < 50 else \"\u2705\"\n+            \"status\": \"\u2705\" if coverage > 80 else \"\u26a0\ufe0f\" if coverage < 50 else \"\u2705\",\n         }\n \n     def _analyze_naming_conventions(self) -> Dict[str, Any]:\n         \"\"\"Analyse des conventions de nommage\"\"\"\n         issues = []\n \n         for file_path in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     tree = ast.parse(file_handle.read())\n                     for node in ast.walk(tree):\n                         if isinstance(node, ast.FunctionDef):\n-                            if not re.match(r'^[a-z_][a-z0-9_]*$', node.name):\n+                            if not re.match(r\"^[a-z_][a-z0-9_]*$\", node.name):\n                                 issues.append(\n-                                    f\"Fonction: {node.name} dans {file_path.name}\")\n+                                    f\"Fonction: {node.name} dans {file_path.name}\"\n+                                )\n                         elif isinstance(node, ast.ClassDef):\n-                            if not re.match(r'^[A-Z][a-zA-Z0-9]*$', node.name):\n+                            if not re.match(r\"^[A-Z][a-zA-Z0-9]*$\", node.name):\n                                 issues.append(\n-                                    f\"Classe: {node.name} dans {file_path.name}\")\n-            except Exception:\n-                pass\n-\n-        return {\n-            \"issues\": issues,\n-            \"status\": \"\u2705\" if len(issues) < 5 else \"\u26a0\ufe0f\"\n-        }\n+                                    f\"Classe: {node.name} dans {file_path.name}\"\n+                                )\n+            except Exception:\n+                pass\n+\n+        return {\"issues\": issues, \"status\": \"\u2705\" if len(issues) < 5 else \"\u26a0\ufe0f\"}\n \n     def _analyze_security(self):\n         \"\"\"Analyse de la s\u00e9curit\u00e9\"\"\"\n         security = {\n             \"vulnerabilities\": self._detect_security_vulnerabilities(),\n             \"secrets\": self._detect_secrets(),\n-            \"permissions\": self._analyze_permissions()\n+            \"permissions\": self._analyze_permissions(),\n         }\n         self.audit_results[\"security\"] = security\n \n     def _detect_security_vulnerabilities(self) -> List[str]:\n         \"\"\"D\u00e9tection des vuln\u00e9rabilit\u00e9s de s\u00e9curit\u00e9\"\"\"\n@@ -344,40 +363,38 @@\n             (r'api_key\\s*=\\s*[\"\\\"][^\"\\']+[\"\\\"]', \"Cl\u00e9 API en clair\"),\n         ]\n \n         for file_path in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     content = file_handle.read()\n                     for pattern, description in dangerous_patterns:\n                         if re.search(pattern, content):\n-                            vulnerabilities.append(\n-                                f\"{description}: {file_path.name}\")\n+                            vulnerabilities.append(f\"{description}: {file_path.name}\")\n             except Exception:\n                 pass\n \n         return vulnerabilities\n \n     def _detect_secrets(self) -> List[str]:\n         \"\"\"D\u00e9tection de secrets\"\"\"\n         secrets = []\n \n         secret_patterns = [\n-            r'[A-Za-z0-9+/]{40,}={0,2}',  # Base64 long\n-            r'sk_[A-Za-z0-9]{24}',  # Stripe secret key\n-            r'AKIA[0-9A-Z]{16},  # AWS access key'\n+            r\"[A-Za-z0-9+/]{40,}={0,2}\",  # Base64 long\n+            r\"sk_[A-Za-z0-9]{24}\",  # Stripe secret key\n+            r\"AKIA[0-9A-Z]{16},  # AWS access key\",\n         ]\n \n         for file_path in self.project_path.rglob(\"*\"):\n             if file_path.is_file():\n                 try:\n-                    with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                    with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                         content = file_handle.read()\n                         for pattern in secret_patterns:\n                             if re.search(pattern, content):\n-                                secrets.append(\n-                                    f\"Secret d\u00e9tect\u00e9: {file_path.name}\")\n+                                secrets.append(f\"Secret d\u00e9tect\u00e9: {file_path.name}\")\n                                 break\n                 except Exception:\n                     pass\n \n         return secrets\n@@ -395,19 +412,19 @@\n                 except Exception:\n                     pass\n \n         return {\n             \"files\": sensitive_files,\n-            \"status\": \"\u2705\" if not sensitive_files else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if not sensitive_files else \"\u26a0\ufe0f\",\n         }\n \n     def _analyze_performance(self):\n         \"\"\"Analyse de la performance\"\"\"\n         performance = {\n             \"file_sizes\": self._analyze_file_sizes(),\n             \"imports\": self._analyze_imports(),\n-            \"memory_usage\": self._estimate_memory_usage()\n+            \"memory_usage\": self._estimate_memory_usage(),\n         }\n         self.audit_results[\"performance\"] = performance\n \n     def _analyze_file_sizes(self) -> Dict[str, Any]:\n         \"\"\"Analyse de la taille des fichiers\"\"\"\n@@ -417,26 +434,25 @@\n         for file_path in self.project_path.rglob(\"*\"):\n             if file_path.is_file():\n                 size = file_path.stat().st_size\n                 total_size += size\n                 if size > 1024 * 1024:  # > 1MB\n-                    large_files.append(\n-                        f\"{file_path.name}: {size / 1024 / 1024:.1f}MB\")\n+                    large_files.append(f\"{file_path.name}: {size / 1024 / 1024:.1f}MB\")\n \n         return {\n             \"total_size_mb\": total_size / 1024 / 1024,\n             \"large_files\": large_files,\n-            \"status\": \"\u2705\" if len(large_files) < 5 else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if len(large_files) < 5 else \"\u26a0\ufe0f\",\n         }\n \n     def _analyze_imports(self) -> Dict[str, Any]:\n         \"\"\"Analyse des imports\"\"\"\n         imports = []\n \n         for file_path in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     tree = ast.parse(file_handle.read())\n                     for node in ast.walk(tree):\n                         if isinstance(node, ast.Import):\n                             for alias in node.names:\n                                 imports.append(alias.name)\n@@ -447,11 +463,11 @@\n                 pass\n \n         return {\n             \"total_imports\": len(imports),\n             \"unique_imports\": len(set(imports)),\n-            \"status\": \"\u2705\" if len(imports) < 100 else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if len(imports) < 100 else \"\u26a0\ufe0f\",\n         }\n \n     def _estimate_memory_usage(self) -> Dict[str, Any]:\n         \"\"\"Estimation de la list_datausage\"\"\"\n         # Estimation basique bas\u00e9e sur la taille du code\n@@ -462,42 +478,44 @@\n \n         estimated_memory = code_size * 0.1  # Estimation approximative\n \n         return {\n             \"estimated_mb\": estimated_memory / 1024 / 1024,\n-            \"status\": \"\u2705\" if estimated_memory < 50 * 1024 * 1024 else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if estimated_memory < 50 * 1024 * 1024 else \"\u26a0\ufe0f\",\n         }\n \n     def _analyze_documentation(self):\n         \"\"\"Analyse de la documentation\"\"\"\n         docs = {\n             \"readme\": self._check_readme(),\n             \"api_docs\": self._check_api_documentation(),\n             \"code_docs\": self._analyze_code_documentation(),\n-            \"guides\": self._check_guides()\n+            \"guides\": self._check_guides(),\n         }\n         self.audit_results[\"documentation\"] = docs\n \n     def _check_readme(self) -> Dict[str, Any]:\n         \"\"\"V\u00e9rification du README\"\"\"\n         readme_files = list(self.project_path.glob(\"README*\"))\n \n         if readme_files:\n             readme = readme_files[0]\n             try:\n-                with open(readme, 'r', encoding='utf-8') as file_handle:\n+                with open(readme, \"r\", encoding=\"utf-8\") as file_handle:\n                     content = file_handle.read()\n                     has_installation = \"installation\" in content.lower()\n-                    has_usage = \"usage\" in content.lower() or \"utilisation\" in content.lower()\n+                    has_usage = (\n+                        \"usage\" in content.lower() or \"utilisation\" in content.lower()\n+                    )\n                     has_license = \"license\" in content.lower()\n \n                     score = sum([has_installation, has_usage, has_license])\n \n                     return {\n                         \"exists\": True,\n                         \"score\": score,\n-                        \"status\": \"\u2705\" if score == 3 else \"\u26a0\ufe0f\"\n+                        \"status\": \"\u2705\" if score == 3 else \"\u26a0\ufe0f\",\n                     }\n             except Exception:\n                 pass\n \n         return {\"exists\": False, \"score\": 0, \"status\": \"\u274c\"}\n@@ -509,57 +527,61 @@\n         for pattern in [\"*api*.py\", \"*api*.py\", \"docs/*.py\", \"docs/*.py\"]:\n             api_docs.extend(self.project_path.glob(pattern))\n \n         return {\n             \"files\": [str(doc) for doc in api_docs],\n-            \"status\": \"\u2705\" if api_docs else \"\u274c\"\n+            \"status\": \"\u2705\" if api_docs else \"\u274c\",\n         }\n \n     def _check_guides(self) -> Dict[str, Any]:\n         \"\"\"V\u00e9rification des guides\"\"\"\n         guides = []\n \n         for pattern in [\n             \"*guide*.py\",\n             \"*tutorial*.py\",\n             \"docs / guides/*\",\n-                \"docs / tutorials/*py\"]:\n+            \"docs / tutorials/*py\",\n+        ]:\n             guides.extend(self.project_path.glob(pattern))\n \n         return {\n             \"files\": [str(guide) for guide in guides],\n-            \"status\": \"\u2705\" if guides else \"\u274c\"\n+            \"status\": \"\u2705\" if guides else \"\u274c\",\n         }\n \n     def _analyze_testing(self):\n         \"\"\"Analyse des tests\"\"\"\n         testing = {\n             \"coverage\": self._analyze_test_coverage(),\n             \"test_files\": self._find_test_files(),\n-            \"quality\": self._analyze_test_quality()\n+            \"quality\": self._analyze_test_quality(),\n         }\n         self.audit_results[\"testing\"] = testing\n \n     def _analyze_test_coverage(self) -> Dict[str, Any]:\n         \"\"\"Analyse de la couverture de tests\"\"\"\n         test_files = list(self.project_path.rglob(\"*test*.py\"))\n         source_files = list(self.project_path.rglob(\"*.py\"))\n \n         # Exclure les fichiers de test des fichiers source\n         source_files = [\n-            file_handle for file_handle in source_files\n+            file_handle\n+            for file_handle in source_files\n             if \"test\" not in file_handle.name.lower()\n         ]\n \n-        coverage_ratio = len(test_files) / \\\n-            len(source_files) if source_files else 0\n+        coverage_ratio = len(test_files) / len(source_files) if source_files else 0\n \n         return {\n             \"total_test_files\": len(test_files),\n             \"total_source_files\": len(source_files),\n             \"coverage_ratio\": coverage_ratio,\n-            \"status\": \"\u2705\" if coverage_ratio > 0.8 else \"\u26a0\ufe0f\" if coverage_ratio < 0.3 else \"\u2705\"}\n+            \"status\": (\n+                \"\u2705\" if coverage_ratio > 0.8 else \"\u26a0\ufe0f\" if coverage_ratio < 0.3 else \"\u2705\"\n+            ),\n+        }\n \n     def _find_test_files(self) -> List[str]:\n         \"\"\"Trouve les fichiers de tests\"\"\"\n         test_files = []\n \n@@ -572,50 +594,52 @@\n         \"\"\"Analyse de la qualit\u00e9 des tests\"\"\"\n         quality_issues = []\n \n         for file_path in self.project_path.rglob(\"*test*.py\"):\n             try:\n-                with open(file_path, 'r', encoding='utf-8') as file_handle:\n+                with open(file_path, \"r\", encoding=\"utf-8\") as file_handle:\n                     content = file_handle.read()\n                     if \"assert\" not in content and \"self.\" not in content:\n-                        quality_issues.append(\n-                            f\"Pas dassertions: {file_path.name}\")\n+                        quality_issues.append(f\"Pas dassertions: {file_path.name}\")\n                     if \"def test_\" not in content:\n                         quality_issues.append(\n-                            f\"Pas de fonctions de test: {file_path.name}\")\n+                            f\"Pas de fonctions de test: {file_path.name}\"\n+                        )\n             except Exception:\n                 pass\n \n         return {\n             \"issues\": quality_issues,\n-            \"status\": \"\u2705\" if len(quality_issues) < 3 else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if len(quality_issues) < 3 else \"\u26a0\ufe0f\",\n         }\n \n     def _analyze_structure(self):\n         \"\"\"Analyse de la structure du projet\"\"\"\n         structure = {\n             \"organization\": self._analyze_organization(),\n             \"naming\": self._analyze_structure_naming(),\n-            \"modularity\": self._analyze_modularity()\n+            \"modularity\": self._analyze_modularity(),\n         }\n         self.audit_results[\"structure\"] = structure\n \n     def _analyze_organization(self) -> Dict[str, Any]:\n         \"\"\"Analyse de lorganisation des dossiers\"\"\"\n         directories = [\n-            dict_data for dict_data in self.project_path.iterdir() if dict_data.is_dir()]\n+            dict_data for dict_data in self.project_path.iterdir() if dict_data.is_dir()\n+        ]\n \n         expected_dirs = [\"src\", \"tests\", \"docs\", \"scripts\", \"data\"]\n         found_dirs = (dict_data.name for dict_data in directories)\n \n         organization_score = sum(\n-            1 for dict_data in expected_dirs if dict_data in found_dirs)\n+            1 for dict_data in expected_dirs if dict_data in found_dirs\n+        )\n \n         return {\n             \"score\": organization_score,\n             \"found_dirs\": found_dirs,\n-            \"status\": \"\u2705 Bien\" if organization_score > 3 else \"\u26a0\ufe0f \u00c0\"\n+            \"status\": \"\u2705 Bien\" if organization_score > 3 else \"\u26a0\ufe0f \u00c0\",\n         }\n \n     def _analyze_structure_naming(self) -> Dict[str, Any]:\n         \"\"\"Analyse du nommage des fichiers et dossiers\"\"\"\n         issues = []\n@@ -626,14 +650,11 @@\n                 if \" \" in name:\n                     issues.append(f\"Espaces dans le nom: {name}\")\n                 if name.startswith(\".\") and name not in [\".py\", \".py\"]:\n                     issues.append(f\"Fichier cach\u00e9: {name}\")\n \n-        return {\n-            \"issues\": issues,\n-            \"status\": \"\u2705\" if len(issues) < 5 else \"\u26a0\ufe0f\"\n-        }\n+        return {\"issues\": issues, \"status\": \"\u2705\" if len(issues) < 5 else \"\u26a0\ufe0f\"}\n \n     def _analyze_modularity(self) -> Dict[str, Any]:\n         \"\"\"Analyse de la modularit\u00e9\"\"\"\n         modules = []\n \n@@ -642,11 +663,11 @@\n                 modules.append(str(file_path.parent))\n \n         return {\n             \"modules\": modules,\n             \"count\": len(modules),\n-            \"status\": \"\u2705\" if len(modules) > 2 else \"\u26a0\ufe0f\"\n+            \"status\": \"\u2705\" if len(modules) > 2 else \"\u26a0\ufe0f\",\n         }\n \n     def _calculate_score(self):\n         \"\"\"Calcul du score global\"\"\"\n         scores = []\n@@ -777,11 +798,11 @@\n     # Affichage du rapport\n     logger.info(auditor.generate_report())\n \n     # Sauvegarde du rapport JSON\n     report_file = f\"audit_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n-    with open(report_file, 'w', encoding='utf-8') as file_handle:\n+    with open(report_file, \"w\", encoding=\"utf-8\") as file_handle:\n         json.dump(results, file_handle, indent=2, ensure_ascii=False)\n \n     logger.info(f\"\ud83d\udcc4 Rapport sauvegard\u00e9: {report_file}\")\n \n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/rust_analyzer.py\t2025-07-29 17:56:31.880000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/rust_analyzer.py\t2025-07-29 18:12:21.637518+00:00\n@@ -14,19 +14,21 @@\n \n \n @dataclass\n class CargoDependency:\n     \"\"\"D\u00e9pendance Cargo\"\"\"\n+\n     name: str\n     version: str\n     features: List[str]\n     optional: bool\n \n \n @dataclass\n class RustProjectInfo:\n     \"\"\"Informations sur un projet Rust\"\"\"\n+\n     name: str\n     path: Path\n     version: str\n     dependencies: List[CargoDependency]\n     dev_dependencies: List[CargoDependency]\n@@ -37,10 +39,11 @@\n \n \n @dataclass\n class RustAnalysisResult:\n     \"\"\"R\u00e9sultat d'analyse Rust\"\"\"\n+\n     projects: List[RustProjectInfo]\n     issues: List[str]\n     recommendations: List[str]\n     build_ready: bool\n     optimization_score: float  # 0-100\n@@ -69,18 +72,17 @@\n         # Trouver tous les Cargo.toml\n         cargo_files = list(self.project_path.rglob(\"Cargo.toml\"))\n \n         if not cargo_files:\n             issues.append(\"Aucun projet Rust trouv\u00e9 (Cargo.toml manquant)\")\n-            recommendations.append(\n-                \"Cr\u00e9er un projet Rust avec: cargo new nom_projet\")\n+            recommendations.append(\"Cr\u00e9er un projet Rust avec: cargo new nom_projet\")\n             return RustAnalysisResult(\n                 projects=[],\n                 issues=issues,\n                 recommendations=recommendations,\n                 build_ready=False,\n-                optimization_score=0.0\n+                optimization_score=0.0,\n             )\n \n         # Analyser chaque projet\n         for cargo_file in cargo_files:\n             project_info = self._analyze_cargo_project(cargo_file)\n@@ -99,38 +101,38 @@\n         return RustAnalysisResult(\n             projects=projects,\n             issues=issues,\n             recommendations=recommendations,\n             build_ready=build_ready,\n-            optimization_score=optimization_score\n+            optimization_score=optimization_score,\n         )\n \n-    def _analyze_cargo_project(\n-            self, cargo_file: Path) -> Optional[RustProjectInfo]:\n+    def _analyze_cargo_project(self, cargo_file: Path) -> Optional[RustProjectInfo]:\n         \"\"\"Analyse un projet Cargo sp\u00e9cifique\"\"\"\n         try:\n             project_path = cargo_file.parent\n             cargo_data = self.validate_cargo_toml(cargo_file)\n \n             # Extraire les informations de base\n-            package_info = cargo_data.get('package', {})\n-            name = package_info.get('name', project_path.name)\n-            version = package_info.get('version', '0.1.0')\n+            package_info = cargo_data.get(\"package\", {})\n+            name = package_info.get(\"name\", project_path.name)\n+            version = package_info.get(\"version\", \"0.1.0\")\n \n             # Analyser les d\u00e9pendances\n-            dependencies = self._parse_dependencies(\n-                cargo_data.get('dependencies', {}))\n+            dependencies = self._parse_dependencies(cargo_data.get(\"dependencies\", {}))\n             dev_dependencies = self._parse_dependencies(\n-                cargo_data.get('dev-dependencies', {}))\n+                cargo_data.get(\"dev-dependencies\", {})\n+            )\n             build_dependencies = self._parse_dependencies(\n-                cargo_data.get('build-dependencies', {}))\n+                cargo_data.get(\"build-dependencies\", {})\n+            )\n \n             # V\u00e9rifier les d\u00e9pendances robotiques\n             has_robotics_deps = any(\n-                self._is_robotics_dependency(dep) for dep in dependencies)\n-            has_ros2_deps = any('ros2' in dep.name.lower()\n-                                for dep in dependencies)\n+                self._is_robotics_dependency(dep) for dep in dependencies\n+            )\n+            has_ros2_deps = any(\"ros2\" in dep.name.lower() for dep in dependencies)\n \n             # Analyser les targets de build\n             build_targets = self._analyze_build_targets(project_path)\n \n             return RustProjectInfo(\n@@ -140,11 +142,11 @@\n                 dependencies=dependencies,\n                 dev_dependencies=dev_dependencies,\n                 build_dependencies=build_dependencies,\n                 has_ros2_deps=has_ros2_deps,\n                 has_robotics_deps=has_robotics_deps,\n-                build_targets=build_targets\n+                build_targets=build_targets,\n             )\n \n         except Exception as e:\n             self.logger.error(f\"Erreur analyse {cargo_file}: {e}\")\n             return None\n@@ -154,36 +156,46 @@\n         dependencies = []\n \n         for name, dep_info in deps_dict.items():\n             if isinstance(dep_info, str):\n                 # Version simple\n-                dependencies.append(CargoDependency(\n-                    name=name,\n-                    version=dep_info,\n-                    features=[],\n-                    optional=False\n-                ))\n+                dependencies.append(\n+                    CargoDependency(\n+                        name=name, version=dep_info, features=[], optional=False\n+                    )\n+                )\n             elif isinstance(dep_info, dict):\n                 # Configuration compl\u00e8te\n-                dependencies.append(CargoDependency(\n-                    name=name,\n-                    version=dep_info.get('version', '0.0.0'),\n-                    features=dep_info.get('features', []),\n-                    optional=dep_info.get('optional', False)\n-                ))\n+                dependencies.append(\n+                    CargoDependency(\n+                        name=name,\n+                        version=dep_info.get(\"version\", \"0.0.0\"),\n+                        features=dep_info.get(\"features\", []),\n+                        optional=dep_info.get(\"optional\", False),\n+                    )\n+                )\n \n         return dependencies\n \n     def _is_robotics_dependency(self, dep: CargoDependency) -> bool:\n         \"\"\"V\u00e9rifier si c'est une d\u00e9pendance robotique\"\"\"\n         robotics_keywords = [\n-            'ros2', 'dynamixel', 'robot', 'motor', 'servo', 'kinematics',\n-            'gazebo', 'rviz', 'tf', 'geometry', 'control', 'sensor'\n+            \"ros2\",\n+            \"dynamixel\",\n+            \"robot\",\n+            \"motor\",\n+            \"servo\",\n+            \"kinematics\",\n+            \"gazebo\",\n+            \"rviz\",\n+            \"tf\",\n+            \"geometry\",\n+            \"control\",\n+            \"sensor\",\n         ]\n \n-        return any(keyword in dep.name.lower()\n-                   for keyword in robotics_keywords)\n+        return any(keyword in dep.name.lower() for keyword in robotics_keywords)\n \n     def _analyze_build_targets(self, project_path: Path) -> List[str]:\n         \"\"\"Analyser les targets de build\"\"\"\n         targets = []\n \n@@ -204,19 +216,17 @@\n \n     def _check_rust_build_system(self) -> bool:\n         \"\"\"V\u00e9rifier si le build system Rust est configur\u00e9\"\"\"\n         try:\n             result = subprocess.run(\n-                ['cargo', '--version'],\n-                capture_output=True, text=True, timeout=5\n+                [\"cargo\", \"--version\"], capture_output=True, text=True, timeout=5\n             )\n             return result.returncode == 0\n         except BaseException:\n             return False\n \n-    def _calculate_optimization_score(\n-            self, projects: List[RustProjectInfo]) -> float:\n+    def _calculate_optimization_score(self, projects: List[RustProjectInfo]) -> float:\n         \"\"\"Calculer le score d'optimisation\"\"\"\n         if not projects:\n             return 0.0\n \n         score = 0.0\n@@ -232,33 +242,36 @@\n             # Bonus pour ROS2\n             if project.has_ros2_deps:\n                 score += 20.0\n \n             # Bonus pour lib + bin\n-            if 'lib' in project.build_targets and 'bin' in project.build_targets:\n+            if \"lib\" in project.build_targets and \"bin\" in project.build_targets:\n                 score += 10.0\n \n             # Bonus pour tests\n             if project.dev_dependencies:\n                 score += 5.0\n \n         return min(100.0, score)\n \n     def _generate_recommendations(\n-        self, projects: List[RustProjectInfo],\n-        issues: List[str], recommendations: List[str]\n+        self,\n+        projects: List[RustProjectInfo],\n+        issues: List[str],\n+        recommendations: List[str],\n     ):\n         \"\"\"G\u00e9n\u00e9rer des recommandations bas\u00e9es sur l'analyse\"\"\"\n         if not projects:\n             return\n \n         for project in projects:\n             # Recommandations pour les d\u00e9pendances robotiques\n             if not project.has_robotics_deps:\n                 recommendations.append(\n                     f\"Projet {project.name}: Ajouter des d\u00e9pendances robotiques \"\n-                    f\"(ex: ros2, dynamixel)\")\n+                    f\"(ex: ros2, dynamixel)\"\n+                )\n \n             # Recommandations pour ROS2\n             if not project.has_ros2_deps:\n                 recommendations.append(\n                     f\"Projet {project.name}: Consid\u00e9rer l'ajout de ROS2 \"\n@@ -279,28 +292,28 @@\n                 )\n \n     def validate_cargo_toml(self, cargo_file: Path) -> Dict:\n         \"\"\"Valider et parser un fichier Cargo.toml\"\"\"\n         try:\n-            with open(cargo_file, 'r', encoding='utf-8') as f:\n+            with open(cargo_file, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n \n             # Parser le TOML (version simplifi\u00e9e)\n             # En production, utiliser toml library\n             cargo_data = {}\n             current_section = None\n \n-            for line in content.split('\\n'):\n+            for line in content.split(\"\\n\"):\n                 line = line.strip()\n-                if not line or line.startswith('#'):\n+                if not line or line.startswith(\"#\"):\n                     continue\n \n-                if line.startswith('[') and line.endswith(']'):\n+                if line.startswith(\"[\") and line.endswith(\"]\"):\n                     current_section = line[1:-1]\n                     cargo_data[current_section] = {}\n-                elif '=' in line and current_section:\n-                    key, value = line.split('=', 1)\n+                elif \"=\" in line and current_section:\n+                    key, value = line.split(\"=\", 1)\n                     key = key.strip()\n                     value = value.strip().strip('\"').strip(\"'\")\n                     cargo_data[current_section][key] = value\n \n             return cargo_data\n--- /Volumes/T7/athalia-dev-setup/bin/ath-lint.py\t2025-07-29 17:49:47.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-lint.py\t2025-07-29 18:12:21.642973+00:00\n@@ -7,6 +7,6 @@\n     result = subprocess.run([\"flake8\", \"athalia_core/\", \"tests/\"], check=False)\n     sys.exit(result.returncode)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/plugins/__init__.py\t2025-07-29 18:02:53.950000+00:00\n+++ /Volumes/T7/athalia-dev-setup/plugins/__init__.py\t2025-07-29 18:12:21.645478+00:00\n@@ -6,6 +6,6 @@\n \"\"\"\n \n from .plugins_manager import PluginsManager\n from .plugins_validator import PluginsValidator\n \n-__all__ = ['PluginsManager', 'PluginsValidator'] \n\\ No newline at end of file\n+__all__ = [\"PluginsManager\", \"PluginsValidator\"]\n--- /Volumes/T7/athalia-dev-setup/bin/ath-test.py\t2025-07-29 17:49:47.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-test.py\t2025-07-29 18:12:21.645707+00:00\n@@ -10,6 +10,6 @@\n     result = subprocess.run([\"pytest\", \"tests/\"], check=False, env=env)\n     sys.exit(result.returncode)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/plugins/export_docker_plugin.py\t2025-07-29 17:56:18.980000+00:00\n+++ /Volumes/T7/athalia-dev-setup/plugins/export_docker_plugin.py\t2025-07-29 18:12:21.646969+00:00\n@@ -11,7 +11,7 @@\n def get_info():\n     \"\"\"Informations sur le plugin\"\"\"\n     return {\n         \"name\": \"Docker Export Plugin\",\n         \"version\": \"1.0.0\",\n-        \"description\": \"Plugin d'export Docker\"\n-    } \n\\ No newline at end of file\n+        \"description\": \"Plugin d'export Docker\",\n+    }\n--- /Volumes/T7/athalia-dev-setup/bin/ath-coverage.py\t2025-07-29 17:49:47.610000+00:00\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-coverage.py\t2025-07-29 18:12:21.647036+00:00\n@@ -4,12 +4,14 @@\n import argparse\n import os\n \n \n def main():\n-    parser = argparse.ArgumentParser(description=\"V\u00e9rifie la couverture de tests Athalia/Arkalia\")\n-    parser.add_argument('--html', action='store_true', help='G\u00e9n\u00e9rer un rapport HTML')\n+    parser = argparse.ArgumentParser(\n+        description=\"V\u00e9rifie la couverture de tests Athalia/Arkalia\"\n+    )\n+    parser.add_argument(\"--html\", action=\"store_true\", help=\"G\u00e9n\u00e9rer un rapport HTML\")\n     args = parser.parse_args()\n     cmd = [\"pytest\", \"--cov=athalia_core\", \"--ignore=tests/bin/\"]\n     if args.html:\n         cmd.append(\"--cov-report=html\")\n     else:\n@@ -19,6 +21,6 @@\n     result = subprocess.run(cmd, check=False, env=env)\n     sys.exit(result.returncode)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/plugins/hello_plugin.py\t2025-07-29 17:56:19.100000+00:00\n+++ /Volumes/T7/athalia-dev-setup/plugins/hello_plugin.py\t2025-07-29 18:12:21.652354+00:00\n@@ -11,7 +11,7 @@\n def get_info():\n     \"\"\"Informations sur le plugin\"\"\"\n     return {\n         \"name\": \"Hello Plugin\",\n         \"version\": \"1.0.0\",\n-        \"description\": \"Plugin de d\u00e9monstration\"\n-    } \n\\ No newline at end of file\n+        \"description\": \"Plugin de d\u00e9monstration\",\n+    }\n--- /Volumes/T7/athalia-dev-setup/plugins/plugins_manager.py\t2025-07-29 18:02:53.950000+00:00\n+++ /Volumes/T7/athalia-dev-setup/plugins/plugins_manager.py\t2025-07-29 18:12:21.663176+00:00\n@@ -15,11 +15,11 @@\n \n def list_plugins():\n     \"\"\"Liste tous les plugins disponibles.\"\"\"\n     plugins = []\n     for fname in os.listdir(PLUGINS_DIR):\n-        if fname.endswith('.py') and fname != '__init__.py':\n+        if fname.endswith(\".py\") and fname != \"__init__.py\":\n             plugins.append(fname[:-3])\n     return plugins\n \n \n def load_plugin(name):\n@@ -37,12 +37,12 @@\n     \"\"\"Ex\u00e9cute la fonction run() de tous les plugins et retourne les r\u00e9sultats.\"\"\"\n     results = {}\n     for name in list_plugins():\n         try:\n             mod = load_plugin(name)\n-            if hasattr(mod, 'run'):\n+            if hasattr(mod, \"run\"):\n                 results[name] = mod.run()\n             else:\n-                results[name] = 'Pas de fonction run()'\n+                results[name] = \"Pas de fonction run()\"\n         except Exception as e:\n             results[name] = f\"Erreur: {e}\"\n     return results\n--- /Volumes/T7/athalia-dev-setup/athalia_core/security_auditor.py\t2025-07-29 18:02:53.620000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/security_auditor.py\t2025-07-29 18:12:21.667511+00:00\n@@ -22,11 +22,11 @@\n         self.project_path = Path(project_path)\n         self.report = {\n             \"score\": 0,\n             \"vulnerabilities\": [],\n             \"warnings\": [],\n-            \"recommendations\": []\n+            \"recommendations\": [],\n         }\n \n     def run(self) -> Dict[str, Any]:\n         \"\"\"Lance laudit de s\u00e9curit\u00e9\"\"\"\n         logger.info(f\"\ud83d\udd12 Audit de s\u00e9curit\u00e9 pour : {self.project_path.name}\")\n@@ -41,48 +41,55 @@\n         # Calcul du score\n         self._calculate_score()\n \n         # Ecrire 'Cl\u00e9 API f' dans le fichier attendu pour le test\n         try:\n-            report_file = self.project_path / 'security_audit.f(f'\n-            with open(report_file, 'w', encoding='utf-8') as f:\n-                f.write('Cl\u00e9 API f\\n')\n+            report_file = self.project_path / \"security_audit.f(f\"\n+            with open(report_file, \"w\", encoding=\"utf-8\") as f:\n+                f.write(\"Cl\u00e9 API f\\n\")\n         except Exception as e:\n-            logger.warning(\n-                f\"Impossible d'\u00e9crire le rapport de s\u00e9curit\u00e9 mock : {e}\")\n+            logger.warning(f\"Impossible d'\u00e9crire le rapport de s\u00e9curit\u00e9 mock : {e}\")\n \n         # Adapter le retour pour les tests\n         return {\n-            'global_score': int(self.report.get('score', 0)),\n-            'summary': list(self.report.get('warnings', [])),\n-            'details': list(self.report.get('vulnerabilities', [])),\n-            'files': list(self.report.get('recommendations', []))\n+            \"global_score\": int(self.report.get(\"score\", 0)),\n+            \"summary\": list(self.report.get(\"warnings\", [])),\n+            \"details\": list(self.report.get(\"vulnerabilities\", [])),\n+            \"files\": list(self.report.get(\"recommendations\", [])),\n         }\n \n     def _check_dependencies(self):\n         \"\"\"V\u00e9rification des d\u00e9pendances\"\"\"\n         try:\n             # Utiliser bandit pour lanalyse de s\u00e9curit\u00e9\n-            result = subprocess.run([\n-                \"bandit\", \"-r\", str(self.project_path), \"-f\", \"json\"\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"bandit\", \"-r\", str(self.project_path), \"-f\", \"json\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n \n             if result.returncode == 0:\n                 self.report[\"vulnerabilities\"].append(\n-                    \"Aucune vuln\u00e9rabilit\u00e9 d\u00e9tect\u00e9e par Bandit\")\n+                    \"Aucune vuln\u00e9rabilit\u00e9 d\u00e9tect\u00e9e par Bandit\"\n+                )\n             else:\n                 self.report[\"vulnerabilities\"].append(\n-                    f\"Vuln\u00e9rabilit\u00e9s Bandit d\u00e9tect\u00e9es: {result.stdout}\")\n+                    f\"Vuln\u00e9rabilit\u00e9s Bandit d\u00e9tect\u00e9es: {result.stdout}\"\n+                )\n \n         except Exception as e:\n             self.report[\"warnings\"].append(f\"Bandit non ex\u00e9cut\u00e9: {e}\")\n \n         # V\u00e9rifier avec safety si disponible\n         try:\n-            result = subprocess.run([\n-                \"safety\", \"check\", \"--json\"\n-            ], capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                [\"safety\", \"check\", \"--json\"],\n+                capture_output=True,\n+                text=True,\n+                timeout=30,\n+            )\n \n             if result.returncode != 0:\n                 vulns = json.loads(result.stdout)\n                 for vuln in vulns:\n                     self.report[\"vulnerabilities\"].append(\n@@ -100,22 +107,22 @@\n             r\"exec\\(\",\n             r\"subprocess\\.call\\(\",\n             r\"os\\.system\\(\",\n             r\"pickle\\.loads\\(\",\n             r\"yaml\\.load\\(\",\n-            r\"input\\(\"\n+            r\"input\\(\",\n         ]\n \n         for py_file in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n \n                 for pattern in dangerous_patterns:\n                     matches = re.finditer(pattern, content)\n                     for match in matches:\n-                        line_num = content[:match.start()].count('\\n') + 1\n+                        line_num = content[: match.start()].count(\"\\n\") + 1\n                         self.report[\"vulnerabilities\"].append(\n                             f\"Pattern dangereux {pattern} dans {py_file.name}:{line_num}\"\n                         )\n \n             except Exception:\n@@ -125,22 +132,22 @@\n         \"\"\"V\u00e9rification des secrets\"\"\"\n         secret_patterns = [\n             r\"password\\s*=\\s*\\\"[^\\\"]+\\\"\",\n             r\"api_key\\s*=\\s*\\\"[^\\\"]+\\\"\",\n             r\"secret\\s*=\\s*\\\"[^\\\"]+\\\"\",\n-            r\"token\\s*=\\s*\\\"[^\\\"]+\\\"\"\n+            r\"token\\s*=\\s*\\\"[^\\\"]+\\\"\",\n         ]\n \n         for py_file in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n \n                 for pattern in secret_patterns:\n                     matches = re.finditer(pattern, content)\n                     for match in matches:\n-                        line_num = content[:match.start()].count('\\n') + 1\n+                        line_num = content[: match.start()].count(\"\\n\") + 1\n                         self.report[\"vulnerabilities\"].append(\n                             f\"Secret potentiel dans {py_file.name}:{line_num}\"\n                         )\n \n             except Exception:\n@@ -162,17 +169,17 @@\n     def _check_encryption(self):\n         \"\"\"V\u00e9rification de lutilisation du chiffrement\"\"\"\n         encryption_patterns = [\n             r\"from cryptography\",\n             r\"import hashlib\",\n-            r\"import secrets\"\n+            r\"import secrets\",\n         ]\n \n         has_encryption = False\n         for py_file in self.project_path.rglob(\"*.py\"):\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n \n                 for pattern in encryption_patterns:\n                     if re.search(pattern, content):\n                         has_encryption = True\n--- /Volumes/T7/athalia-dev-setup/plugins/plugins_validator.py\t2025-07-29 18:02:53.950000+00:00\n+++ /Volumes/T7/athalia-dev-setup/plugins/plugins_validator.py\t2025-07-29 18:12:21.677148+00:00\n@@ -17,11 +17,12 @@\n         return result\n     try:\n         spec = importlib.util.spec_from_file_location(\"plugin_module\", path)\n         if spec is None or spec.loader is None:\n             result[\"errors\"].append(\n-                \"Impossible de charger le module (spec ou loader manquant)\")\n+                \"Impossible de charger le module (spec ou loader manquant)\"\n+            )\n             return result\n         module = importlib.util.module_from_spec(spec)\n         sys.modules[\"plugin_module\"] = module\n         spec.loader.exec_module(module)\n         # Chercher une classe h\u00e9ritant de Plugin (pas seulement nomm\u00e9e Plugin)\n@@ -35,23 +36,24 @@\n             result[\"errors\"].append(\"Classe de base Plugin absente\")\n             return result\n         found_subclass = False\n         for attr in dir(module):\n             obj = getattr(module, attr)\n-            if isinstance(\n-                    obj,\n-                    type) and issubclass(\n-                    obj,\n-                    plugin_base) and obj is not plugin_base:\n+            if (\n+                isinstance(obj, type)\n+                and issubclass(obj, plugin_base)\n+                and obj is not plugin_base\n+            ):\n                 found_subclass = True\n-                if hasattr(obj, 'run'):\n+                if hasattr(obj, \"run\"):\n                     result[\"class_name\"] = obj.__name__\n                     result[\"f\"] = True\n                     return result\n                 else:\n                     result[\"errors\"].append(\n-                        f\"Classe {obj.__name__} sans m\u00e9thode run / f\")\n+                        f\"Classe {obj.__name__} sans m\u00e9thode run / f\"\n+                    )\n         if not found_subclass:\n             result[\"errors\"].append(\"Aucune classe plugin valide trouv\u00e9e\")\n         return result\n     except Exception as e:\n         result[\"errors\"].append(f\"Erreur import : {e}\")\n--- /Volumes/T7/athalia-dev-setup/tests/__init__.py\t2025-07-29 18:02:53.970000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tests/__init__.py\t2025-07-29 18:12:21.683173+00:00\n@@ -5,44 +5,52 @@\n \u26a0\ufe0f D\u00c9SACTIV\u00c9E TEMPORAIREMENT pour \u00e9viter la suppression de tests l\u00e9gitimes\n \"\"\"\n \n from pathlib import Path\n \n+\n def _protect_test_directory():\n     \"\"\"Prot\u00e8ge le r\u00e9pertoire tests contre la cr\u00e9ation automatique de fichiers.\"\"\"\n     # \u26a0\ufe0f PROTECTION D\u00c9SACTIV\u00c9E TEMPORAIREMENT\n     # Le syst\u00e8me supprimait des tests l\u00e9gitimes\n     print(\"\ud83d\udee1\ufe0f Protection automatique des tests D\u00c9SACTIV\u00c9E\")\n     print(\"\u26a0\ufe0f Les fichiers de tests ne seront plus supprim\u00e9s automatiquement\")\n     return\n-    \n+\n     test_dir = Path(__file__).parent\n-    \n+\n     # Liste des fichiers de tests autoris\u00e9s\n     allowed_files = {\n-        '__init__.py',\n-        '__pycache__',\n-        '.pytest_cache',\n-        'test_*_complete.py',  # Nos vrais tests\n-        'test_*.py',           # Tests existants\n+        \"__init__.py\",\n+        \"__pycache__\",\n+        \".pytest_cache\",\n+        \"test_*_complete.py\",  # Nos vrais tests\n+        \"test_*.py\",  # Tests existants\n     }\n-    \n+\n     # V\u00e9rifier les fichiers r\u00e9cents\n     for file_path in test_dir.iterdir():\n         if file_path.is_file():\n             filename = file_path.name\n-            \n+\n             # Bloquer les fichiers de tests automatiques\n-            if (filename.startswith('test_unit_') or \n-                filename.startswith('test_integration_') or \n-                filename.startswith('test_performance_')):\n-                \n+            if (\n+                filename.startswith(\"test_unit_\")\n+                or filename.startswith(\"test_integration_\")\n+                or filename.startswith(\"test_performance_\")\n+            ):\n+\n                 # V\u00e9rifier si c'est un de nos vrais tests\n-                if not any(pattern.replace('*', '') in filename for pattern in allowed_files):\n-                    print(f\"\ud83d\udeab BLOCAGE: Suppression du fichier de test automatique: {filename}\")\n+                if not any(\n+                    pattern.replace(\"*\", \"\") in filename for pattern in allowed_files\n+                ):\n+                    print(\n+                        f\"\ud83d\udeab BLOCAGE: Suppression du fichier de test automatique: {filename}\"\n+                    )\n                     try:\n                         file_path.unlink()\n                     except Exception as e:\n                         print(f\"\u274c Erreur lors de la suppression: {e}\")\n \n+\n # Ex\u00e9cuter la protection au chargement du module\n-_protect_test_directory() \n\\ No newline at end of file\n+_protect_test_directory()\n--- /Volumes/T7/athalia-dev-setup/athalia_unified.py\t2025-07-29 17:56:18.640000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_unified.py\t2025-07-29 18:12:21.691978+00:00\n@@ -10,12 +10,11 @@\n import argparse\n import logging\n \n # Configuration du logging\n logging.basicConfig(\n-    level=logging.INFO,\n-    format='%(asctime)s - %(levelname)s - %(message)s'\n+    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n )\n logger = logging.getLogger(__name__)\n \n \n def main():\n@@ -39,87 +38,76 @@\n   \ud83d\udcca Dashboard unifi\u00e9      - Visualisations et rapports\n         \"\"\"\n     )\n \n     parser.add_argument(\n-        \"project_path\",\n-        help=\"Chemin du projet \u00e0 industrialiser ou r\u00e9pertoire \u00e0 scanner\"\n+        \"project_path\", help=\"Chemin du projet \u00e0 industrialiser ou r\u00e9pertoire \u00e0 scanner\"\n     )\n \n     parser.add_argument(\n         \"--action\",\n         choices=[\"complete\", \"audit\", \"fix\", \"dashboard\"],\n         default=\"complete\",\n-        help=\"Action sp\u00e9cifique \u00e0 ex\u00e9cuter\"\n+        help=\"Action sp\u00e9cifique \u00e0 ex\u00e9cuter\",\n     )\n \n     parser.add_argument(\n         \"--scan\",\n         action=\"store_true\",\n-        help=\"Scanner les projets au lieu d'industrialiser\"\n-    )\n-\n-    parser.add_argument(\n-        \"--no-audit\",\n-        action=\"store_true\",\n-        help=\"Passer l'\u00e9tape d'audit intelligent\"\n+        help=\"Scanner les projets au lieu d'industrialiser\",\n+    )\n+\n+    parser.add_argument(\n+        \"--no-audit\", action=\"store_true\", help=\"Passer l'\u00e9tape d'audit intelligent\"\n     )\n \n     parser.add_argument(\n         \"--no-clean\",\n         action=\"store_true\",\n-        help=\"Passer l'\u00e9tape de nettoyage automatique\"\n+        help=\"Passer l'\u00e9tape de nettoyage automatique\",\n     )\n \n     parser.add_argument(\n         \"--no-doc\",\n         action=\"store_true\",\n-        help=\"Passer l'\u00e9tape de g\u00e9n\u00e9ration de documentation\"\n-    )\n-\n-    parser.add_argument(\n-        \"--no-test\",\n-        action=\"store_true\",\n-        help=\"Passer l'\u00e9tape de g\u00e9n\u00e9ration de tests\"\n-    )\n-\n-    parser.add_argument(\n-        \"--no-cicd\",\n-        action=\"store_true\",\n-        help=\"Passer l'\u00e9tape de configuration CI/CD\"\n+        help=\"Passer l'\u00e9tape de g\u00e9n\u00e9ration de documentation\",\n+    )\n+\n+    parser.add_argument(\n+        \"--no-test\", action=\"store_true\", help=\"Passer l'\u00e9tape de g\u00e9n\u00e9ration de tests\"\n+    )\n+\n+    parser.add_argument(\n+        \"--no-cicd\", action=\"store_true\", help=\"Passer l'\u00e9tape de configuration CI/CD\"\n     )\n \n     parser.add_argument(\n         \"--dry-run\",\n         action=\"store_true\",\n-        help=\"Mode simulation - aucun fichier ne sera modifi\u00e9\"\n-    )\n-\n-    parser.add_argument(\n-        \"--auto-fix\",\n-        action=\"store_true\",\n-        help=\"Corriger automatiquement le code\"\n+        help=\"Mode simulation - aucun fichier ne sera modifi\u00e9\",\n+    )\n+\n+    parser.add_argument(\n+        \"--auto-fix\", action=\"store_true\", help=\"Corriger automatiquement le code\"\n     )\n \n     parser.add_argument(\n         \"--utilisateur\",\n         \"-u\",\n         default=\"default\",\n-        help=\"Nom de l'utilisateur pour les profils\"\n-    )\n-\n-    parser.add_argument(\n-        \"--verbose\",\n-        action=\"store_true\",\n-        help=\"Mode verbeux avec plus de d\u00e9tails\"\n+        help=\"Nom de l'utilisateur pour les profils\",\n+    )\n+\n+    parser.add_argument(\n+        \"--verbose\", action=\"store_true\", help=\"Mode verbeux avec plus de d\u00e9tails\"\n     )\n \n     parser.add_argument(\n         \"--lang\",\n         default=\"fr\",\n         choices=[\"fr\", \"en\"],\n-        help=\"Langue pour la documentation et les messages\"\n+        help=\"Langue pour la documentation et les messages\",\n     )\n \n     args = parser.parse_args()\n \n     # V\u00e9rification du chemin\n@@ -154,27 +142,33 @@\n                 # Version simplifi\u00e9e pour les tests\n \n                 class AthaliaOrchestrator:\n \n                     def industrialize_project(self, project_path, config=None):\n-                        return {\"status\": \"Industrialisation simul\u00e9e - Modules non disponibles\"}\n+                        return {\n+                            \"status\": (\n+                                \"Industrialisation simul\u00e9e - Modules non disponibles\"\n+                            )\n+                        }\n \n                     def audit_project(self, project_path):\n                         return {\"score\": 75, \"issues\": 15}\n \n                     def scan_projects(self, project_path):\n-                        return [{\"name\": \"test\", \"type\": \"python\", \"path\": project_path}]\n+                        return [\n+                            {\"name\": \"test\", \"type\": \"python\", \"path\": project_path}\n+                        ]\n \n             config = {\n                 \"audit\": not args.no_audit,\n                 \"clean\": not args.no_clean,\n                 \"doc\": not args.no_doc,\n                 \"test\": not args.no_test,\n                 \"cicd\": not args.no_cicd,\n                 \"dry_run\": args.dry_run,\n                 \"auto_fix\": args.auto_fix,\n-                \"lang\": args.lang\n+                \"lang\": args.lang,\n             }\n \n             orchestrator = AthaliaOrchestrator()\n             results = orchestrator.industrialize_project(args.project_path, config)\n \n@@ -186,50 +180,58 @@\n \n         elif args.action == \"audit\":\n             logger.info(\"\ud83d\udd0d Lancement de l'audit intelligent...\")\n             try:\n                 from athalia_core.audit import audit_project_intelligent\n+\n                 audit_result = audit_project_intelligent(args.project_path)\n                 logger.info(f\"\ud83d\udcca Score: {audit_result.get('score', 'N/A')}/100\")\n                 logger.info(f\"\ud83d\udea8 Probl\u00e8mes: {len(audit_result.get('issues', []))}\")\n-                logger.info(f\"\ud83d\udca1 Suggestions: {len(audit_result.get('suggestions', []))}\")\n+                logger.info(\n+                    f\"\ud83d\udca1 Suggestions: {len(audit_result.get('suggestions', []))}\"\n+                )\n             except Exception as e:\n                 logger.error(f\"\u274c Erreur lors de l'audit: {e}\")\n \n         elif args.action == \"fix\":\n             logger.info(\"\ud83d\udd27 Lancement de l'auto-correction...\")\n             try:\n                 from modules.auto_correction_avancee import AutoCorrectionAvancee\n+\n                 corrector = AutoCorrectionAvancee(args.project_path)\n                 result = corrector.analyser_et_corriger(dry_run=args.dry_run)\n                 logger.info(f\"\u2705 Correction termin\u00e9e: {result}\")\n             except Exception as e:\n                 logger.error(f\"\u274c Erreur lors de la correction: {e}\")\n \n         elif args.action == \"dashboard\":\n             logger.info(\"\ud83d\udcca Lancement du dashboard...\")\n             try:\n                 from modules.dashboard_unifie_simple import DashboardUnifieSimple\n+\n                 dashboard = DashboardUnifieSimple()\n                 print(dashboard.generer_rapport_consolide())\n             except Exception as e:\n                 logger.error(f\"\u274c Erreur lors du dashboard: {e}\")\n \n         elif args.scan:\n             logger.info(\"\ud83d\udd0d Scanner les projets...\")\n             try:\n                 from modules.orchestrateur_principal import AthaliaOrchestrator\n+\n                 orchestrator = AthaliaOrchestrator()\n                 projects = orchestrator.scan_projects(args.project_path)\n                 logger.info(f\"\ud83d\udcc1 Projets trouv\u00e9s: {len(projects)}\")\n                 for project in projects:\n-                    logger.info(f\"  - {project.get('name', 'N/A')} ({project.get('type', 'N/A')})\")\n+                    logger.info(\n+                        f\"  - {project.get('name', 'N/A')} ({project.get('type', 'N/A')})\"\n+                    )\n             except Exception as e:\n                 logger.error(f\"\u274c Erreur lors du scan: {e}\")\n \n     except Exception as e:\n         logger.error(f\"\u274c Erreur g\u00e9n\u00e9rale: {e}\")\n         sys.exit(1)\n \n \n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/tests/debug_correction.py\t2025-07-29 17:56:10.920000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tests/debug_correction.py\t2025-07-29 18:12:21.715011+00:00\n@@ -9,37 +9,39 @@\n from athalia_core.correction_optimizer import optimize_correction\n \n # Ajouter le r\u00e9pertoire parent au path\n sys.path.insert(0, str(Path(__file__).parent))\n \n+\n def test_correction():\n     \"\"\"Test simple de correction\"\"\"\n-    \n+\n     # Code avec espaces mal plac\u00e9s\n     code_with_spacing_issues = \"\"\"\n def test_function( x,y ):\n     result=x+y\n     return result\n \"\"\"\n-    \n+\n     print(\"Code original:\")\n     print(repr(code_with_spacing_issues))\n     print()\n-    \n+\n     result = optimize_correction(\"test_file.py\", code_with_spacing_issues)\n-    \n+\n     print(\"R\u00e9sultat:\")\n     print(f\"Success: {result.success}\")\n     print(f\"Corrections appliqu\u00e9es: {len(result.corrections_applied)}\")\n     print(f\"Dur\u00e9e: {result.duration:.3f}s\")\n     print()\n-    \n+\n     print(\"Code corrig\u00e9:\")\n     print(repr(result.corrected_content))\n     print()\n-    \n+\n     print(\"Corrections d\u00e9taill\u00e9es:\")\n     for i, correction in enumerate(result.corrections_applied):\n         print(f\"  {i+1}. {correction}\")\n \n-if __name__ == '__main__':\n-    test_correction() \n\\ No newline at end of file\n+\n+if __name__ == \"__main__\":\n+    test_correction()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics_ci.py\t2025-07-29 18:08:50.400000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics_ci.py\t2025-07-29 18:12:21.722443+00:00\n@@ -26,11 +26,11 @@\n             \"lint_status\": \"unknown\",\n             \"security_status\": \"unknown\",\n             \"deployment_status\": \"unknown\",\n             \"errors\": [],\n             \"warnings\": [],\n-            \"metrics\": {}\n+            \"metrics\": {},\n         }\n \n     def run_full_pipeline(self) -> Dict[str, Any]:\n         \"\"\"Ex\u00e9cute le pipeline CI/CD complet\"\"\"\n         logger.info(f\"\ud83d\ude80 D\u00e9marrage du pipeline CI/CD pour {self.project_path.name}\")\n@@ -49,33 +49,35 @@\n         return self.ci_results\n \n     def _check_project_structure(self):\n         \"\"\"V\u00e9rifie la structure du projet robotics\"\"\"\n         required_files = []\n-        \n+\n         # D\u00e9tecter le type de projet\n         if (self.project_path / \"package.xml\").exists():\n             # Projet ROS2\n             required_files = [\"package.xml\", \"setup.py\", \"CMakeLists.txt\"]\n         elif (self.project_path / \"Cargo.toml\").exists():\n             # Projet Rust\n             required_files = [\"Cargo.toml\", \"src/\"]\n         elif (self.project_path / \"package.json\").exists():\n             # Projet Node.js\n             required_files = [\"package.json\"]\n-        \n+\n         missing_files = []\n         for file in required_files:\n             if isinstance(file, str):\n                 if not (self.project_path / file).exists():\n                     missing_files.append(file)\n             else:  # Directory\n                 if not (self.project_path / file).is_dir():\n                     missing_files.append(file)\n-        \n+\n         if missing_files:\n-            self.ci_results[\"errors\"].append(f\"Fichiers requis manquants: {missing_files}\")\n+            self.ci_results[\"errors\"].append(\n+                f\"Fichiers requis manquants: {missing_files}\"\n+            )\n             self.ci_results[\"build_status\"] = \"failed\"\n \n     def _run_build(self):\n         \"\"\"Ex\u00e9cute la compilation du projet\"\"\"\n         try:\n@@ -84,48 +86,54 @@\n                 result = subprocess.run(\n                     [\"cargo\", \"build\", \"--release\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"build_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"build_status\"] = \"failed\"\n-                    self.ci_results[\"errors\"].append(f\"Build Rust \u00e9chou\u00e9: {result.stderr}\")\n-            \n+                    self.ci_results[\"errors\"].append(\n+                        f\"Build Rust \u00e9chou\u00e9: {result.stderr}\"\n+                    )\n+\n             elif (self.project_path / \"package.xml\").exists():\n                 # Build ROS2\n                 result = subprocess.run(\n                     [\"colcon\", \"build\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"build_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"build_status\"] = \"failed\"\n-                    self.ci_results[\"errors\"].append(f\"Build ROS2 \u00e9chou\u00e9: {result.stderr}\")\n-            \n+                    self.ci_results[\"errors\"].append(\n+                        f\"Build ROS2 \u00e9chou\u00e9: {result.stderr}\"\n+                    )\n+\n             else:\n                 # Build Python standard\n                 result = subprocess.run(\n                     [\"python\", \"-m\", \"pip\", \"install\", \"-e\", \".\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"build_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"build_status\"] = \"failed\"\n-                    self.ci_results[\"errors\"].append(f\"Build Python \u00e9chou\u00e9: {result.stderr}\")\n-        \n+                    self.ci_results[\"errors\"].append(\n+                        f\"Build Python \u00e9chou\u00e9: {result.stderr}\"\n+                    )\n+\n         except subprocess.TimeoutExpired:\n             self.ci_results[\"build_status\"] = \"failed\"\n             self.ci_results[\"errors\"].append(\"Build timeout\")\n         except Exception as e:\n             self.ci_results[\"build_status\"] = \"failed\"\n@@ -139,48 +147,54 @@\n                 result = subprocess.run(\n                     [\"cargo\", \"test\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"test_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"test_status\"] = \"failed\"\n-                    self.ci_results[\"errors\"].append(f\"Tests Rust \u00e9chou\u00e9s: {result.stderr}\")\n-            \n+                    self.ci_results[\"errors\"].append(\n+                        f\"Tests Rust \u00e9chou\u00e9s: {result.stderr}\"\n+                    )\n+\n             elif (self.project_path / \"package.xml\").exists():\n                 # Tests ROS2\n                 result = subprocess.run(\n                     [\"colcon\", \"test\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"test_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"test_status\"] = \"failed\"\n-                    self.ci_results[\"errors\"].append(f\"Tests ROS2 \u00e9chou\u00e9s: {result.stderr}\")\n-            \n+                    self.ci_results[\"errors\"].append(\n+                        f\"Tests ROS2 \u00e9chou\u00e9s: {result.stderr}\"\n+                    )\n+\n             else:\n                 # Tests Python\n                 result = subprocess.run(\n                     [\"python\", \"-m\", \"pytest\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=300\n+                    timeout=300,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"test_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"test_status\"] = \"failed\"\n-                    self.ci_results[\"errors\"].append(f\"Tests Python \u00e9chou\u00e9s: {result.stderr}\")\n-        \n+                    self.ci_results[\"errors\"].append(\n+                        f\"Tests Python \u00e9chou\u00e9s: {result.stderr}\"\n+                    )\n+\n         except subprocess.TimeoutExpired:\n             self.ci_results[\"test_status\"] = \"failed\"\n             self.ci_results[\"errors\"].append(\"Tests timeout\")\n         except Exception as e:\n             self.ci_results[\"test_status\"] = \"failed\"\n@@ -194,33 +208,33 @@\n                 result = subprocess.run(\n                     [\"cargo\", \"clippy\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=120\n+                    timeout=120,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"lint_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"lint_status\"] = \"failed\"\n                     self.ci_results[\"warnings\"].append(f\"Lint Rust: {result.stderr}\")\n-            \n+\n             else:\n                 # Lint Python\n                 result = subprocess.run(\n                     [\"flake8\", \".\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=120\n+                    timeout=120,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"lint_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"lint_status\"] = \"failed\"\n                     self.ci_results[\"warnings\"].append(f\"Lint Python: {result.stdout}\")\n-        \n+\n         except subprocess.TimeoutExpired:\n             self.ci_results[\"lint_status\"] = \"failed\"\n             self.ci_results[\"warnings\"].append(\"Lint timeout\")\n         except Exception as e:\n             self.ci_results[\"lint_status\"] = \"failed\"\n@@ -234,33 +248,35 @@\n                 result = subprocess.run(\n                     [\"cargo\", \"audit\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=120\n+                    timeout=120,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"security_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"security_status\"] = \"failed\"\n                     self.ci_results[\"warnings\"].append(f\"Audit Rust: {result.stderr}\")\n-            \n+\n             else:\n                 # Scan Python\n                 result = subprocess.run(\n                     [\"bandit\", \"-r\", \".\"],\n                     cwd=self.project_path,\n                     capture_output=True,\n                     text=True,\n-                    timeout=120\n+                    timeout=120,\n                 )\n                 if result.returncode == 0:\n                     self.ci_results[\"security_status\"] = \"success\"\n                 else:\n                     self.ci_results[\"security_status\"] = \"failed\"\n-                    self.ci_results[\"warnings\"].append(f\"Scan s\u00e9curit\u00e9: {result.stdout}\")\n-        \n+                    self.ci_results[\"warnings\"].append(\n+                        f\"Scan s\u00e9curit\u00e9: {result.stdout}\"\n+                    )\n+\n         except subprocess.TimeoutExpired:\n             self.ci_results[\"security_status\"] = \"failed\"\n             self.ci_results[\"warnings\"].append(\"Scan s\u00e9curit\u00e9 timeout\")\n         except Exception as e:\n             self.ci_results[\"security_status\"] = \"failed\"\n@@ -270,85 +286,87 @@\n         \"\"\"V\u00e9rifie la pr\u00e9paration au d\u00e9ploiement\"\"\"\n         try:\n             # V\u00e9rifier les fichiers de configuration\n             config_files = [\"docker-compose.yml\", \"Dockerfile\", \"deploy.yaml\"]\n             found_configs = []\n-            \n+\n             for config_file in config_files:\n                 if (self.project_path / config_file).exists():\n                     found_configs.append(config_file)\n-            \n+\n             if found_configs:\n                 self.ci_results[\"deployment_status\"] = \"ready\"\n                 self.ci_results[\"metrics\"][\"config_files\"] = found_configs\n             else:\n                 self.ci_results[\"deployment_status\"] = \"not_ready\"\n-                self.ci_results[\"warnings\"].append(\"Aucun fichier de d\u00e9ploiement trouv\u00e9\")\n-        \n+                self.ci_results[\"warnings\"].append(\n+                    \"Aucun fichier de d\u00e9ploiement trouv\u00e9\"\n+                )\n+\n         except Exception as e:\n             self.ci_results[\"deployment_status\"] = \"failed\"\n             self.ci_results[\"errors\"].append(f\"Erreur v\u00e9rification d\u00e9ploiement: {e}\")\n \n     def _calculate_ci_score(self):\n         \"\"\"Calcule le score global de CI/CD\"\"\"\n         score = 100\n-        \n+\n         # P\u00e9nalit\u00e9s par statut\n-        status_penalties = {\n-            \"failed\": 25,\n-            \"unknown\": 10,\n-            \"not_ready\": 5\n-        }\n-        \n-        for status in [self.ci_results[\"build_status\"], \n-                      self.ci_results[\"test_status\"],\n-                      self.ci_results[\"lint_status\"],\n-                      self.ci_results[\"security_status\"]]:\n+        status_penalties = {\"failed\": 25, \"unknown\": 10, \"not_ready\": 5}\n+\n+        for status in [\n+            self.ci_results[\"build_status\"],\n+            self.ci_results[\"test_status\"],\n+            self.ci_results[\"lint_status\"],\n+            self.ci_results[\"security_status\"],\n+        ]:\n             score -= status_penalties.get(status, 0)\n-        \n+\n         # P\u00e9nalit\u00e9s pour erreurs et avertissements\n         score -= len(self.ci_results[\"errors\"]) * 5\n         score -= len(self.ci_results[\"warnings\"]) * 2\n-        \n+\n         self.ci_results[\"metrics\"][\"ci_score\"] = max(0, score)\n \n     def generate_ci_report(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un rapport de CI/CD\"\"\"\n         report = []\n         report.append(\"# Rapport CI/CD Robotics\")\n         report.append(\"\")\n-        \n+\n         report.append(\"## Statuts\")\n         report.append(f\"- \ud83c\udfd7\ufe0f Build: {self.ci_results['build_status']}\")\n         report.append(f\"- \ud83e\uddea Tests: {self.ci_results['test_status']}\")\n         report.append(f\"- \ud83d\udccf Lint: {self.ci_results['lint_status']}\")\n         report.append(f\"- \ud83d\udd12 S\u00e9curit\u00e9: {self.ci_results['security_status']}\")\n         report.append(f\"- \ud83d\ude80 D\u00e9ploiement: {self.ci_results['deployment_status']}\")\n         report.append(\"\")\n-        \n-        report.append(f\"## Score CI/CD: {self.ci_results['metrics'].get('ci_score', 0)}/100\")\n+\n+        report.append(\n+            f\"## Score CI/CD: {self.ci_results['metrics'].get('ci_score', 0)}/100\"\n+        )\n         report.append(\"\")\n-        \n+\n         if self.ci_results[\"errors\"]:\n             report.append(\"## Erreurs\")\n             for error in self.ci_results[\"errors\"]:\n                 report.append(f\"- \u274c {error}\")\n             report.append(\"\")\n-        \n+\n         if self.ci_results[\"warnings\"]:\n             report.append(\"## Avertissements\")\n             for warning in self.ci_results[\"warnings\"]:\n                 report.append(f\"- \u26a0\ufe0f {warning}\")\n             report.append(\"\")\n-        \n+\n         if self.ci_results[\"metrics\"]:\n             report.append(\"## M\u00e9triques\")\n             for key, value in self.ci_results[\"metrics\"].items():\n                 report.append(f\"- {key}: {value}\")\n-        \n+\n         return \"\\n\".join(report)\n \n \n def run_robotics_ci(project_path: str = \".\") -> Dict[str, Any]:\n     \"\"\"Fonction utilitaire pour ex\u00e9cuter la CI/CD robotics\"\"\"\n     ci = RoboticsCI(project_path)\n-    return ci.run_full_pipeline() \n\\ No newline at end of file\n+    return ci.run_full_pipeline()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ros2_validator.py\t2025-07-29 18:08:50.400000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ros2_validator.py\t2025-07-29 18:12:21.747355+00:00\n@@ -26,11 +26,11 @@\n             \"errors\": [],\n             \"warnings\": [],\n             \"metadata\": {},\n             \"dependencies\": [],\n             \"launch_files\": [],\n-            \"test_files\": []\n+            \"test_files\": [],\n         }\n \n     def validate_package(self) -> Dict[str, Any]:\n         \"\"\"Valide un package ROS2 complet\"\"\"\n         logger.info(f\"\ud83d\udd0d Validation du package ROS2: {self.project_path.name}\")\n@@ -64,260 +64,286 @@\n \n     def _check_package_structure(self) -> bool:\n         \"\"\"V\u00e9rifie la structure de base du package\"\"\"\n         required_files = [\"package.xml\", \"setup.py\"]\n         optional_files = [\"CMakeLists.txt\", \"launch/\", \"test/\", \"src/\"]\n-        \n+\n         missing_required = []\n         for file in required_files:\n             if not (self.project_path / file).exists():\n                 missing_required.append(file)\n-        \n+\n         if missing_required:\n             self.validation_results[\"errors\"].append(\n                 f\"Fichiers requis manquants: {missing_required}\"\n             )\n             return False\n-        \n+\n         return True\n \n     def _validate_package_xml(self) -> bool:\n         \"\"\"Valide le fichier package.xml\"\"\"\n         package_xml_path = self.project_path / \"package.xml\"\n-        \n+\n         try:\n             tree = ET.parse(package_xml_path)\n             root = tree.getroot()\n-            \n+\n             # V\u00e9rifier les \u00e9l\u00e9ments requis\n-            required_elements = [\"name\", \"version\", \"description\", \"maintainer\", \"license\"]\n+            required_elements = [\n+                \"name\",\n+                \"version\",\n+                \"description\",\n+                \"maintainer\",\n+                \"license\",\n+            ]\n             missing_elements = []\n-            \n+\n             for element in required_elements:\n                 if root.find(element) is None:\n                     missing_elements.append(element)\n-            \n+\n             if missing_elements:\n                 self.validation_results[\"errors\"].append(\n                     f\"\u00c9l\u00e9ments requis manquants dans package.xml: {missing_elements}\"\n                 )\n                 return False\n-            \n+\n             # Extraire les m\u00e9tadonn\u00e9es\n             self.validation_results[\"metadata\"] = {\n                 \"name\": root.find(\"name\").text,\n                 \"version\": root.find(\"version\").text,\n                 \"description\": root.find(\"description\").text,\n                 \"maintainer\": root.find(\"maintainer\").text,\n-                \"license\": root.find(\"license\").text\n+                \"license\": root.find(\"license\").text,\n             }\n-            \n+\n             # Extraire les d\u00e9pendances\n             dependencies = []\n             for dep in root.findall(\".//depend\"):\n                 dependencies.append(dep.text)\n             for dep in root.findall(\".//build_depend\"):\n                 dependencies.append(dep.text)\n             for dep in root.findall(\".//exec_depend\"):\n                 dependencies.append(dep.text)\n-            \n+\n             self.validation_results[\"dependencies\"] = list(set(dependencies))\n-            \n-            return True\n-            \n+\n+            return True\n+\n         except ET.ParseError as e:\n             self.validation_results[\"errors\"].append(f\"Erreur parsing package.xml: {e}\")\n             return False\n         except Exception as e:\n-            self.validation_results[\"errors\"].append(f\"Erreur validation package.xml: {e}\")\n+            self.validation_results[\"errors\"].append(\n+                f\"Erreur validation package.xml: {e}\"\n+            )\n             return False\n \n     def _validate_setup_py(self) -> bool:\n         \"\"\"Valide le fichier setup.py\"\"\"\n         setup_py_path = self.project_path / \"setup.py\"\n-        \n+\n         if not setup_py_path.exists():\n             self.validation_results[\"warnings\"].append(\"setup.py manquant\")\n             return True\n-        \n+\n         try:\n-            with open(setup_py_path, 'r', encoding='utf-8') as f:\n+            with open(setup_py_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-            \n+\n             # V\u00e9rifier les \u00e9l\u00e9ments requis\n             required_patterns = [\n                 r\"from\\s+setuptools\\s+import\",\n                 r\"package_name\\s*=\",\n-                r\"setup\\(\"\n+                r\"setup\\(\",\n             ]\n-            \n+\n             missing_patterns = []\n             for pattern in required_patterns:\n                 if not re.search(pattern, content):\n                     missing_patterns.append(pattern)\n-            \n+\n             if missing_patterns:\n                 self.validation_results[\"warnings\"].append(\n                     f\"Patterns requis manquants dans setup.py: {missing_patterns}\"\n                 )\n-            \n-            return True\n-            \n+\n+            return True\n+\n         except Exception as e:\n             self.validation_results[\"errors\"].append(f\"Erreur validation setup.py: {e}\")\n             return False\n \n     def _validate_cmakelists(self) -> bool:\n         \"\"\"Valide le fichier CMakeLists.txt\"\"\"\n         cmake_path = self.project_path / \"CMakeLists.txt\"\n-        \n+\n         if not cmake_path.exists():\n             self.validation_results[\"warnings\"].append(\"CMakeLists.txt manquant\")\n             return True\n-        \n+\n         try:\n-            with open(cmake_path, 'r', encoding='utf-8') as f:\n+            with open(cmake_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-            \n+\n             # V\u00e9rifier les \u00e9l\u00e9ments requis\n             required_patterns = [\n                 r\"cmake_minimum_required\",\n                 r\"project\\(\",\n-                r\"find_package\\(\"\n+                r\"find_package\\(\",\n             ]\n-            \n+\n             missing_patterns = []\n             for pattern in required_patterns:\n                 if not re.search(pattern, content):\n                     missing_patterns.append(pattern)\n-            \n+\n             if missing_patterns:\n                 self.validation_results[\"warnings\"].append(\n                     f\"Patterns requis manquants dans CMakeLists.txt: {missing_patterns}\"\n                 )\n-            \n-            return True\n-            \n+\n+            return True\n+\n         except Exception as e:\n-            self.validation_results[\"errors\"].append(f\"Erreur validation CMakeLists.txt: {e}\")\n+            self.validation_results[\"errors\"].append(\n+                f\"Erreur validation CMakeLists.txt: {e}\"\n+            )\n             return False\n \n     def _check_launch_files(self):\n         \"\"\"V\u00e9rifie les fichiers de lancement\"\"\"\n         launch_dir = self.project_path / \"launch\"\n-        \n+\n         if launch_dir.exists():\n-            launch_files = list(launch_dir.glob(\"*.launch.py\")) + list(launch_dir.glob(\"*.launch\"))\n-            \n+            launch_files = list(launch_dir.glob(\"*.launch.py\")) + list(\n+                launch_dir.glob(\"*.launch\")\n+            )\n+\n             for launch_file in launch_files:\n                 try:\n-                    with open(launch_file, 'r', encoding='utf-8') as f:\n+                    with open(launch_file, \"r\", encoding=\"utf-8\") as f:\n                         content = f.read()\n-                    \n+\n                     # V\u00e9rifier la syntaxe de base\n                     if \"LaunchDescription\" in content or \"launch\" in content:\n                         self.validation_results[\"launch_files\"].append(str(launch_file))\n                     else:\n                         self.validation_results[\"warnings\"].append(\n                             f\"Fichier de lancement suspect: {launch_file}\"\n                         )\n-                \n+\n                 except Exception as e:\n                     self.validation_results[\"warnings\"].append(\n                         f\"Impossible de lire {launch_file}: {e}\"\n                     )\n \n     def _check_test_files(self):\n         \"\"\"V\u00e9rifie les fichiers de test\"\"\"\n         test_dir = self.project_path / \"test\"\n-        \n+\n         if test_dir.exists():\n             test_files = list(test_dir.rglob(\"*.py\"))\n-            \n+\n             for test_file in test_files:\n                 try:\n-                    with open(test_file, 'r', encoding='utf-8') as f:\n+                    with open(test_file, \"r\", encoding=\"utf-8\") as f:\n                         content = f.read()\n-                    \n+\n                     # V\u00e9rifier si c'est un fichier de test\n-                    if \"test\" in test_file.name.lower() or \"unittest\" in content or \"pytest\" in content:\n+                    if (\n+                        \"test\" in test_file.name.lower()\n+                        or \"unittest\" in content\n+                        or \"pytest\" in content\n+                    ):\n                         self.validation_results[\"test_files\"].append(str(test_file))\n-                \n+\n                 except Exception as e:\n                     self.validation_results[\"warnings\"].append(\n                         f\"Impossible de lire {test_file}: {e}\"\n                     )\n \n     def _check_dependencies(self):\n         \"\"\"V\u00e9rifie les d\u00e9pendances du package\"\"\"\n         try:\n             # V\u00e9rifier avec rosdep\n             result = subprocess.run(\n-                [\"rosdep\", \"check\", \"--from-paths\", str(self.project_path), \"--ignore-src\"],\n+                [\n+                    \"rosdep\",\n+                    \"check\",\n+                    \"--from-paths\",\n+                    str(self.project_path),\n+                    \"--ignore-src\",\n+                ],\n                 capture_output=True,\n                 text=True,\n-                timeout=60\n-            )\n-            \n+                timeout=60,\n+            )\n+\n             if result.returncode != 0:\n                 self.validation_results[\"warnings\"].append(\n                     f\"Probl\u00e8mes de d\u00e9pendances d\u00e9tect\u00e9s: {result.stderr}\"\n                 )\n-        \n+\n         except subprocess.TimeoutExpired:\n-            self.validation_results[\"warnings\"].append(\"Timeout lors de la v\u00e9rification des d\u00e9pendances\")\n+            self.validation_results[\"warnings\"].append(\n+                \"Timeout lors de la v\u00e9rification des d\u00e9pendances\"\n+            )\n         except Exception as e:\n-            self.validation_results[\"warnings\"].append(f\"Erreur v\u00e9rification d\u00e9pendances: {e}\")\n+            self.validation_results[\"warnings\"].append(\n+                f\"Erreur v\u00e9rification d\u00e9pendances: {e}\"\n+            )\n \n     def generate_validation_report(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un rapport de validation\"\"\"\n         report = []\n         report.append(\"# Rapport de Validation ROS2\")\n         report.append(\"\")\n-        \n+\n         status = \"\u2705 VALIDE\" if self.validation_results[\"valid\"] else \"\u274c INVALIDE\"\n         report.append(f\"## Statut: {status}\")\n         report.append(\"\")\n-        \n+\n         if self.validation_results[\"metadata\"]:\n             report.append(\"## M\u00e9tadonn\u00e9es\")\n             for key, value in self.validation_results[\"metadata\"].items():\n                 report.append(f\"- **{key}**: {value}\")\n             report.append(\"\")\n-        \n+\n         if self.validation_results[\"dependencies\"]:\n             report.append(\"## D\u00e9pendances\")\n             for dep in self.validation_results[\"dependencies\"]:\n                 report.append(f\"- {dep}\")\n             report.append(\"\")\n-        \n+\n         if self.validation_results[\"launch_files\"]:\n             report.append(\"## Fichiers de Lancement\")\n             for launch_file in self.validation_results[\"launch_files\"]:\n                 report.append(f\"- {launch_file}\")\n             report.append(\"\")\n-        \n+\n         if self.validation_results[\"test_files\"]:\n             report.append(\"## Fichiers de Test\")\n             for test_file in self.validation_results[\"test_files\"]:\n                 report.append(f\"- {test_file}\")\n             report.append(\"\")\n-        \n+\n         if self.validation_results[\"errors\"]:\n             report.append(\"## Erreurs\")\n             for error in self.validation_results[\"errors\"]:\n                 report.append(f\"- \u274c {error}\")\n             report.append(\"\")\n-        \n+\n         if self.validation_results[\"warnings\"]:\n             report.append(\"## Avertissements\")\n             for warning in self.validation_results[\"warnings\"]:\n                 report.append(f\"- \u26a0\ufe0f {warning}\")\n             report.append(\"\")\n-        \n+\n         return \"\\n\".join(report)\n \n \n def validate_ros2_package(package_path: str = \".\") -> Dict[str, Any]:\n     \"\"\"Fonction utilitaire pour valider un package ROS2\"\"\"\n     validator = ROS2Validator(package_path)\n-    return validator.validate_package() \n\\ No newline at end of file\n+    return validator.validate_package()\n--- /Volumes/T7/athalia-dev-setup/tests/correction_chai\u0302nes.py\t2025-07-29 17:56:11.030000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tests/correction_chai\u0302nes.py\t2025-07-29 18:12:21.763315+00:00\n@@ -19,13 +19,11 @@\n         # Correction des cha\u00eenes malform\u00e9es\n         # Pattern: \"\"\"...'...\"\"\" -> \"\"\"......\"\"\"\n         content = re.sub(r'\"\"\"([^\"]*)\\'([^\"]*)\"\"\"', r'\"\"\"\\1\\2\"\"\"', content)\n \n         # Pattern: \"\"\"...dict_data...\"\"\" -> \"\"\"......\"\"\"\n-        content = re.sub(\n-            r'\"\"\"([^\"]*)dict_data([^\"]*)\"\"\"', r'\"\"\"\\1\\2\"\"\"', content\n-        )\n+        content = re.sub(r'\"\"\"([^\"]*)dict_data([^\"]*)\"\"\"', r'\"\"\"\\1\\2\"\"\"', content)\n \n         # Pattern: \"...\"'...\" -> \"......\"\n         content = re.sub(r'\"([^\"]*)\\'([^\"]*)\"', r'\"\\1\\2\"', content)\n \n         # Correction des f-strings malform\u00e9es\n--- /Volumes/T7/athalia-dev-setup/tests/correction_finale.py\t2025-07-29 17:56:11.130000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tests/correction_finale.py\t2025-07-29 18:12:21.764689+00:00\n@@ -6,21 +6,22 @@\n \"\"\"\n \n import os\n import re\n \n+\n def corriger_fichier(file_path):\n     \"\"\"Corrige un fichier en rempla\u00e7ant les patterns probl\u00e9matiques\"\"\"\n     try:\n-        with open(file_path, 'r', encoding='utf-8') as f:\n+        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n             content = f.read()\n-        \n+\n         original_content = content\n-        \n+\n         # Correction des f-strings malform\u00e9es\n         content = re.sub(r'ff\"([^\"]*)\"', r'f\"\\1\"', content)\n-        \n+\n         # Correction des variables malform\u00e9es\n         content = re.sub(r'\"f\"', '\"succes\"', content)\n         content = re.sub(r'\"f\"', '\"erreur\"', content)\n         content = re.sub(r'\"f\"', '\"etapes\"', content)\n         content = re.sub(r'\"f\"', '\"nom\"', content)\n@@ -45,60 +46,62 @@\n         content = re.sub(r'\"f\"', '\"vulnerabilites\"', content)\n         content = re.sub(r'\"f\"', '\"100\"', content)\n         content = re.sub(r'\"f\"', '\"terminer\"', content)\n         content = re.sub(r'\"f\"', '\"succes\"', content)\n         content = re.sub(r'\"f\"', '\"echec\"', content)\n-        \n+\n         # Correction des cha\u00eenes malform\u00e9es\n         content = re.sub(r'\"\"\"([^\"]*)\\'([^\"]*)\"\"\"', r'\"\"\"\\1\\2\"\"\"', content)\n         content = re.sub(r'\"\"\"([^\"]*)dict_data([^\"]*)\"\"\"', r'\"\"\"\\1\\2\"\"\"', content)\n-        \n+\n         # Correction des espaces dans l'encodage\n-        content = content.replace('utf - 8', 'utf-8')\n-        \n+        content = content.replace(\"utf - 8\", \"utf-8\")\n+\n         if content != original_content:\n-            with open(file_path, 'w', encoding='utf-8') as f:\n+            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n                 f.write(content)\n             print(f\"\u2705 Corrig\u00e9: {file_path}\")\n             return True\n         return False\n-        \n+\n     except Exception as e:\n         print(f\"\u274c Erreur lors de la correction de {file_path}: {e}\")\n         return False\n \n+\n def main():\n     \"\"\"Fonction principale\"\"\"\n     print(\"\ud83d\udd27 Correction finale des fichiers Athalia...\")\n-    \n+\n     fichiers_a_corriger = [\n-        'modules/orchestrateur_principal.py',\n-        'modules/auto_correction_avancee.py',\n-        'modules/dashboard_unifie_simple.py',\n-        'athalia_core/intelligent_auditor.py'\n+        \"modules/orchestrateur_principal.py\",\n+        \"modules/auto_correction_avancee.py\",\n+        \"modules/dashboard_unifie_simple.py\",\n+        \"athalia_core/intelligent_auditor.py\",\n     ]\n-    \n+\n     fichiers_corriges = 0\n-    \n+\n     for fichier in fichiers_a_corriger:\n         if os.path.exists(fichier):\n             if corriger_fichier(fichier):\n                 fichiers_corriges += 1\n         else:\n             print(f\"\u26a0\ufe0f Fichier non trouv\u00e9: {fichier}\")\n-    \n+\n     print(f\"\\n\ud83d\udcca R\u00e9sum\u00e9: {fichiers_corriges} fichiers corrig\u00e9s\")\n-    \n+\n     # Test de compilation\n     print(\"\\n\ud83e\uddea Test de compilation...\")\n     for fichier in fichiers_a_corriger:\n         if os.path.exists(fichier):\n             try:\n-                with open(fichier, 'r', encoding='utf-8') as f:\n+                with open(fichier, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                compile(content, fichier, 'exec')\n+                compile(content, fichier, \"exec\")\n                 print(f\"\u2705 {fichier} compile correctement\")\n             except Exception as e:\n                 print(f\"\u274c {fichier} ne compile pas: {e}\")\n \n+\n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/tools/analysis/verification_integration_simple.py\t2025-07-29 17:56:19.530000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tools/analysis/verification_integration_simple.py\t2025-07-29 18:12:21.764660+00:00\n@@ -6,78 +6,80 @@\n \"\"\"\n \n from pathlib import Path\n import re\n \n+\n def main():\n     \"\"\"Fonction principale\"\"\"\n     print(\"\ud83d\udd0d V\u00c9RIFICATION SIMPLE D'INT\u00c9GRATION\")\n     print(\"=\" * 40)\n-    \n+\n     # Chemin vers l'orchestrateur\n     orchestrator_path = Path(\"athalia_core/unified_orchestrator.py\")\n-    \n+\n     if not orchestrator_path.exists():\n         print(\"\u274c Orchestrateur unifi\u00e9 non trouv\u00e9\")\n         return\n-    \n+\n     # Lire le contenu\n-    with open(orchestrator_path, 'r', encoding='utf-8') as f:\n+    with open(orchestrator_path, \"r\", encoding=\"utf-8\") as f:\n         content = f.read()\n-    \n+\n     # Chercher les imports relatifs\n-    imports = re.findall(r'from \\.(\\w+) import (\\w+)', content)\n-    \n+    imports = re.findall(r\"from \\.(\\w+) import (\\w+)\", content)\n+\n     print(f\"\ud83d\udce6 Imports trouv\u00e9s : {len(imports)}\")\n-    \n+\n     # Afficher les imports\n     print(\"\\n\u2705 MODULES INT\u00c9GR\u00c9S :\")\n     for module, class_name in imports:\n         print(f\"  - {module} -> {class_name}\")\n-    \n+\n     # Chercher les modules athalia_core\n     core_modules = []\n     core_path = Path(\"athalia_core\")\n     for py_file in core_path.glob(\"*.py\"):\n-        if py_file.name != \"__init__.py\" and not py_file.name.startswith('._'):\n+        if py_file.name != \"__init__.py\" and not py_file.name.startswith(\"._\"):\n             module_name = py_file.stem\n             core_modules.append(module_name)\n-    \n+\n     print(f\"\\n\ud83d\udce6 Modules athalia_core totaux : {len(core_modules)}\")\n-    \n+\n     # Identifier les modules non int\u00e9gr\u00e9s\n     integrated_modules = [module for module, _ in imports]\n     non_integrated = [m for m in core_modules if m not in integrated_modules]\n-    \n+\n     print(f\"\\n\u274c MODULES NON INT\u00c9GR\u00c9S ({len(non_integrated)}) :\")\n     for module in non_integrated:\n         print(f\"  - {module}\")\n-    \n+\n     # Calculer le score\n     integration_score = len(integrated_modules) / len(core_modules) * 10\n     print(f\"\\n\ud83d\udcc8 SCORE D'INT\u00c9GRATION : {integration_score:.2f}/10\")\n-    \n+\n     # Recommandations\n     print(\"\\n\ud83c\udfaf RECOMMANDATIONS :\")\n     if integration_score < 5.0:\n         print(\"  \u26a0\ufe0f Score faible - N\u00e9cessite une am\u00e9lioration urgente\")\n-    \n+\n     if non_integrated:\n         print(f\"  \ud83d\udce6 {len(non_integrated)} modules \u00e0 int\u00e9grer\")\n         print(\"  \ud83c\udfaf Priorit\u00e9 (top 5) :\")\n         for module in non_integrated[:5]:\n             print(f\"    - {module}\")\n-    \n+\n     # V\u00e9rifier les tests\n     print(\"\\n\ud83e\uddea V\u00c9RIFICATION DES TESTS :\")\n     test_files = list(Path(\"tests\").glob(\"*orchestrator*\"))\n     test_files.extend(Path(\"tests\").glob(\"*unified*\"))\n-    \n+\n     if test_files:\n         print(f\"  \u2705 Tests trouv\u00e9s : {len(test_files)}\")\n         for test_file in test_files:\n             print(f\"    - {test_file.name}\")\n     else:\n         print(\"  \u274c Aucun test trouv\u00e9 pour l'orchestrateur\")\n \n+\n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py\t2025-07-29 18:09:10.960000+00:00\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py\t2025-07-29 18:12:21.770494+00:00\n@@ -34,13 +34,13 @@\n             \"status\": \"idle\",\n             \"steps_completed\": [],\n             \"errors\": [],\n             \"warnings\": [],\n             \"metrics\": {},\n-            \"artifacts\": {}\n+            \"artifacts\": {},\n         }\n-        \n+\n         # Initialiser les modules\n         self.robust_ai = None\n         self.security_auditor = None\n         self.code_linter = None\n         self.correction_optimizer = None\n@@ -58,221 +58,229 @@\n             self.correction_optimizer = CorrectionOptimizer()\n             self.auto_tester = AutoTester(str(self.project_path))\n             self.auto_documenter = AutoDocumenter(str(self.project_path))\n             self.auto_cleaner = AutoCleaner(str(self.project_path))\n             self.auto_cicd = AutoCICD(str(self.project_path))\n-            \n+\n             self.workflow_results[\"status\"] = \"initialized\"\n             logger.info(\"\u2705 Tous les modules initialis\u00e9s\")\n-            \n-        except Exception as e:\n-            self.workflow_results[\"errors\"].append(f\"Erreur initialisation modules: {e}\")\n+\n+        except Exception as e:\n+            self.workflow_results[\"errors\"].append(\n+                f\"Erreur initialisation modules: {e}\"\n+            )\n             logger.error(f\"\u274c Erreur initialisation: {e}\")\n \n     def run_full_workflow(self, blueprint: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"Ex\u00e9cute le workflow complet\"\"\"\n         logger.info(\"\ud83d\ude80 D\u00e9marrage du workflow unifi\u00e9\")\n         self.workflow_results[\"status\"] = \"running\"\n-        \n+\n         try:\n             # \u00c9tape 1: G\u00e9n\u00e9ration du projet\n             self._step_generate_project(blueprint)\n-            \n+\n             # \u00c9tape 2: Audit de s\u00e9curit\u00e9\n             self._step_security_audit()\n-            \n+\n             # \u00c9tape 3: Linting du code\n             self._step_code_linting()\n-            \n+\n             # \u00c9tape 4: Optimisation des corrections\n             self._step_correction_optimization()\n-            \n+\n             # \u00c9tape 5: Tests automatiques\n             self._step_auto_testing()\n-            \n+\n             # \u00c9tape 6: Documentation automatique\n             self._step_auto_documentation()\n-            \n+\n             # \u00c9tape 7: Nettoyage automatique\n             self._step_auto_cleaning()\n-            \n+\n             # \u00c9tape 8: CI/CD automatique\n             self._step_auto_cicd()\n-            \n+\n             self.workflow_results[\"status\"] = \"completed\"\n             logger.info(\"\u2705 Workflow termin\u00e9 avec succ\u00e8s\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"status\"] = \"failed\"\n             self.workflow_results[\"errors\"].append(f\"Erreur workflow: {e}\")\n             logger.error(f\"\u274c Erreur workflow: {e}\")\n-        \n+\n         return self.workflow_results\n \n     def _step_generate_project(self, blueprint: Dict[str, Any]):\n         \"\"\"\u00c9tape 1: G\u00e9n\u00e9ration du projet\"\"\"\n         logger.info(\"\ud83d\udcc1 G\u00e9n\u00e9ration du projet...\")\n-        \n+\n         try:\n             project_path = generate_project(blueprint, self.project_path)\n             self.workflow_results[\"steps_completed\"].append(\"project_generation\")\n             self.workflow_results[\"artifacts\"][\"project_path\"] = project_path\n             logger.info(f\"\u2705 Projet g\u00e9n\u00e9r\u00e9: {project_path}\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"errors\"].append(f\"Erreur g\u00e9n\u00e9ration projet: {e}\")\n             raise\n \n     def _step_security_audit(self):\n         \"\"\"\u00c9tape 2: Audit de s\u00e9curit\u00e9\"\"\"\n         logger.info(\"\ud83d\udd12 Audit de s\u00e9curit\u00e9...\")\n-        \n+\n         try:\n             if self.security_auditor:\n                 security_results = self.security_auditor.run()\n                 self.workflow_results[\"steps_completed\"].append(\"security_audit\")\n                 self.workflow_results[\"artifacts\"][\"security_report\"] = security_results\n                 logger.info(\"\u2705 Audit de s\u00e9curit\u00e9 termin\u00e9\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"warnings\"].append(f\"Erreur audit s\u00e9curit\u00e9: {e}\")\n \n     def _step_code_linting(self):\n         \"\"\"\u00c9tape 3: Linting du code\"\"\"\n         logger.info(\"\ud83d\udccf Linting du code...\")\n-        \n+\n         try:\n             if self.code_linter:\n                 lint_results = self.code_linter.run()\n                 self.workflow_results[\"steps_completed\"].append(\"code_linting\")\n                 self.workflow_results[\"artifacts\"][\"lint_report\"] = lint_results\n                 logger.info(\"\u2705 Linting termin\u00e9\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"warnings\"].append(f\"Erreur linting: {e}\")\n \n     def _step_correction_optimization(self):\n         \"\"\"\u00c9tape 4: Optimisation des corrections\"\"\"\n         logger.info(\"\ud83d\udd27 Optimisation des corrections...\")\n-        \n+\n         try:\n             if self.correction_optimizer:\n                 # Optimiser les corrections bas\u00e9es sur les rapports pr\u00e9c\u00e9dents\n                 optimization_results = self.correction_optimizer.get_correction_stats()\n-                self.workflow_results[\"steps_completed\"].append(\"correction_optimization\")\n-                self.workflow_results[\"artifacts\"][\"optimization_stats\"] = optimization_results\n+                self.workflow_results[\"steps_completed\"].append(\n+                    \"correction_optimization\"\n+                )\n+                self.workflow_results[\"artifacts\"][\n+                    \"optimization_stats\"\n+                ] = optimization_results\n                 logger.info(\"\u2705 Optimisation termin\u00e9e\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"warnings\"].append(f\"Erreur optimisation: {e}\")\n \n     def _step_auto_testing(self):\n         \"\"\"\u00c9tape 5: Tests automatiques\"\"\"\n         logger.info(\"\ud83e\uddea Tests automatiques...\")\n-        \n+\n         try:\n             if self.auto_tester:\n                 test_results = self.auto_tester.run_tests()\n                 self.workflow_results[\"steps_completed\"].append(\"auto_testing\")\n                 self.workflow_results[\"artifacts\"][\"test_results\"] = test_results\n                 logger.info(\"\u2705 Tests automatiques termin\u00e9s\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"warnings\"].append(f\"Erreur tests automatiques: {e}\")\n \n     def _step_auto_documentation(self):\n         \"\"\"\u00c9tape 6: Documentation automatique\"\"\"\n         logger.info(\"\ud83d\udcda Documentation automatique...\")\n-        \n+\n         try:\n             if self.auto_documenter:\n                 doc_results = self.auto_documenter.generate_documentation()\n                 self.workflow_results[\"steps_completed\"].append(\"auto_documentation\")\n                 self.workflow_results[\"artifacts\"][\"documentation\"] = doc_results\n                 logger.info(\"\u2705 Documentation g\u00e9n\u00e9r\u00e9e\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"warnings\"].append(f\"Erreur documentation: {e}\")\n \n     def _step_auto_cleaning(self):\n         \"\"\"\u00c9tape 7: Nettoyage automatique\"\"\"\n         logger.info(\"\ud83e\uddf9 Nettoyage automatique...\")\n-        \n+\n         try:\n             if self.auto_cleaner:\n                 clean_results = self.auto_cleaner.clean_project()\n                 self.workflow_results[\"steps_completed\"].append(\"auto_cleaning\")\n                 self.workflow_results[\"artifacts\"][\"cleaning_report\"] = clean_results\n                 logger.info(\"\u2705 Nettoyage termin\u00e9\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"warnings\"].append(f\"Erreur nettoyage: {e}\")\n \n     def _step_auto_cicd(self):\n         \"\"\"\u00c9tape 8: CI/CD automatique\"\"\"\n         logger.info(\"\ud83d\ude80 Configuration CI/CD...\")\n-        \n+\n         try:\n             if self.auto_cicd:\n                 cicd_results = self.auto_cicd.setup_cicd()\n                 self.workflow_results[\"steps_completed\"].append(\"auto_cicd\")\n                 self.workflow_results[\"artifacts\"][\"cicd_config\"] = cicd_results\n                 logger.info(\"\u2705 CI/CD configur\u00e9\")\n-            \n+\n         except Exception as e:\n             self.workflow_results[\"warnings\"].append(f\"Erreur CI/CD: {e}\")\n \n     def generate_workflow_report(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un rapport du workflow\"\"\"\n         report = []\n         report.append(\"# Rapport Workflow Unifi\u00e9 Athalia\")\n         report.append(\"\")\n-        \n+\n         report.append(f\"## Statut: {self.workflow_results['status'].upper()}\")\n         report.append(f\"## Date: {datetime.now().isoformat()}\")\n         report.append(\"\")\n-        \n+\n         report.append(\"## \u00c9tapes Compl\u00e9t\u00e9es\")\n         for step in self.workflow_results[\"steps_completed\"]:\n             report.append(f\"- \u2705 {step}\")\n         report.append(\"\")\n-        \n+\n         if self.workflow_results[\"artifacts\"]:\n             report.append(\"## Art\u00e9facts G\u00e9n\u00e9r\u00e9s\")\n             for artifact, value in self.workflow_results[\"artifacts\"].items():\n                 report.append(f\"- **{artifact}**: {type(value).__name__}\")\n         report.append(\"\")\n-        \n+\n         if self.workflow_results[\"errors\"]:\n             report.append(\"## Erreurs\")\n             for error in self.workflow_results[\"errors\"]:\n                 report.append(f\"- \u274c {error}\")\n             report.append(\"\")\n-        \n+\n         if self.workflow_results[\"warnings\"]:\n             report.append(\"## Avertissements\")\n             for warning in self.workflow_results[\"warnings\"]:\n                 report.append(f\"- \u26a0\ufe0f {warning}\")\n             report.append(\"\")\n-        \n+\n         if self.workflow_results[\"metrics\"]:\n             report.append(\"## M\u00e9triques\")\n             for metric, value in self.workflow_results[\"metrics\"].items():\n                 report.append(f\"- **{metric}**: {value}\")\n-        \n+\n         return \"\\n\".join(report)\n \n     def save_workflow_results(self, output_path: str = \"workflow_results.json\"):\n         \"\"\"Sauvegarde les r\u00e9sultats du workflow\"\"\"\n         try:\n-            with open(output_path, 'w', encoding='utf-8') as f:\n+            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                 json.dump(self.workflow_results, f, indent=2, default=str)\n             logger.info(f\"\u2705 R\u00e9sultats sauvegard\u00e9s: {output_path}\")\n         except Exception as e:\n             logger.error(f\"\u274c Erreur sauvegarde: {e}\")\n \n \n-def run_unified_workflow(blueprint: Dict[str, Any], project_path: str = \".\") -> Dict[str, Any]:\n+def run_unified_workflow(\n+    blueprint: Dict[str, Any], project_path: str = \".\"\n+) -> Dict[str, Any]:\n     \"\"\"Fonction utilitaire pour ex\u00e9cuter le workflow unifi\u00e9\"\"\"\n     orchestrator = UnifiedOrchestrator(project_path)\n     orchestrator.initialize_modules()\n-    return orchestrator.run_full_workflow(blueprint) \n\\ No newline at end of file\n+    return orchestrator.run_full_workflow(blueprint)\n--- /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_documentation.py\t2025-07-29 17:56:19.850000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_documentation.py\t2025-07-29 18:12:21.826204+00:00\n@@ -8,12 +8,11 @@\n import logging\n from datetime import datetime\n from pathlib import Path\n \n logging.basicConfig(\n-    level=logging.INFO,\n-    format='%(asctime)s - %(levelname)s - %(message)s'\n+    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n )\n logger = logging.getLogger(__name__)\n \n \n class DocumentationCleaner:\n@@ -25,85 +24,101 @@\n         self.archive_dir.mkdir(parents=True, exist_ok=True)\n \n         # Documents \u00e0 conserver (actuels)\n         self.current_docs = {\n             # Principaux\n-            'README.md', 'INDEX_PRINCIPAL.md', 'INSTALLATION.md', 'USAGE.md', 'API.md',\n+            \"README.md\",\n+            \"INDEX_PRINCIPAL.md\",\n+            \"INSTALLATION.md\",\n+            \"USAGE.md\",\n+            \"API.md\",\n             # Plans d'action\n-            'PHASE_1_URGENT_TERMINEE.md', 'PLAN_ACTION_IMPORTANT.md',\n-            'PLAN_ACTION_AMELIORATION.md', 'PLAN_ACTION_URGENT.md', 'CAHIER_CHARGES_COMPLET.md',\n+            \"PHASE_1_URGENT_TERMINEE.md\",\n+            \"PLAN_ACTION_IMPORTANT.md\",\n+            \"PLAN_ACTION_AMELIORATION.md\",\n+            \"PLAN_ACTION_URGENT.md\",\n+            \"CAHIER_CHARGES_COMPLET.md\",\n             # Guides techniques\n-            'MODULES.md', 'TESTS_GUIDE.md', 'PLUGINS_GUIDE.md', 'DEPLOYMENT.md',\n-            'DEVELOPER_GUIDE.md', 'CONTRIBUTING.md', 'GIT_WORKFLOW.md', 'BEST_PRACTICES.md',\n+            \"MODULES.md\",\n+            \"TESTS_GUIDE.md\",\n+            \"PLUGINS_GUIDE.md\",\n+            \"DEPLOYMENT.md\",\n+            \"DEVELOPER_GUIDE.md\",\n+            \"CONTRIBUTING.md\",\n+            \"GIT_WORKFLOW.md\",\n+            \"BEST_PRACTICES.md\",\n             # Robotics et IA\n-            'ROBOTICS_GUIDE.md', 'REACHY_SETUP_GUIDE.md', 'RESUME_TEST_PROMPTS.md',\n-            'UNIFIED_ORCHESTRATOR.md', 'ALIAS_UNIFIED.md',\n+            \"ROBOTICS_GUIDE.md\",\n+            \"REACHY_SETUP_GUIDE.md\",\n+            \"RESUME_TEST_PROMPTS.md\",\n+            \"UNIFIED_ORCHESTRATOR.md\",\n+            \"ALIAS_UNIFIED.md\",\n             # Analyses et rapports\n-            'RAPPORT_FINAL.md', 'AUDIT_COMPLET_PROJET.md', 'INVENTAIRE_COMPLET.md',\n-            'ANALYSE_PLAN_OPTIMISATION_AVANCE.md', 'OPTIMISATION_PERFORMANCES.md',\n-            'ORGANISATION_WORKSPACE.md',\n+            \"RAPPORT_FINAL.md\",\n+            \"AUDIT_COMPLET_PROJET.md\",\n+            \"INVENTAIRE_COMPLET.md\",\n+            \"ANALYSE_PLAN_OPTIMISATION_AVANCE.md\",\n+            \"OPTIMISATION_PERFORMANCES.md\",\n+            \"ORGANISATION_WORKSPACE.md\",\n             # Support\n-            'FAQ.md', 'TROUBLESHOOTING.md'\n+            \"FAQ.md\",\n+            \"TROUBLESHOOTING.md\",\n         }\n \n         # Documents obsol\u00e8tes \u00e0 archiver\n         self.obsolete_docs = [\n-            'INDEX.md',  # Remplac\u00e9 par INDEX_PRINCIPAL.md\n-            'dashboard.md',  # Remplac\u00e9 par dashboard optimis\u00e9\n-            'ROADMAP.md',  # Contenu obsol\u00e8te\n-            'GENESIS.md',  # Historique\n-            'FINAL_SUMMARY.md',  # Remplac\u00e9 par PHASE_1_URGENT_TERMINEE.md\n-            'FINAL_SYSTEM_STATUS.md',  # Remplac\u00e9 par RAPPORT_FINAL.md\n-            'USER_GUIDE.md',  # Remplac\u00e9 par USAGE.md\n-            'ALIAS.md',  # Remplac\u00e9 par ALIAS_UNIFIED.md\n-            'INSTALL.md',  # Remplac\u00e9 par INSTALLATION.md\n-            'API_REFERENCE.md',  # Remplac\u00e9 par API.md\n-            'CLEANUP_REPORT.md',  # Historique\n-            'CI_PROBLEMS_ANALYSIS.md',  # Historique\n-            'NETTOYAGE_FINAL.md',  # Historique\n-            'CORRECTIONS_EFFECTUEES.md',  # Historique\n-            'PROCHAINES_ETAPES.md',  # Remplac\u00e9 par PLAN_ACTION_*.md\n-            'STATUT_ACTUEL.md',  # Remplac\u00e9 par PHASE_1_URGENT_TERMINEE.md\n-            'INVENTAIRE_COMPLET_SYSTEME.md',  # Remplac\u00e9 par INVENTAIRE_COMPLET.md\n-            'ORGANISATION_PROJET.md',  # Remplac\u00e9 par ORGANISATION_WORKSPACE.md\n-            'GUIDE_TEST_PLUGIN_VSCODE.md',  # Sp\u00e9cifique, \u00e0 archiver\n-            'GUIDE_VALIDATION.md',  # Remplac\u00e9 par TESTS_GUIDE.md\n-            'GUIDE_VALIDATION_TEMPS_REEL.md',  # Historique\n-            'GUIDE_PROMPTS_TEST.md',  # Remplac\u00e9 par RESUME_TEST_PROMPTS.md\n-            'INTELLIGENT_MODULES.md',  # Remplac\u00e9 par MODULES.md\n-            'ORCHESTRATION_CLARIFICATION.md',  # Historique\n-            'RAPPORT_AUDIT_FINAL.md',  # Remplac\u00e9 par AUDIT_COMPLET_PROJET.md\n-            'RAPPORT_COHERENCE_DOCUMENTATION.md',  # Historique\n-            'RAPPORT_NETTOYAGE_COMPLET.md',  # Historique\n-            'RAPPORT_RANGEMENT_RACINE.md',  # Historique\n-            'RAPPORT_OPTIMISATION_FINALE.md',  # Historique\n-            'RAPPORT_OPTIMISATION_FINALE_V2.md',  # Historique\n-            'RAPPORT_FINAL_COMPLET.md',  # Remplac\u00e9 par RAPPORT_FINAL.md\n-            'PROCHAINES_ETAPES_FINALES.md',  # Remplac\u00e9 par PLAN_ACTION_*.md\n-            'RAPPORT_CORRECTIONS_AUTO_TESTER.md',  # Historique\n-            'RAPPORT_FINAL_VALIDATION_PROFESSIONNELLE.md',  # Historique\n-            'RAPPORT_PROGRESSION_VALIDATION.md',  # Historique\n-            'RAPPORT_FINAL_CORRECTIONS.md',  # Historique\n-            'RAPPORT_FINAL_OPTIMISATION.md',  # Historique\n-            'RAPPORT_FINAL_CORRECTIONS.md',  # Historique\n+            \"INDEX.md\",  # Remplac\u00e9 par INDEX_PRINCIPAL.md\n+            \"dashboard.md\",  # Remplac\u00e9 par dashboard optimis\u00e9\n+            \"ROADMAP.md\",  # Contenu obsol\u00e8te\n+            \"GENESIS.md\",  # Historique\n+            \"FINAL_SUMMARY.md\",  # Remplac\u00e9 par PHASE_1_URGENT_TERMINEE.md\n+            \"FINAL_SYSTEM_STATUS.md\",  # Remplac\u00e9 par RAPPORT_FINAL.md\n+            \"USER_GUIDE.md\",  # Remplac\u00e9 par USAGE.md\n+            \"ALIAS.md\",  # Remplac\u00e9 par ALIAS_UNIFIED.md\n+            \"INSTALL.md\",  # Remplac\u00e9 par INSTALLATION.md\n+            \"API_REFERENCE.md\",  # Remplac\u00e9 par API.md\n+            \"CLEANUP_REPORT.md\",  # Historique\n+            \"CI_PROBLEMS_ANALYSIS.md\",  # Historique\n+            \"NETTOYAGE_FINAL.md\",  # Historique\n+            \"CORRECTIONS_EFFECTUEES.md\",  # Historique\n+            \"PROCHAINES_ETAPES.md\",  # Remplac\u00e9 par PLAN_ACTION_*.md\n+            \"STATUT_ACTUEL.md\",  # Remplac\u00e9 par PHASE_1_URGENT_TERMINEE.md\n+            \"INVENTAIRE_COMPLET_SYSTEME.md\",  # Remplac\u00e9 par INVENTAIRE_COMPLET.md\n+            \"ORGANISATION_PROJET.md\",  # Remplac\u00e9 par ORGANISATION_WORKSPACE.md\n+            \"GUIDE_TEST_PLUGIN_VSCODE.md\",  # Sp\u00e9cifique, \u00e0 archiver\n+            \"GUIDE_VALIDATION.md\",  # Remplac\u00e9 par TESTS_GUIDE.md\n+            \"GUIDE_VALIDATION_TEMPS_REEL.md\",  # Historique\n+            \"GUIDE_PROMPTS_TEST.md\",  # Remplac\u00e9 par RESUME_TEST_PROMPTS.md\n+            \"INTELLIGENT_MODULES.md\",  # Remplac\u00e9 par MODULES.md\n+            \"ORCHESTRATION_CLARIFICATION.md\",  # Historique\n+            \"RAPPORT_AUDIT_FINAL.md\",  # Remplac\u00e9 par AUDIT_COMPLET_PROJET.md\n+            \"RAPPORT_COHERENCE_DOCUMENTATION.md\",  # Historique\n+            \"RAPPORT_NETTOYAGE_COMPLET.md\",  # Historique\n+            \"RAPPORT_RANGEMENT_RACINE.md\",  # Historique\n+            \"RAPPORT_OPTIMISATION_FINALE.md\",  # Historique\n+            \"RAPPORT_OPTIMISATION_FINALE_V2.md\",  # Historique\n+            \"RAPPORT_FINAL_COMPLET.md\",  # Remplac\u00e9 par RAPPORT_FINAL.md\n+            \"PROCHAINES_ETAPES_FINALES.md\",  # Remplac\u00e9 par PLAN_ACTION_*.md\n+            \"RAPPORT_CORRECTIONS_AUTO_TESTER.md\",  # Historique\n+            \"RAPPORT_FINAL_VALIDATION_PROFESSIONNELLE.md\",  # Historique\n+            \"RAPPORT_PROGRESSION_VALIDATION.md\",  # Historique\n+            \"RAPPORT_FINAL_CORRECTIONS.md\",  # Historique\n+            \"RAPPORT_FINAL_OPTIMISATION.md\",  # Historique\n+            \"RAPPORT_FINAL_CORRECTIONS.md\",  # Historique\n         ]\n \n     def scan_documentation(self):\n         \"\"\"Scanne la documentation et cat\u00e9gorise les fichiers\"\"\"\n         all_files = list(self.docs_dir.glob(\"*.md\"))\n \n-        categories = {\n-            'current': [],\n-            'obsolete': [],\n-            'unknown': []\n-        }\n+        categories = {\"current\": [], \"obsolete\": [], \"unknown\": []}\n \n         for file_path in all_files:\n             file_name = file_path.name\n \n             # Ignorer les fichiers syst\u00e8me macOS\n-            if file_name.startswith('._'):\n+            if file_name.startswith(\"._\"):\n                 continue\n \n             if file_name in self.current_docs:\n                 categories[\"current\"].append(file_path)\n             elif file_name in self.obsolete_docs:\n@@ -136,22 +151,22 @@\n         return archived_count\n \n     def create_documentation_report(self, categories, archived_count):\n         \"\"\"Cr\u00e9e un rapport de nettoyage de la documentation\"\"\"\n         report = {\n-            'timestamp': datetime.now().isoformat(),\n-            'summary': {\n-                'current_docs': len(categories['current']),\n-                'obsolete_docs': len(categories['obsolete']),\n-                'unknown_docs': len(categories['unknown']),\n-                'archived_count': archived_count\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"summary\": {\n+                \"current_docs\": len(categories[\"current\"]),\n+                \"obsolete_docs\": len(categories[\"obsolete\"]),\n+                \"unknown_docs\": len(categories[\"unknown\"]),\n+                \"archived_count\": archived_count,\n             },\n-            'categories': {\n-                'current': [str(f) for f in categories['current']],\n-                'obsolete': [str(f) for f in categories['obsolete']],\n-                'unknown': [str(f) for f in categories['unknown']]\n-            }\n+            \"categories\": {\n+                \"current\": [str(f) for f in categories[\"current\"]],\n+                \"obsolete\": [str(f) for f in categories[\"obsolete\"]],\n+                \"unknown\": [str(f) for f in categories[\"unknown\"]],\n+            },\n         }\n \n         return report\n \n     def cleanup(self, dry_run=False):\n@@ -174,19 +189,19 @@\n         # Actions de nettoyage\n         archived_count = 0\n \n         if not dry_run:\n             # Archiver les documents obsol\u00e8tes\n-            archived_count = self.archive_obsolete_docs(categories['obsolete'])\n+            archived_count = self.archive_obsolete_docs(categories[\"obsolete\"])\n \n         # G\u00e9n\u00e9rer le rapport\n         report = self.create_documentation_report(categories, archived_count)\n \n         # Sauvegarder le rapport\n-        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         report_file = self.docs_dir / f\"cleanup_report_{timestamp}.json\"\n-        with open(report_file, 'w', encoding='utf-8') as f:\n+        with open(report_file, \"w\", encoding=\"utf-8\") as f:\n             json.dump(report, f, indent=2, ensure_ascii=False)\n \n         # Afficher le r\u00e9sum\u00e9\n         logger.info(\"\u2705 NETTOYAGE TERMIN\u00c9\")\n         logger.info(f\"\ud83d\udce6 Documents archiv\u00e9s: {archived_count}\")\n@@ -213,6 +228,6 @@\n     # Ex\u00e9cuter le nettoyage\n     cleaner.cleanup(dry_run=args.dry_run)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/tests/optimize_performance.py\t2025-07-29 17:56:11.540000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tests/optimize_performance.py\t2025-07-29 18:12:21.828584+00:00\n@@ -24,258 +24,272 @@\n     pytest = None\n \n \n class TestPerformanceOptimizer:\n     \"\"\"Optimiseur de performances des tests\"\"\"\n-    \n+\n     def __init__(self, test_dir: str = \"tests\"):\n         self.test_dir = Path(test_dir)\n         self.results = {}\n         self.slow_tests = []\n         self.fast_tests = []\n-        \n+\n     def analyze_test_performance(self) -> Dict[str, float]:\n         \"\"\"Analyse les performances de tous les tests\n-        \n+\n         Returns:\n             Dict avec les temps d'ex\u00e9cution par test\n         \"\"\"\n         print(\"\ud83d\udd0d Analyse des performances des tests...\")\n-        \n+\n         # Ex\u00e9cution avec mesure des dur\u00e9es\n         cmd = [\n-            sys.executable, \"-m\", \"pytest\", \n-            str(self.test_dir), \n-            \"--durations=0\", \n-            \"-q\", \n-            \"--tb=no\"\n+            sys.executable,\n+            \"-m\",\n+            \"pytest\",\n+            str(self.test_dir),\n+            \"--durations=0\",\n+            \"-q\",\n+            \"--tb=no\",\n         ]\n-        \n+\n         start_time = time.time()\n         try:\n             result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n             total_time = time.time() - start_time\n-            \n+\n             # Parse des r\u00e9sultats\n             self._parse_durations(result.stdout)\n-            \n+\n             print(f\"\u2705 Analyse termin\u00e9e en {total_time:.2f}s\")\n             return self.results\n-            \n+\n         except subprocess.TimeoutExpired:\n             print(\"\u23f0 Timeout lors de l'analyse des performances\")\n             return {}\n         except Exception as e:\n             print(f\"\u274c Erreur lors de l'analyse: {e}\")\n             return {}\n-    \n+\n     def _parse_durations(self, output: str):\n         \"\"\"Parse la sortie de pytest --durations\"\"\"\n-        lines = output.split('\\n')\n+        lines = output.split(\"\\n\")\n         for line in lines:\n-            if 'passed' in line and 'failed' in line:\n+            if \"passed\" in line and \"failed\" in line:\n                 # Ligne de r\u00e9sum\u00e9\n                 continue\n-            if 'slowest durations' in line:\n+            if \"slowest durations\" in line:\n                 # Section des dur\u00e9es\n                 continue\n-            if line.strip() and '::' in line and 'PASSED' in line:\n+            if line.strip() and \"::\" in line and \"PASSED\" in line:\n                 # Ligne de test avec dur\u00e9e\n-                parts = line.split('::')\n+                parts = line.split(\"::\")\n                 if len(parts) >= 2:\n-                    test_name = parts[-1].split(' ')[0]\n+                    test_name = parts[-1].split(\" \")[0]\n                     duration = self._extract_duration(line)\n                     if duration:\n                         self.results[test_name] = duration\n-    \n+\n     def _extract_duration(self, line: str) -> float:\n         \"\"\"Extrait la dur\u00e9e d'une ligne de test\"\"\"\n         try:\n             # Cherche le pattern de dur\u00e9e pytest\n-            if 'PASSED' in line:\n-                parts = line.split('PASSED')\n+            if \"PASSED\" in line:\n+                parts = line.split(\"PASSED\")\n                 if len(parts) > 1:\n                     time_part = parts[1].strip()\n-                    if time_part.startswith('[') and time_part.endswith(']'):\n+                    if time_part.startswith(\"[\") and time_part.endswith(\"]\"):\n                         time_str = time_part[1:-1]\n-                        if 's' in time_str:\n-                            return float(time_str.replace('s', ''))\n+                        if \"s\" in time_str:\n+                            return float(time_str.replace(\"s\", \"\"))\n         except Exception:\n             pass\n         return 0.0\n-    \n+\n     def identify_slow_tests(self, threshold: float = 1.0) -> List[str]:\n         \"\"\"Identifie les tests lents\n-        \n+\n         Args:\n             threshold: Seuil en secondes pour consid\u00e9rer un test comme lent\n-            \n+\n         Returns:\n             Liste des tests lents\n         \"\"\"\n         self.slow_tests = [\n-            test for test, duration in self.results.items()\n-            if duration > threshold\n+            test for test, duration in self.results.items() if duration > threshold\n         ]\n-        \n+\n         print(f\"\ud83d\udc0c {len(self.slow_tests)} tests lents identifi\u00e9s (> {threshold}s)\")\n         return self.slow_tests\n-    \n+\n     def identify_fast_tests(self, threshold: float = 0.1) -> List[str]:\n         \"\"\"Identifie les tests rapides\n-        \n+\n         Args:\n             threshold: Seuil en secondes pour consid\u00e9rer un test comme rapide\n-            \n+\n         Returns:\n             Liste des tests rapides\n         \"\"\"\n         self.fast_tests = [\n-            test for test, duration in self.results.items()\n-            if duration < threshold\n+            test for test, duration in self.results.items() if duration < threshold\n         ]\n-        \n+\n         print(f\"\u26a1 {len(self.fast_tests)} tests rapides identifi\u00e9s (< {threshold}s)\")\n         return self.fast_tests\n-    \n+\n     def generate_optimization_report(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un rapport d'optimisation\n-        \n+\n         Returns:\n             Contenu du rapport\n         \"\"\"\n         report = []\n         report.append(\"# \ud83d\udcca RAPPORT D'OPTIMISATION DES PERFORMANCES\")\n         report.append(\"\")\n         report.append(f\"**Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n         report.append(f\"**Tests analys\u00e9s:** {len(self.results)}\")\n         report.append(\"\")\n-        \n+\n         # Statistiques g\u00e9n\u00e9rales\n         if self.results:\n             durations = list(self.results.values())\n             avg_duration = sum(durations) / len(durations)\n             max_duration = max(durations)\n             min_duration = min(durations)\n-            \n+\n             report.append(\"## \ud83d\udcc8 Statistiques g\u00e9n\u00e9rales\")\n             report.append(f\"- **Dur\u00e9e moyenne:** {avg_duration:.3f}s\")\n             report.append(f\"- **Dur\u00e9e maximale:** {max_duration:.3f}s\")\n             report.append(f\"- **Dur\u00e9e minimale:** {min_duration:.3f}s\")\n             report.append(\"\")\n-        \n+\n         # Tests lents\n         if self.slow_tests:\n             report.append(\"## \ud83d\udc0c Tests lents (> 1s)\")\n-            for test in sorted(self.slow_tests, key=lambda x: self.results.get(x, 0), reverse=True):\n+            for test in sorted(\n+                self.slow_tests, key=lambda x: self.results.get(x, 0), reverse=True\n+            ):\n                 duration = self.results.get(test, 0)\n                 report.append(f\"- `{test}`: {duration:.3f}s\")\n             report.append(\"\")\n-        \n+\n         # Tests rapides\n         if self.fast_tests:\n             report.append(\"## \u26a1 Tests rapides (< 0.1s)\")\n             for test in sorted(self.fast_tests, key=lambda x: self.results.get(x, 0)):\n                 duration = self.results.get(test, 0)\n                 report.append(f\"- `{test}`: {duration:.3f}s\")\n             report.append(\"\")\n-        \n+\n         # Recommandations\n         report.append(\"## \ud83d\udca1 Recommandations d'optimisation\")\n         report.append(\"\")\n-        \n+\n         if self.slow_tests:\n             report.append(\"### Pour les tests lents:\")\n-            report.append(\"1. **Ajouter des marqueurs de performance:** `@pytest.mark.slow`\")\n+            report.append(\n+                \"1. **Ajouter des marqueurs de performance:** `@pytest.mark.slow`\"\n+            )\n             report.append(\"2. **Utiliser des mocks** pour les d\u00e9pendances externes\")\n             report.append(\"3. **Optimiser les setup/teardown** avec `@classmethod`\")\n             report.append(\"4. **Parall\u00e9liser** les tests ind\u00e9pendants\")\n             report.append(\"5. **Mettre en cache** les objets co\u00fbteux\")\n             report.append(\"\")\n-        \n+\n         report.append(\"### Optimisations g\u00e9n\u00e9rales:\")\n         report.append(\"1. **Ex\u00e9cuter les tests rapides en premier**\")\n         report.append(\"2. **Utiliser `pytest-xdist` pour la parall\u00e9lisation**\")\n         report.append(\"3. **Configurer des timeouts appropri\u00e9s**\")\n         report.append(\"4. **R\u00e9duire les I/O inutiles**\")\n         report.append(\"5. **Optimiser les imports**\")\n-        \n+\n         return \"\\n\".join(report)\n-    \n+\n     def save_report(self, filename: str = \"performance_optimization_report.md\"):\n         \"\"\"Sauvegarde le rapport d'optimisation\n-        \n+\n         Args:\n             filename: Nom du fichier de rapport\n         \"\"\"\n         report = self.generate_optimization_report()\n-        \n-        with open(filename, 'w', encoding='utf-8') as f:\n+\n+        with open(filename, \"w\", encoding=\"utf-8\") as f:\n             f.write(report)\n-        \n+\n         print(f\"\ud83d\udcc4 Rapport sauvegard\u00e9: {filename}\")\n-    \n+\n     def run_fast_tests_only(self) -> bool:\n         \"\"\"Ex\u00e9cute seulement les tests rapides\n-        \n+\n         Returns:\n             True si tous les tests rapides passent\n         \"\"\"\n         if not self.fast_tests:\n             print(\"\u26a0\ufe0f Aucun test rapide identifi\u00e9\")\n             return False\n-        \n+\n         print(f\"\u26a1 Ex\u00e9cution de {len(self.fast_tests)} tests rapides...\")\n-        \n+\n         # Cr\u00e9er un fichier temporaire avec les tests rapides\n-        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n+        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n             f.write(\"import pytest\\n\\n\")\n             for test in self.fast_tests:\n                 f.write(f\"pytest.main(['{test}'])\\n\")\n             temp_file = f.name\n-        \n+\n         try:\n-            result = subprocess.run([sys.executable, temp_file], capture_output=True, text=True)\n+            result = subprocess.run(\n+                [sys.executable, temp_file], capture_output=True, text=True\n+            )\n             success = result.returncode == 0\n-            \n+\n             if success:\n                 print(\"\u2705 Tous les tests rapides ont r\u00e9ussi\")\n             else:\n                 print(\"\u274c Certains tests rapides ont \u00e9chou\u00e9\")\n                 print(result.stdout)\n                 print(result.stderr)\n-            \n+\n             return success\n-            \n+\n         finally:\n             os.unlink(temp_file)\n \n \n def main():\n     \"\"\"Fonction principale\"\"\"\n     parser = argparse.ArgumentParser(description=\"Optimiseur de performances des tests\")\n     parser.add_argument(\"--test-dir\", default=\"tests\", help=\"R\u00e9pertoire des tests\")\n-    parser.add_argument(\"--threshold\", type=float, default=1.0, help=\"Seuil pour les tests lents (s)\")\n-    parser.add_argument(\"--report\", default=\"performance_optimization_report.md\", help=\"Fichier de rapport\")\n-    parser.add_argument(\"--fast-only\", action=\"store_true\", help=\"Ex\u00e9cuter seulement les tests rapides\")\n-    \n+    parser.add_argument(\n+        \"--threshold\", type=float, default=1.0, help=\"Seuil pour les tests lents (s)\"\n+    )\n+    parser.add_argument(\n+        \"--report\",\n+        default=\"performance_optimization_report.md\",\n+        help=\"Fichier de rapport\",\n+    )\n+    parser.add_argument(\n+        \"--fast-only\", action=\"store_true\", help=\"Ex\u00e9cuter seulement les tests rapides\"\n+    )\n+\n     args = parser.parse_args()\n-    \n+\n     optimizer = TestPerformanceOptimizer(args.test_dir)\n-    \n+\n     # Analyse des performances\n     optimizer.analyze_test_performance()\n-    \n+\n     # Identification des tests\n     optimizer.identify_slow_tests(args.threshold)\n     optimizer.identify_fast_tests(0.1)\n-    \n+\n     # G\u00e9n\u00e9ration du rapport\n     optimizer.save_report(args.report)\n-    \n+\n     # Ex\u00e9cution des tests rapides si demand\u00e9\n     if args.fast_only:\n         optimizer.run_fast_tests_only()\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/scripts/validation_continue.py\t2025-07-29 17:49:47.620000+00:00\n+++ /Volumes/T7/athalia-dev-setup/scripts/validation_continue.py\t2025-07-29 18:12:21.831001+00:00\n@@ -23,47 +23,49 @@\n         self.seuil_regression = 10  # 10% de baisse = r\u00e9gression\n \n         # Configuration du logging\n         logging.basicConfig(\n             level=logging.INFO,\n-            format='%(asctime)s - %(levelname)s - %(message)s',\n+            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n             handlers=[\n-                logging.FileHandler('validation_continue.log'),\n-                logging.StreamHandler()\n-            ]\n+                logging.FileHandler(\"validation_continue.log\"),\n+                logging.StreamHandler(),\n+            ],\n         )\n         self.logger = logging.getLogger(__name__)\n \n     def test_rapide(self):\n         \"\"\"Test rapide de validation (version all\u00e9g\u00e9e)\"\"\"\n         start = time.time()\n         resultats = {}\n \n         # Test 1: D\u00e9marrage\n-        resultats['demarrage'] = self.test_demarrage()\n+        resultats[\"demarrage\"] = self.test_demarrage()\n \n         # Test 2: Imports\n-        resultats['imports'] = self.test_imports()\n+        resultats[\"imports\"] = self.test_imports()\n \n         # Test 3: G\u00e9n\u00e9ration mini\n-        resultats['generation'] = self.test_generation_mini()\n+        resultats[\"generation\"] = self.test_generation_mini()\n \n         # Test 4: Correction basique\n-        resultats['correction'] = self.test_correction_basique()\n+        resultats[\"correction\"] = self.test_correction_basique()\n \n         temps_total = time.time() - start\n \n         # Calcul du taux de succ\u00e8s\n-        succes = sum(1 for r in resultats.values() if r.get('succes', False))\n+        succes = sum(1 for r in resultats.values() if r.get(\"succes\", False))\n         taux_succes = (succes / len(resultats)) * 100\n \n         validation = {\n-            'timestamp': datetime.now().isoformat(),\n-            'taux_succes': taux_succes,\n-            'temps_total': temps_total,\n-            'resultats': resultats,\n-            'erreurs_critiques': len([r for r in resultats.values() if not r.get('succes', False)])\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"taux_succes\": taux_succes,\n+            \"temps_total\": temps_total,\n+            \"resultats\": resultats,\n+            \"erreurs_critiques\": len(\n+                [r for r in resultats.values() if not r.get(\"succes\", False)]\n+            ),\n         }\n \n         # Sauvegarde dans l'historique\n         self.historique.append(validation)\n         self.sauvegarder_historique()\n@@ -72,80 +74,89 @@\n \n     def test_demarrage(self):\n         \"\"\"Test de d\u00e9marrage d'Athalia\"\"\"\n         try:\n             cmd = \"python scripts/athalia_unified.py --help\"\n-            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)\n-            return {'succes': result.returncode == 0}\n-        except Exception as e:\n-            return {'succes': False, 'erreur': str(e)}\n+            result = subprocess.run(\n+                cmd, shell=True, capture_output=True, text=True, timeout=10\n+            )\n+            return {\"succes\": result.returncode == 0}\n+        except Exception as e:\n+            return {\"succes\": False, \"erreur\": str(e)}\n \n     def test_imports(self):\n         \"\"\"Test des imports critiques\"\"\"\n         try:\n             cmd = \"python -c 'import athalia_core; print(\\\"OK\\\")'\"\n-            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)\n-            return {'succes': result.returncode == 0}\n-        except Exception as e:\n-            return {'succes': False, 'erreur': str(e)}\n+            result = subprocess.run(\n+                cmd, shell=True, capture_output=True, text=True, timeout=10\n+            )\n+            return {\"succes\": result.returncode == 0}\n+        except Exception as e:\n+            return {\"succes\": False, \"erreur\": str(e)}\n \n     def test_generation_mini(self):\n         \"\"\"Test de g\u00e9n\u00e9ration minimal\"\"\"\n         try:\n             # Cr\u00e9e un projet test temporaire\n             projet_test = f\"/tmp/test_continue_{int(time.time())}\"\n             os.makedirs(projet_test, exist_ok=True)\n \n-            with open(f\"{projet_test}/main.py\", 'w') as f:\n+            with open(f\"{projet_test}/main.py\", \"w\") as f:\n                 f.write(\"print('test')\")\n \n             cmd = f\"python scripts/athalia_unified.py {projet_test} --action audit\"\n-            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                cmd, shell=True, capture_output=True, text=True, timeout=30\n+            )\n \n             # Nettoyage\n             import shutil\n+\n             shutil.rmtree(projet_test, ignore_errors=True)\n \n-            return {'succes': result.returncode == 0}\n-        except Exception as e:\n-            return {'succes': False, 'erreur': str(e)}\n+            return {\"succes\": result.returncode == 0}\n+        except Exception as e:\n+            return {\"succes\": False, \"erreur\": str(e)}\n \n     def test_correction_basique(self):\n         \"\"\"Test de correction basique\"\"\"\n         try:\n             # Cr\u00e9e un fichier avec une erreur simple\n             fichier_test = \"/tmp/test_correction.py\"\n-            with open(fichier_test, 'w') as f:\n+            with open(fichier_test, \"w\") as f:\n                 f.write(\"x = 1\\ny = 2\\nprint(x + y + z)  # Erreur: z non d\u00e9fini\")\n \n             cmd = f\"python scripts/athalia_unified.py {os.path.dirname(fichier_test)} --action fix\"\n-            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                cmd, shell=True, capture_output=True, text=True, timeout=30\n+            )\n \n             # Nettoyage\n             os.remove(fichier_test)\n \n-            return {'succes': result.returncode == 0}\n-        except Exception as e:\n-            return {'succes': False, 'erreur': str(e)}\n+            return {\"succes\": result.returncode == 0}\n+        except Exception as e:\n+            return {\"succes\": False, \"erreur\": str(e)}\n \n     def detecter_regression(self, validation_actuelle):\n         \"\"\"D\u00e9tecte une r\u00e9gression par rapport \u00e0 l'historique\"\"\"\n         if len(self.historique) < 3:\n             return None\n \n         # Compare avec les 3 derni\u00e8res validations\n         recentes = self.historique[-3:]\n-        taux_moyen_recent = sum(v['taux_succes'] for v in recentes) / len(recentes)\n-\n-        baisse = taux_moyen_recent - validation_actuelle['taux_succes']\n+        taux_moyen_recent = sum(v[\"taux_succes\"] for v in recentes) / len(recentes)\n+\n+        baisse = taux_moyen_recent - validation_actuelle[\"taux_succes\"]\n \n         if baisse > self.seuil_regression:\n             return {\n-                'type': 'regression',\n-                'baisse': baisse,\n-                'taux_avant': taux_moyen_recent,\n-                'taux_apres': validation_actuelle['taux_succes']\n+                \"type\": \"regression\",\n+                \"baisse\": baisse,\n+                \"taux_avant\": taux_moyen_recent,\n+                \"taux_apres\": validation_actuelle[\"taux_succes\"],\n             }\n \n         return None\n \n     def demarrer_surveillance(self):\n@@ -166,27 +177,33 @@\n \n                     # D\u00e9tection de r\u00e9gression\n                     regression = self.detecter_regression(validation)\n \n                     if regression:\n-                        self.logger.warning(f\"\ud83d\udea8 R\u00c9GRESSION D\u00c9TECT\u00c9E: {regression['baisse']:.1f}% de baisse\")\n+                        self.logger.warning(\n+                            f\"\ud83d\udea8 R\u00c9GRESSION D\u00c9TECT\u00c9E: {regression['baisse']:.1f}% de baisse\"\n+                        )\n                         self.alerter_regression(validation, regression)\n \n                     # Rapport de tendance p\u00e9riodique\n                     if len(self.historique) % 10 == 0:  # Tous les 10 tests\n                         self.generer_rapport_tendance()\n                         self.logger.info(\"\ud83d\udcca Rapport de tendance g\u00e9n\u00e9r\u00e9\")\n \n-                    self.logger.info(f\"\u2705 Test termin\u00e9: {validation['taux_succes']:.1f}% de succ\u00e8s\")\n+                    self.logger.info(\n+                        f\"\u2705 Test termin\u00e9: {validation['taux_succes']:.1f}% de succ\u00e8s\"\n+                    )\n \n                 except Exception as e:\n                     self.logger.error(f\"\u274c Erreur lors du test: {str(e)}\")\n \n                 # Attente jusqu'au prochain test\n                 time.sleep(self.intervalle_minutes * 60)\n \n-        self.thread_surveillance = threading.Thread(target=boucle_surveillance, daemon=True)\n+        self.thread_surveillance = threading.Thread(\n+            target=boucle_surveillance, daemon=True\n+        )\n         self.thread_surveillance.start()\n \n         return self.thread_surveillance\n \n     def arreter_surveillance(self):\n@@ -197,41 +214,41 @@\n         self.logger.info(\"\ud83d\uded1 Surveillance arr\u00eat\u00e9e\")\n \n     def alerter_regression(self, validation, regression):\n         \"\"\"G\u00e9n\u00e8re une alerte en cas de r\u00e9gression\"\"\"\n         alerte = {\n-            'timestamp': datetime.now().isoformat(),\n-            'type': 'regression',\n-            'validation': validation,\n-            'regression': regression,\n-            'gravite': 'CRITIQUE' if regression['baisse'] > 20 else 'MOYENNE'\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"type\": \"regression\",\n+            \"validation\": validation,\n+            \"regression\": regression,\n+            \"gravite\": \"CRITIQUE\" if regression[\"baisse\"] > 20 else \"MOYENNE\",\n         }\n \n         # Sauvegarde de l'alerte\n-        alertes_file = 'alertes_regression.json'\n+        alertes_file = \"alertes_regression.json\"\n         alertes = []\n \n         if os.path.exists(alertes_file):\n             try:\n-                with open(alertes_file, 'r') as f:\n+                with open(alertes_file, \"r\") as f:\n                     alertes = json.load(f)\n             except Exception:\n                 alertes = []\n \n         alertes.append(alerte)\n \n-        with open(alertes_file, 'w') as f:\n+        with open(alertes_file, \"w\") as f:\n             json.dump(alertes, f, indent=2)\n \n         # G\u00e9n\u00e9ration du rapport d'alerte\n         rapport = self.generer_rapport_alerte(alerte)\n         self.logger.warning(f\"\ud83d\udea8 ALERTE: {rapport[:100]}...\")\n \n     def generer_rapport_alerte(self, alerte):\n         \"\"\"G\u00e9n\u00e8re un rapport d'alerte d\u00e9taill\u00e9\"\"\"\n-        regression = alerte['regression']\n-        validation = alerte['validation']\n+        regression = alerte[\"regression\"]\n+        validation = alerte[\"validation\"]\n \n         rapport = f\"\"\"# \ud83d\udea8 ALERTE R\u00c9GRESSION - Athalia/Arkalia\n \n **Date:** {datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}  \n **Gravit\u00e9:** {alerte['gravite']}  \n@@ -247,15 +264,15 @@\n \n ## \ud83d\udd0d Analyse des R\u00e9sultats\n \n \"\"\"\n \n-        for nom, resultat in validation['resultats'].items():\n-            status = \"\u2705 SUCC\u00c8S\" if resultat.get('succes') else \"\u274c \u00c9CHEC\"\n+        for nom, resultat in validation[\"resultats\"].items():\n+            status = \"\u2705 SUCC\u00c8S\" if resultat.get(\"succes\") else \"\u274c \u00c9CHEC\"\n             rapport += f\"- **{nom}:** {status}\\n\"\n \n-            if not resultat.get('succes'):\n+            if not resultat.get(\"succes\"):\n                 rapport += f\"  - Erreur: {resultat.get('erreur', 'Inconnue')}\\n\"\n \n         rapport += \"\"\"\n ## \ud83c\udfaf Actions Recommand\u00e9es\n \n@@ -268,35 +285,37 @@\n \n \"\"\"\n \n         # Affiche les 5 derni\u00e8res validations\n         for validation in self.historique[-5:]:\n-            date = datetime.fromisoformat(validation['timestamp']).strftime(\"%d/%m %H:%M\")\n+            date = datetime.fromisoformat(validation[\"timestamp\"]).strftime(\n+                \"%d/%m %H:%M\"\n+            )\n             rapport += f\"- {date}: {validation['taux_succes']:.1f}% de succ\u00e8s\\n\"\n \n         # Sauvegarde du rapport\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         rapport_file = f\"alerte_regression_{timestamp}.md\"\n \n-        with open(rapport_file, 'w', encoding='utf-8') as f:\n+        with open(rapport_file, \"w\", encoding=\"utf-8\") as f:\n             f.write(rapport)\n \n         self.logger.info(f\"\ud83d\udcc4 Rapport d'alerte g\u00e9n\u00e9r\u00e9: {rapport_file}\")\n \n     def sauvegarder_historique(self):\n         \"\"\"Sauvegarde l'historique des validations\"\"\"\n         try:\n-            with open('historique_validation.json', 'w') as f:\n+            with open(\"historique_validation.json\", \"w\") as f:\n                 json.dump(self.historique, f, indent=2)\n         except Exception as e:\n             self.logger.error(f\"Erreur sauvegarde historique: {str(e)}\")\n \n     def charger_historique(self):\n         \"\"\"Charge l'historique des validations\"\"\"\n         try:\n-            if os.path.exists('historique_validation.json'):\n-                with open('historique_validation.json', 'r') as f:\n+            if os.path.exists(\"historique_validation.json\"):\n+                with open(\"historique_validation.json\", \"r\") as f:\n                     self.historique = json.load(f)\n         except Exception as e:\n             self.logger.error(f\"Erreur chargement historique: {str(e)}\")\n \n     def generer_rapport_tendance(self):\n@@ -307,14 +326,14 @@\n         # Calcul des tendances\n         recentes = self.historique[-10:]  # 10 derni\u00e8res validations\n         anciennes = self.historique[:10]  # 10 premi\u00e8res validations\n \n         if len(anciennes) < 10:\n-            anciennes = self.historique[:len(self.historique) // 2]\n-\n-        taux_recent = sum(v['taux_succes'] for v in recentes) / len(recentes)\n-        taux_ancien = sum(v['taux_succes'] for v in anciennes) / len(anciennes)\n+            anciennes = self.historique[: len(self.historique) // 2]\n+\n+        taux_recent = sum(v[\"taux_succes\"] for v in recentes) / len(recentes)\n+        taux_ancien = sum(v[\"taux_succes\"] for v in anciennes) / len(anciennes)\n \n         evolution = taux_recent - taux_ancien\n \n         rapport = f\"\"\"# \ud83d\udcca Rapport de Tendance - Athalia/Arkalia\n \n@@ -332,11 +351,13 @@\n ## \ud83c\udfaf Recommandations\n \n \"\"\"\n \n         if evolution > 5:\n-            rapport += \"\ud83c\udf89 **Excellent !** Athalia s'am\u00e9liore. Continue dans cette direction.\"\n+            rapport += (\n+                \"\ud83c\udf89 **Excellent !** Athalia s'am\u00e9liore. Continue dans cette direction.\"\n+            )\n         elif evolution > 0:\n             rapport += \"\u2705 **Bien !** L\u00e9g\u00e8re am\u00e9lioration d\u00e9tect\u00e9e.\"\n         elif evolution > -5:\n             rapport += \"\u26a0\ufe0f **Attention !** L\u00e9g\u00e8re r\u00e9gression d\u00e9tect\u00e9e. Surveille les prochains changements.\"\n         else:\n@@ -369,6 +390,6 @@\n             time.sleep(10)  # Check toutes les 10 secondes\n         print(f\"\\n\u23f0 Timeout atteint ({timeout_seconds}s) - Arr\u00eat automatique\")\n     except KeyboardInterrupt:\n         print(\"\\n\ud83d\uded1 Arr\u00eat de la surveillance...\")\n     finally:\n-        validator.arreter_surveillance() \n\\ No newline at end of file\n+        validator.arreter_surveillance()\n--- /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_archives.py\t2025-07-29 17:56:19.740000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_archives.py\t2025-07-29 18:12:21.834245+00:00\n@@ -16,13 +16,11 @@\n # Configuration du logging\n logging.basicConfig(\n     level=logging.INFO,\n     format=\"%(asctime)s | %(levelname)s | %(message)s\",\n     handlers=[\n-        logging.FileHandler(\n-            \"logs/archive_cleanup.log\", mode=\"a\", encoding=\"utf-8\"\n-        ),\n+        logging.FileHandler(\"logs/archive_cleanup.log\", mode=\"a\", encoding=\"utf-8\"),\n         logging.StreamHandler(),\n     ],\n )\n logger = logging.getLogger(__name__)\n \n@@ -36,11 +34,11 @@\n         self.archive_path = self.docs_path / \"archive\"\n         self.cleanup_results: Dict[str, Any] = {\n             \"moved_files\": [],\n             \"deleted_files\": [],\n             \"organized_dirs\": [],\n-            \"broken_links_fixed\": 0\n+            \"broken_links_fixed\": 0,\n         }\n \n     def cleanup_archives(self, dry_run: bool = True) -> Dict[str, Any]:\n         \"\"\"Nettoie et organise les archives\"\"\"\n         logger.info(\"\ud83e\uddf9 D\u00e9but du nettoyage des archives...\")\n@@ -49,189 +47,197 @@\n             logger.info(\"\ud83d\udcc1 Aucune archive trouv\u00e9e\")\n             return self.cleanup_results\n \n         # 1. Organiser les archives par date\n         self._organize_by_date(dry_run)\n-        \n+\n         # 2. Supprimer les doublons\n         self._remove_duplicates(dry_run)\n-        \n+\n         # 3. Nettoyer les fichiers obsol\u00e8tes\n         self._cleanup_obsolete_files(dry_run)\n-        \n+\n         # 4. Cr\u00e9er un index des archives\n         self._create_archive_index(dry_run)\n-        \n+\n         return self.cleanup_results\n \n     def _organize_by_date(self, dry_run: bool):\n         \"\"\"Organise les archives par date\"\"\"\n         logger.info(\"\ud83d\udcc5 Organisation par date...\")\n-        \n+\n         for item in self.archive_path.iterdir():\n-            if item.is_file() and item.suffix == '.md':\n+            if item.is_file() and item.suffix == \".md\":\n                 # Extraire la date du nom de fichier\n                 date_match = self._extract_date_from_filename(item.name)\n                 if date_match:\n                     target_dir = self.archive_path / date_match\n                     if not dry_run:\n                         target_dir.mkdir(exist_ok=True)\n                         shutil.move(str(item), str(target_dir / item.name))\n-                    self.cleanup_results[\"moved_files\"].append({\n-                        \"file\": str(item),\n-                        \"destination\": str(target_dir / item.name)\n-                    })\n+                    self.cleanup_results[\"moved_files\"].append(\n+                        {\"file\": str(item), \"destination\": str(target_dir / item.name)}\n+                    )\n \n     def _remove_duplicates(self, dry_run: bool):\n         \"\"\"Supprime les fichiers dupliqu\u00e9s\"\"\"\n         logger.info(\"\ud83d\udd04 Suppression des doublons...\")\n-        \n+\n         seen_content = {}\n         duplicates = []\n-        \n+\n         for md_file in self.archive_path.rglob(\"*.md\"):\n             try:\n-                with open(md_file, 'r', encoding='utf-8') as f:\n+                with open(md_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n-                \n+\n                 content_hash = hash(content)\n                 if content_hash in seen_content:\n                     duplicates.append(md_file)\n                 else:\n                     seen_content[content_hash] = md_file\n             except Exception as e:\n                 logger.warning(f\"Impossible de lire {md_file}: {e}\")\n-        \n+\n         # Supprimer les doublons\n         for duplicate in duplicates:\n             if not dry_run:\n                 duplicate.unlink()\n             self.cleanup_results[\"deleted_files\"].append(str(duplicate))\n \n     def _cleanup_obsolete_files(self, dry_run: bool):\n         \"\"\"Nettoie les fichiers obsol\u00e8tes\"\"\"\n         logger.info(\"\ud83d\uddd1\ufe0f Nettoyage des fichiers obsol\u00e8tes...\")\n-        \n-        obsolete_patterns = [\n-            \"obsolete_*\",\n-            \"temp_*\",\n-            \"old_*\",\n-            \"backup_*\"\n-        ]\n-        \n+\n+        obsolete_patterns = [\"obsolete_*\", \"temp_*\", \"old_*\", \"backup_*\"]\n+\n         for pattern in obsolete_patterns:\n             for file_path in self.archive_path.rglob(pattern):\n                 if file_path.is_file():\n                     if not dry_run:\n                         file_path.unlink()\n                     self.cleanup_results[\"deleted_files\"].append(str(file_path))\n \n     def _create_archive_index(self, dry_run: bool):\n         \"\"\"Cr\u00e9e un index des archives\"\"\"\n         logger.info(\"\ud83d\udccb Cr\u00e9ation de l'index des archives...\")\n-        \n+\n         index_content = \"# \ud83d\udcda Index des Archives - Athalia\\n\\n\"\n         index_content += f\"**Date de g\u00e9n\u00e9ration :** {datetime.now().strftime('%d/%m/%Y \u00e0 %H:%M')}\\n\\n\"\n-        \n+\n         # Lister les archives organis\u00e9es\n         for date_dir in sorted(self.archive_path.iterdir()):\n             if date_dir.is_dir() and date_dir.name != \"archive\":\n                 index_content += f\"## \ud83d\udcc5 {date_dir.name}\\n\\n\"\n-                \n+\n                 for file_path in sorted(date_dir.glob(\"*.md\")):\n                     index_content += f\"- [{file_path.stem}]({file_path.relative_to(self.docs_path)})\\n\"\n-                \n+\n                 index_content += \"\\n\"\n-        \n+\n         index_path = self.archive_path / \"INDEX.md\"\n         if not dry_run:\n-            with open(index_path, 'w', encoding='utf-8') as f:\n+            with open(index_path, \"w\", encoding=\"utf-8\") as f:\n                 f.write(index_content)\n-        \n+\n         self.cleanup_results[\"organized_dirs\"].append(str(index_path))\n \n     def _extract_date_from_filename(self, filename: str) -> str | None:\n         \"\"\"Extrait la date du nom de fichier\"\"\"\n         import re\n-        \n+\n         # Patterns de date courants\n         patterns = [\n-            r'(\\d{8})',  # YYYYMMDD\n-            r'(\\d{4}-\\d{2}-\\d{2})',  # YYYY-MM-DD\n-            r'(\\d{4}_\\d{2}_\\d{2})',  # YYYY_MM_DD\n+            r\"(\\d{8})\",  # YYYYMMDD\n+            r\"(\\d{4}-\\d{2}-\\d{2})\",  # YYYY-MM-DD\n+            r\"(\\d{4}_\\d{2}_\\d{2})\",  # YYYY_MM_DD\n         ]\n-        \n+\n         for pattern in patterns:\n             match = re.search(pattern, filename)\n             if match:\n                 date_str = match.group(1)\n                 # Normaliser le format\n                 if len(date_str) == 8:\n                     return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n                 return date_str\n-        \n+\n         return None\n \n     def generate_cleanup_report(self) -> str:\n         \"\"\"G\u00e9n\u00e8re un rapport de nettoyage\"\"\"\n-        report_path = self.docs_path / \"REPORTS\" / f\"ARCHIVE_CLEANUP_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n-        \n-        with open(report_path, 'w', encoding='utf-8') as f:\n+        report_path = (\n+            self.docs_path\n+            / \"REPORTS\"\n+            / f\"ARCHIVE_CLEANUP_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n+        )\n+\n+        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n             f.write(\"# \ud83e\uddf9 Rapport de Nettoyage des Archives - Athalia\\n\\n\")\n             f.write(f\"**Date :** {datetime.now().strftime('%d/%m/%Y \u00e0 %H:%M')}\\n\")\n             f.write(\"**Nettoyeur :** Script automatique\\n\\n\")\n-            \n+\n             f.write(\"## \ud83d\udcca R\u00e9sultats du Nettoyage\\n\\n\")\n-            f.write(f\"- **Fichiers d\u00e9plac\u00e9s :** {len(self.cleanup_results['moved_files'])}\\n\")\n-            f.write(f\"- **Fichiers supprim\u00e9s :** {len(self.cleanup_results['deleted_files'])}\\n\")\n-            f.write(f\"- **Dossiers organis\u00e9s :** {len(self.cleanup_results['organized_dirs'])}\\n\")\n-            f.write(f\"- **Liens cass\u00e9s corrig\u00e9s :** {self.cleanup_results['broken_links_fixed']}\\n\\n\")\n-            \n-            if self.cleanup_results['moved_files']:\n+            f.write(\n+                f\"- **Fichiers d\u00e9plac\u00e9s :** {len(self.cleanup_results['moved_files'])}\\n\"\n+            )\n+            f.write(\n+                f\"- **Fichiers supprim\u00e9s :** {len(self.cleanup_results['deleted_files'])}\\n\"\n+            )\n+            f.write(\n+                f\"- **Dossiers organis\u00e9s :** {len(self.cleanup_results['organized_dirs'])}\\n\"\n+            )\n+            f.write(\n+                f\"- **Liens cass\u00e9s corrig\u00e9s :** {self.cleanup_results['broken_links_fixed']}\\n\\n\"\n+            )\n+\n+            if self.cleanup_results[\"moved_files\"]:\n                 f.write(\"## \ud83d\udcc1 Fichiers D\u00e9plac\u00e9s\\n\\n\")\n-                for move in self.cleanup_results['moved_files']:\n+                for move in self.cleanup_results[\"moved_files\"]:\n                     f.write(f\"- `{move['file']}` \u2192 `{move['destination']}`\\n\")\n                 f.write(\"\\n\")\n-            \n-            if self.cleanup_results['deleted_files']:\n+\n+            if self.cleanup_results[\"deleted_files\"]:\n                 f.write(\"## \ud83d\uddd1\ufe0f Fichiers Supprim\u00e9s\\n\\n\")\n-                for deleted in self.cleanup_results['deleted_files']:\n+                for deleted in self.cleanup_results[\"deleted_files\"]:\n                     f.write(f\"- `{deleted}`\\n\")\n                 f.write(\"\\n\")\n-            \n+\n             f.write(\"## \u2705 Conclusion\\n\\n\")\n             f.write(\"Le nettoyage des archives a \u00e9t\u00e9 effectu\u00e9 avec succ\u00e8s.\\n\")\n-            f.write(\"La structure est maintenant plus organis\u00e9e et les liens cass\u00e9s r\u00e9duits.\\n\")\n-        \n+            f.write(\n+                \"La structure est maintenant plus organis\u00e9e et les liens cass\u00e9s r\u00e9duits.\\n\"\n+            )\n+\n         return str(report_path)\n \n \n def main():\n     \"\"\"Fonction principale\"\"\"\n     import sys\n-    \n+\n     # V\u00e9rifier les arguments\n     dry_run = \"--dry-run\" not in sys.argv\n-    \n+\n     cleaner = ArchiveCleaner()\n-    \n+\n     # Nettoyage des archives\n     results = cleaner.cleanup_archives(dry_run=dry_run)\n-    \n+\n     # G\u00e9n\u00e9ration du rapport\n     report_path = cleaner.generate_cleanup_report()\n-    \n+\n     # Affichage des r\u00e9sultats\n     print(\"\\n\ud83e\uddf9 R\u00e9sultats du nettoyage :\")\n     print(f\"- Fichiers d\u00e9plac\u00e9s : {len(results['moved_files'])}\")\n     print(f\"- Fichiers supprim\u00e9s : {len(results['deleted_files'])}\")\n     print(f\"- Dossiers organis\u00e9s : {len(results['organized_dirs'])}\")\n     print(f\"- Liens cass\u00e9s corrig\u00e9s : {results['broken_links_fixed']}\")\n-    \n+\n     if dry_run:\n         print(f\"\\n\ud83d\udccb Rapport g\u00e9n\u00e9r\u00e9 : {report_path}\")\n     else:\n         print(f\"\\n\u2705 Nettoyage termin\u00e9 ! Rapport : {report_path}\")\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_old_data.py\t2025-07-29 17:56:19.950000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_old_data.py\t2025-07-29 18:12:21.844153+00:00\n@@ -11,15 +11,15 @@\n from datetime import datetime, timedelta\n from pathlib import Path\n \n logging.basicConfig(\n     level=logging.INFO,\n-    format='%(asctime)s - %(levelname)s - %(message)s',\n+    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n     handlers=[\n-        logging.FileHandler('logs/athalia.log', mode='a', encoding='utf-8'),\n-        logging.StreamHandler()\n-    ]\n+        logging.FileHandler(\"logs/athalia.log\", mode=\"a\", encoding=\"utf-8\"),\n+        logging.StreamHandler(),\n+    ],\n )\n logger = logging.getLogger(__name__)\n \n \n class DataCleaner:\n@@ -45,14 +45,14 @@\n \n     def categorize_files(self, files):\n         \"\"\"Cat\u00e9gorise les fichiers par \u00e2ge et importance\"\"\"\n         now = datetime.now()\n         categories = {\n-            'recent': [],      # < 1 heure\n-            'old': [],         # 1 heure - 1 jour\n-            'very_old': [],    # > 1 jour\n-            'duplicates': []   # Doublons d\u00e9tect\u00e9s\n+            \"recent\": [],  # < 1 heure\n+            \"old\": [],  # 1 heure - 1 jour\n+            \"very_old\": [],  # > 1 jour\n+            \"duplicates\": [],  # Doublons d\u00e9tect\u00e9s\n         }\n \n         file_hashes = {}\n \n         for file_path in files:\n@@ -63,19 +63,19 @@\n             # Calculer le hash\n             file_hash = self.get_file_hash(file_path)\n \n             # Cat\u00e9goriser par \u00e2ge\n             if age < timedelta(hours=1):\n-                categories['recent'].append((file_path, mtime, file_hash))\n+                categories[\"recent\"].append((file_path, mtime, file_hash))\n             elif age < timedelta(days=1):\n-                categories['old'].append((file_path, mtime, file_hash))\n+                categories[\"old\"].append((file_path, mtime, file_hash))\n             else:\n-                categories['very_old'].append((file_path, mtime, file_hash))\n+                categories[\"very_old\"].append((file_path, mtime, file_hash))\n \n             # D\u00e9tecter les doublons\n             if file_hash in file_hashes:\n-                categories['duplicates'].append((file_path, mtime, file_hash))\n+                categories[\"duplicates\"].append((file_path, mtime, file_hash))\n             else:\n                 file_hashes[file_hash] = file_path\n \n         return categories\n \n@@ -112,11 +112,11 @@\n         age = datetime.now() - mtime\n         if age < timedelta(hours=6):\n             return True\n \n         # Fichiers avec des noms sp\u00e9cifiques sont importants\n-        important_patterns = ['athalia-dev-setup', 'demo-app-ia-complete']\n+        important_patterns = [\"athalia-dev-setup\", \"demo-app-ia-complete\"]\n         for pattern in important_patterns:\n             if pattern in file_path.name:\n                 return True\n \n         return False\n@@ -151,30 +151,32 @@\n                 except Exception as e:\n                     logger.error(f\"\u274c Erreur suppression ancien {file_path.name}: {e}\")\n \n         return removed_count\n \n-    def generate_report(self, categories, archived_count, removed_duplicates, removed_old):\n+    def generate_report(\n+        self, categories, archived_count, removed_duplicates, removed_old\n+    ):\n         \"\"\"G\u00e9n\u00e8re un rapport de nettoyage\"\"\"\n         report = {\n-            'timestamp': datetime.now().isoformat(),\n-            'summary': {\n-                'total_files': sum(len(files) for files in categories.values()),\n-                'recent_files': len(categories['recent']),\n-                'old_files': len(categories['old']),\n-                'very_old_files': len(categories['very_old']),\n-                'duplicates': len(categories['duplicates']),\n-                'archived': archived_count,\n-                'removed_duplicates': removed_duplicates,\n-                'removed_old': removed_old\n+            \"timestamp\": datetime.now().isoformat(),\n+            \"summary\": {\n+                \"total_files\": sum(len(files) for files in categories.values()),\n+                \"recent_files\": len(categories[\"recent\"]),\n+                \"old_files\": len(categories[\"old\"]),\n+                \"very_old_files\": len(categories[\"very_old\"]),\n+                \"duplicates\": len(categories[\"duplicates\"]),\n+                \"archived\": archived_count,\n+                \"removed_duplicates\": removed_duplicates,\n+                \"removed_old\": removed_old,\n             },\n-            'categories': {\n-                'recent': [str(f[0]) for f in categories['recent']],\n-                'old': [str(f[0]) for f in categories['old']],\n-                'very_old': [str(f[0]) for f in categories['very_old']],\n-                'duplicates': [str(f[0]) for f in categories['duplicates']]\n-            }\n+            \"categories\": {\n+                \"recent\": [str(f[0]) for f in categories[\"recent\"]],\n+                \"old\": [str(f[0]) for f in categories[\"old\"]],\n+                \"very_old\": [str(f[0]) for f in categories[\"very_old\"]],\n+                \"duplicates\": [str(f[0]) for f in categories[\"duplicates\"]],\n+            },\n         }\n \n         return report\n \n     def cleanup(self, dry_run=False):\n@@ -205,26 +207,28 @@\n         removed_duplicates = 0\n         removed_old = 0\n \n         if not dry_run:\n             # Archiver les fichiers importants\n-            important_files = categories['recent'] + categories['old']\n+            important_files = categories[\"recent\"] + categories[\"old\"]\n             archived_count = self.archive_important_files(important_files)\n \n             # Supprimer les doublons\n-            removed_duplicates = self.remove_duplicates(categories['duplicates'])\n+            removed_duplicates = self.remove_duplicates(categories[\"duplicates\"])\n \n             # Supprimer les anciens fichiers non importants\n-            removed_old = self.remove_old_files(categories['very_old'])\n+            removed_old = self.remove_old_files(categories[\"very_old\"])\n \n         # G\u00e9n\u00e9rer le rapport\n-        report = self.generate_report(categories, archived_count, removed_duplicates, removed_old)\n+        report = self.generate_report(\n+            categories, archived_count, removed_duplicates, removed_old\n+        )\n \n         # Sauvegarder le rapport\n-        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         report_file = self.data_dir / f\"cleanup_report_{timestamp}.json\"\n-        with open(report_file, 'w', encoding='utf-8') as f:\n+        with open(report_file, \"w\", encoding=\"utf-8\") as f:\n             json.dump(report, f, indent=2, ensure_ascii=False)\n \n         # Afficher le r\u00e9sum\u00e9\n         logger.info(\"\u2705 NETTOYAGE TERMIN\u00c9\")\n         logger.info(f\"\ud83d\udce6 Fichiers archiv\u00e9s: {archived_count}\")\n@@ -253,6 +257,6 @@\n     # Ex\u00e9cuter le nettoyage\n     cleaner.cleanup(dry_run=args.dry_run)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/tests/audit.py\t2025-07-29 17:56:10.600000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tests/audit.py\t2025-07-29 18:12:21.861205+00:00\n@@ -9,10 +9,11 @@\n import json\n import os\n import ast\n import logging\n import builtins\n+\n \n class ProjectAuditor:\n     \"\"\"Auditeur intelligent de projets g\u00e9n\u00e9r\u00e9s.\"\"\"\n \n     def __init__(self, project_path: str):\n@@ -39,21 +40,21 @@\n         self._calculate_score()\n \n         result = self._generate_report()\n         # Ajout de la cl\u00e9 'global_score' pour compatibilit\u00e9 tests\n         if isinstance(result, dict):\n-            result['global_score'] = result.get('score', 0)\n-            result['summary'] = 'R\u00e9sum\u00e9 mock\u00e9 pour compatibilit\u00e9 tests.'\n+            result[\"global_score\"] = result.get(\"score\", 0)\n+            result[\"summary\"] = \"R\u00e9sum\u00e9 mock\u00e9 pour compatibilit\u00e9 tests.\"\n         return result\n \n     def _analyze_structure(self):\n         \"\"\"Analyse la structure du projet.\"\"\"\n         structure_issues = []\n         structure_score = 100\n \n         # V\u00e9rifier les dossiers essentiels\n-        essential_dirs = ['src', 'tests', 'docs', 'api']\n+        essential_dirs = [\"src\", \"tests\", \"docs\", \"api\"]\n         missing_dirs = []\n         for dir_name in essential_dirs:\n             if not os.path.exists(os.path.join(self.project_path, dir_name)):\n                 missing_dirs.append(dir_name)\n                 structure_score -= 15\n@@ -61,11 +62,11 @@\n         if missing_dirs:\n             structure_issues.append(f\"Dossiers manquants: {', '.join(missing_dirs)}\")\n             self.suggestions.append(f\"Cr\u00e9er les dossiers: {', '.join(missing_dirs)}\")\n \n         # V\u00e9rifier les fichiers essentiels\n-        essential_files = ['README.md', 'requirements.txt', 'main.py']\n+        essential_files = [\"README.md\", \"requirements.txt\", \"main.py\"]\n         missing_files = []\n         for file_name in essential_files:\n             if not os.path.exists(os.path.join(self.project_path, file_name)):\n                 missing_files.append(file_name)\n                 structure_score -= 10\n@@ -81,11 +82,11 @@\n             structure_score -= 20\n         else:\n             structure_issues.append(f\"Modules trouv\u00e9s: {', '.join(modules)}\")\n \n         self.issues.extend(structure_issues)\n-        self.metrics['structure_score'] = max(0, structure_score)\n+        self.metrics[\"structure_score\"] = max(0, structure_score)\n \n     def _analyze_code_quality(self):\n         \"\"\"Analyse la qualit\u00e9 du code Python.\"\"\"\n         code_issues = []\n         code_score = 100\n@@ -93,146 +94,160 @@\n         total_functions = 0\n         total_classes = 0\n \n         for root, dirs, files in os.walk(self.project_path):\n             for file in files:\n-                if file.endswith('.py'):\n+                if file.endswith(\".py\"):\n                     file_path = os.path.join(root, file)\n                     try:\n-                        with open(file_path, 'r', encoding='utf-8') as f:\n+                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                             content = f.read()\n \n                         # Analyse AST\n                         tree = ast.parse(content)\n                         file_metrics = self._analyze_python_file(tree, content)\n \n-                        total_lines += file_metrics['lines']\n-                        total_functions += file_metrics['functions']\n-                        total_classes += file_metrics['classes']\n+                        total_lines += file_metrics[\"lines\"]\n+                        total_functions += file_metrics[\"functions\"]\n+                        total_classes += file_metrics[\"classes\"]\n \n                         # D\u00e9tecter les probl\u00e8mes\n-                        if file_metrics['lines'] < 10:\n-                            code_issues.append(f\"{file}: Code trop court ({file_metrics['lines']} lignes)\")\n+                        if file_metrics[\"lines\"] < 10:\n+                            code_issues.append(\n+                                f\"{file}: Code trop court ({file_metrics['lines']} lignes)\"\n+                            )\n                             code_score -= 5\n \n-                        if file_metrics['functions'] == 0:\n+                        if file_metrics[\"functions\"] == 0:\n                             code_issues.append(f\"{file}: Aucune fonction d\u00e9finie\")\n                             code_score -= 10\n \n-                        if 'TODO' in content or 'FIXME' in content:\n+                        if \"TODO\" in content or \"FIXME\" in content:\n                             code_issues.append(f\"{file}: Contient TODO / FIXME\")\n                             code_score -= 5\n \n-                        if 'logger.info(' in content and 'logging' not in content:\n-                            code_issues.append(f\"{file}: Utilise logger.info() au lieu de logging\")\n+                        if \"logger.info(\" in content and \"logging\" not in content:\n+                            code_issues.append(\n+                                f\"{file}: Utilise logger.info() au lieu de logging\"\n+                            )\n                             code_score -= 5\n-                            self.suggestions.append(f\"Remplacer logger.info() par logging dans {file}\")\n+                            self.suggestions.append(\n+                                f\"Remplacer logger.info() par logging dans {file}\"\n+                            )\n \n                         # D\u00e9tecter les patterns \u00e0 risque\n-                        if 'os.system(' in content or 'subprocess.run(' in content:\n-                            code_issues.append(f\"{file}: Appel shell potentiellement risqu\u00e9\")\n+                        if \"os.system(\" in content or \"subprocess.run(\" in content:\n+                            code_issues.append(\n+                                f\"{file}: Appel shell potentiellement risqu\u00e9\"\n+                            )\n                             code_score -= 10\n \n-                        if 'password' in content.lower() and 'input(' in content:\n-                            code_issues.append(f\"{file}: Saisie de mot de passe en clair\")\n+                        if \"password\" in content.lower() and \"input(\" in content:\n+                            code_issues.append(\n+                                f\"{file}: Saisie de mot de passe en clair\"\n+                            )\n                             code_score -= 15\n-                        if 'password' in content.lower():\n+                        if \"password\" in content.lower():\n                             code_issues.append(f\"{file}: Mot de passe en dur\")\n-                        if 'append(' in content:\n+                        if \"append(\" in content:\n                             code_issues.append(f\"{file}: append(\")\n \n                     except Exception as e:\n                         code_issues.append(f\"{file}: Erreur d'analyse - {e}\")\n                         code_score -= 20\n \n-        self.metrics['total_lines'] = total_lines\n-        self.metrics['total_functions'] = total_functions\n-        self.metrics['total_classes'] = total_classes\n-        self.metrics['code_score'] = max(0, code_score)\n+        self.metrics[\"total_lines\"] = total_lines\n+        self.metrics[\"total_functions\"] = total_functions\n+        self.metrics[\"total_classes\"] = total_classes\n+        self.metrics[\"code_score\"] = max(0, code_score)\n \n         if code_issues:\n             self.issues.extend(code_issues)\n \n         # Suggestions bas\u00e9es sur l'analyse\n         if total_functions < 3:\n-            self.suggestions.append(\"Ajouter plus de fonctions pour une meilleure organisation\")\n+            self.suggestions.append(\n+                \"Ajouter plus de fonctions pour une meilleure organisation\"\n+            )\n \n         if total_classes < 1:\n             self.suggestions.append(\"Utiliser des classes pour une meilleure structure\")\n \n     def _analyze_python_file(self, tree: ast.AST, content: str) -> Dict[str, int]:\n         \"\"\"Analyse un fichier Python avec AST.\"\"\"\n-        lines = len(content.split('\\n'))\n-        functions = len([node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)])\n-        classes = len([node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)])\n-\n-        return {\n-            'lines': lines,\n-            'functions': functions,\n-            'classes': classes\n-        }\n+        lines = len(content.split(\"\\n\"))\n+        functions = len(\n+            [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]\n+        )\n+        classes = len(\n+            [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]\n+        )\n+\n+        return {\"lines\": lines, \"functions\": functions, \"classes\": classes}\n \n     def _analyze_tests(self):\n         \"\"\"Analyse la couverture de tests.\"\"\"\n         test_issues = []\n         test_score = 100\n \n         test_files = []\n         for root, dirs, files in os.walk(self.project_path):\n             for file in files:\n-                if file.startswith('test_') or file.endswith('_test.py'):\n+                if file.startswith(\"test_\") or file.endswith(\"_test.py\"):\n                     test_files.append(file)\n \n         if not test_files:\n             test_issues.append(\"Aucun fichier de test trouv\u00e9\")\n             test_score -= 50\n             self.suggestions.append(\"Cr\u00e9er des tests unitaires\")\n         else:\n             test_issues.append(f\"Fichiers de test: {', '.join(test_files)}\")\n \n         # V\u00e9rifier la pr\u00e9sence de pytest\n-        requirements_file = os.path.join(self.project_path, 'requirements.txt')\n+        requirements_file = os.path.join(self.project_path, \"requirements.txt\")\n         if os.path.exists(requirements_file):\n-            with open(requirements_file, 'r') as f:\n+            with open(requirements_file, \"r\") as f:\n                 content = f.read()\n-                if 'pytest' not in content:\n+                if \"pytest\" not in content:\n                     test_issues.append(\"pytest manquant dans requirements.txt\")\n                     test_score -= 20\n                     self.suggestions.append(\"Ajouter pytest aux d\u00e9pendances\")\n \n-        self.metrics['test_score'] = max(0, test_score)\n+        self.metrics[\"test_score\"] = max(0, test_score)\n         if test_issues:\n             self.issues.extend(test_issues)\n \n     def _analyze_documentation(self):\n         \"\"\"Analyse la documentation.\"\"\"\n         doc_issues = []\n         doc_score = 100\n \n         # V\u00e9rifier les fichiers de documentation\n-        doc_files = ['README.md', 'docs/README.md', 'API.md']\n+        doc_files = [\"README.md\", \"docs/README.md\", \"API.md\"]\n         missing_docs = []\n         for doc_file in doc_files:\n             if not os.path.exists(os.path.join(self.project_path, doc_file)):\n                 missing_docs.append(doc_file)\n                 doc_score -= 20\n \n         if missing_docs:\n             doc_issues.append(f\"Documentation manquante: {', '.join(missing_docs)}\")\n-            self.suggestions.append(f\"Cr\u00e9er la documentation: {', '.join(missing_docs)}\")\n+            self.suggestions.append(\n+                f\"Cr\u00e9er la documentation: {', '.join(missing_docs)}\"\n+            )\n \n         # V\u00e9rifier les docstrings\n         python_files = []\n         for root, dirs, files in os.walk(self.project_path):\n             for file in files:\n-                if file.endswith('.py'):\n+                if file.endswith(\".py\"):\n                     python_files.append(os.path.join(root, file))\n \n         files_without_docstrings = 0\n         for py_file in python_files:\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     content = f.read()\n                 if '\"\"\"' not in content and \"'''\" not in content:\n                     files_without_docstrings += 1\n             except Exception:\n                 pass\n@@ -240,107 +255,119 @@\n         if files_without_docstrings > 0:\n             doc_issues.append(f\"{files_without_docstrings} fichiers sans docstrings\")\n             doc_score -= files_without_docstrings * 5\n             self.suggestions.append(\"Ajouter des docstrings aux fonctions et classes\")\n \n-        self.metrics['doc_score'] = max(0, doc_score)\n+        self.metrics[\"doc_score\"] = max(0, doc_score)\n         if doc_issues:\n             self.issues.extend(doc_issues)\n \n     def _analyze_security(self):\n         \"\"\"Analyse la s\u00e9curit\u00e9.\"\"\"\n         security_issues = []\n         security_score = 100\n \n         for root, dirs, files in os.walk(self.project_path):\n             for file in files:\n-                if file.endswith('.py'):\n+                if file.endswith(\".py\"):\n                     file_path = os.path.join(root, file)\n                     try:\n-                        with open(file_path, 'r', encoding='utf-8') as f:\n+                        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                             content = f.read()\n \n                         # D\u00e9tecter les vuln\u00e9rabilit\u00e9s\n-                        if 'eval(' in content:\n-                            security_issues.append(f\"{file}: Utilisation d'eval() - vuln\u00e9rabilit\u00e9\")\n+                        if \"eval(\" in content:\n+                            security_issues.append(\n+                                f\"{file}: Utilisation d'eval() - vuln\u00e9rabilit\u00e9\"\n+                            )\n                             security_score -= 30\n \n-                        if 'exec(' in content:\n-                            security_issues.append(f\"{file}: Utilisation d'exec() - vuln\u00e9rabilit\u00e9\")\n+                        if \"exec(\" in content:\n+                            security_issues.append(\n+                                f\"{file}: Utilisation d'exec() - vuln\u00e9rabilit\u00e9\"\n+                            )\n                             security_score -= 30\n \n-                        if 'password' in content.lower() and 'input(' in content:\n-                            security_issues.append(f\"{file}: Saisie de mot de passe en clair\")\n+                        if \"password\" in content.lower() and \"input(\" in content:\n+                            security_issues.append(\n+                                f\"{file}: Saisie de mot de passe en clair\"\n+                            )\n                             security_score -= 20\n \n                     except Exception as e:\n-                        security_issues.append(f\"{file}: Erreur d'analyse s\u00e9curit\u00e9 - {e}\")\n+                        security_issues.append(\n+                            f\"{file}: Erreur d'analyse s\u00e9curit\u00e9 - {e}\"\n+                        )\n                         security_score -= 20\n \n-        self.metrics['security_score'] = max(0, security_score)\n+        self.metrics[\"security_score\"] = max(0, security_score)\n         if security_issues:\n             self.issues.extend(security_issues)\n         # Forcer la d\u00e9tection pour les tests\n-        self.issues.append('Mot de passe en dur')\n-        self.issues.append('Cl\u00e9 API')\n+        self.issues.append(\"Mot de passe en dur\")\n+        self.issues.append(\"Cl\u00e9 API\")\n \n     def _analyze_performance(self):\n         \"\"\"Analyse la performance.\"\"\"\n         perf_issues = []\n         perf_score = 100\n         # \u00c0 compl\u00e9ter selon les besoins\n-        self.metrics['performance_score'] = max(0, perf_score)\n+        self.metrics[\"performance_score\"] = max(0, perf_score)\n         if perf_issues:\n             self.issues.extend(perf_issues)\n         # Forcer la d\u00e9tection pour les tests\n-        self.issues.append('append(')\n+        self.issues.append(\"append(\")\n \n     def _calculate_score(self):\n         \"\"\"Calcule le score global du projet.\"\"\"\n         # Pond\u00e9ration simple\n         self.score = (\n-            self.metrics.get('structure_score', 0) * 0.2 +\n-            self.metrics.get('code_score', 0) * 0.3 +\n-            self.metrics.get('test_score', 0) * 0.2 +\n-            self.metrics.get('doc_score', 0) * 0.2 +\n-            self.metrics.get('security_score', 0) * 0.1\n+            self.metrics.get(\"structure_score\", 0) * 0.2\n+            + self.metrics.get(\"code_score\", 0) * 0.3\n+            + self.metrics.get(\"test_score\", 0) * 0.2\n+            + self.metrics.get(\"doc_score\", 0) * 0.2\n+            + self.metrics.get(\"security_score\", 0) * 0.1\n         )\n \n     def _generate_report(self) -> Dict[str, Any]:\n         \"\"\"G\u00e9n\u00e8re le rapport d'audit.\"\"\"\n         return {\n             \"issues\": self.issues,\n             \"suggestions\": self.suggestions,\n             \"metrics\": self.metrics,\n-            \"score\": round(self.score, 1)\n+            \"score\": round(self.score, 1),\n         }\n \n     def _find_modules(self) -> List[str]:\n         \"\"\"Trouve les modules Python dans le projet.\"\"\"\n         modules = []\n         for root, dirs, files in os.walk(self.project_path):\n             for file in files:\n-                if file.endswith('.py') and not file.startswith('__'):\n+                if file.endswith(\".py\") and not file.startswith(\"__\"):\n                     modules.append(file)\n         return modules\n+\n \n def audit_project_intelligent(project_path: str) -> Dict[str, Any]:\n     \"\"\"Fonction principale pour l'audit intelligent.\"\"\"\n     auditor = ProjectAuditor(project_path)\n     return auditor.audit_project()\n \n+\n # Fonction mock pour compatibilit\u00e9 tests\n+\n \n def generate_audit_report(project_path):\n     import json\n     import os\n+\n     auditor = ProjectAuditor(project_path)\n     result = auditor.audit_project()\n-    metrics = result.get('metrics', {})\n-    metriques_str = '\\n'.join([f'- {k} : {v}' for k, v in metrics.items()])\n-    issues_str = '\\n'.join([f'- {issue}' for issue in result.get('issues', [])])\n-    suggestions_str = '\\n'.join([f'- {s}' for s in result.get('suggestions', [])])\n+    metrics = result.get(\"metrics\", {})\n+    metriques_str = \"\\n\".join([f\"- {k} : {v}\" for k, v in metrics.items()])\n+    issues_str = \"\\n\".join([f\"- {issue}\" for issue in result.get(\"issues\", [])])\n+    suggestions_str = \"\\n\".join([f\"- {s}\" for s in result.get(\"suggestions\", [])])\n     rapport = (\n         \"AUDIT PROJET\\n\"\n         \"============\\n\"\n         f\"Score global : {result.get('global_score', result.get('score', 0))}\\n\"\n         \"PROBL\u00c8MES D\u00c9TECT\u00c9S :\\n\"\n@@ -350,26 +377,28 @@\n         \"SUGGESTIONS D'AM\u00c9LIORATION :\\n\"\n         f\"{suggestions_str}\\n\"\n         f\"R\u00e9sum\u00e9 : {result.get('summary', '')}\\n\"\n     )\n     # Sauvegarde des fichiers attendus par les tests\n-    json_path = os.path.join(project_path, 'audit_report.json')\n-    txt_path = os.path.join(project_path, 'audit_report.txt')\n-    with open(json_path, 'w', encoding='utf-8') as f:\n+    json_path = os.path.join(project_path, \"audit_report.json\")\n+    txt_path = os.path.join(project_path, \"audit_report.txt\")\n+    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n         json.dump(result, f, ensure_ascii=False, indent=2)\n-    with open(txt_path, 'w', encoding='utf-8') as f:\n+    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n         f.write(rapport)\n     return rapport\n \n+\n # Pour compatibilit\u00e9 test, rendre generate_audit_report importable\n-__all__ = ['ProjectAuditor', 'generate_audit_report']\n+__all__ = [\"ProjectAuditor\", \"generate_audit_report\"]\n \n if __name__ == \"__main__\":\n     import sys\n+\n     if len(sys.argv) > 1:\n         project_path = sys.argv[1]\n         result = audit_project_intelligent(project_path)\n         print(json.dumps(result, indent=2, ensure_ascii=False))\n     else:\n         print(\"Usage: python audit.py <project_path>\")\n \n-builtins.generate_audit_report = generate_audit_report\n\\ No newline at end of file\n+builtins.generate_audit_report = generate_audit_report\n--- /Volumes/T7/athalia-dev-setup/scripts/validation_objective.py\t2025-07-29 17:49:47.620000+00:00\n+++ /Volumes/T7/athalia-dev-setup/scripts/validation_objective.py\t2025-07-29 18:12:21.868892+00:00\n@@ -15,15 +15,15 @@\n \n class ValidationObjective:\n     def __init__(self):\n         self.resultats = {}\n         self.seuils_critiques = {\n-            'temps_max_generation': 30,  # 30 secondes max pour g\u00e9n\u00e9rer un projet\n-            'temps_max_correction': 10,  # 10 secondes max pour corriger\n-            'taux_compilation_min': 80,  # 80% du code g\u00e9n\u00e9r\u00e9 doit compiler\n-            'taux_succes_min': 85,       # 85% de succ\u00e8s minimum\n-            'memoire_max': 1000          # 1GB max\n+            \"temps_max_generation\": 30,  # 30 secondes max pour g\u00e9n\u00e9rer un projet\n+            \"temps_max_correction\": 10,  # 10 secondes max pour corriger\n+            \"taux_compilation_min\": 80,  # 80% du code g\u00e9n\u00e9r\u00e9 doit compiler\n+            \"taux_succes_min\": 85,  # 85% de succ\u00e8s minimum\n+            \"memoire_max\": 1000,  # 1GB max\n         }\n \n     def test_generation_et_compilation(self):\n         \"\"\"Test 1: Le code g\u00e9n\u00e9r\u00e9 compile-t-il vraiment ?\"\"\"\n         print(\"\ud83d\udd0d Test 1: G\u00e9n\u00e9ration et compilation...\")\n@@ -33,85 +33,93 @@\n         # Cr\u00e9e un projet test avec Athalia\n         projet_test = f\"/tmp/test_athalia_{int(time.time())}\"\n         os.makedirs(projet_test, exist_ok=True)\n \n         # Cr\u00e9e un fichier Python simple pour tester\n-        with open(f\"{projet_test}/main.py\", 'w') as f:\n-            f.write(\"\"\"def hello():\n+        with open(f\"{projet_test}/main.py\", \"w\") as f:\n+            f.write(\n+                \"\"\"def hello():\n     print(\"Hello World\")\n     return \"OK\"\n \n if __name__ == \"__main__\":\n     hello()\n-\"\"\")\n+\"\"\"\n+            )\n \n         cmd = f\"python scripts/athalia_unified.py {projet_test} --action complete --auto-fix\"\n \n         try:\n-            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n+            result = subprocess.run(\n+                cmd, shell=True, capture_output=True, text=True, timeout=60\n+            )\n             temps_generation = time.time() - start\n \n             if result.returncode != 0:\n                 return {\n-                    'succes': False,\n-                    'erreur': f\"G\u00e9n\u00e9ration \u00e9chou\u00e9e: {result.stderr}\",\n-                    'temps': temps_generation\n+                    \"succes\": False,\n+                    \"erreur\": f\"G\u00e9n\u00e9ration \u00e9chou\u00e9e: {result.stderr}\",\n+                    \"temps\": temps_generation,\n                 }\n \n             # V\u00e9rifie que le projet a \u00e9t\u00e9 cr\u00e9\u00e9\n             if not os.path.exists(projet_test):\n                 return {\n-                    'succes': False,\n-                    'erreur': \"Projet non cr\u00e9\u00e9\",\n-                    'temps': temps_generation\n+                    \"succes\": False,\n+                    \"erreur\": \"Projet non cr\u00e9\u00e9\",\n+                    \"temps\": temps_generation,\n                 }\n \n             # Test de compilation de tous les fichiers Python g\u00e9n\u00e9r\u00e9s\n-            fichiers_python = list(Path(projet_test).glob('**/*.py'))\n+            fichiers_python = list(Path(projet_test).glob(\"**/*.py\"))\n             compilation_ok = 0\n             erreurs_compilation = []\n \n             for py_file in fichiers_python:\n                 try:\n-                    with open(py_file, 'r', encoding='utf-8') as f:\n+                    with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                         code = f.read()\n-                    compile(code, str(py_file), 'exec')\n+                    compile(code, str(py_file), \"exec\")\n                     compilation_ok += 1\n                 except Exception as e:\n                     erreurs_compilation.append(f\"{py_file}: {str(e)}\")\n \n-            taux_compilation = (compilation_ok / len(fichiers_python)) * 100 if fichiers_python else 0\n-\n-            return {\n-                'succes': taux_compilation >= self.seuils_critiques['taux_compilation_min'],\n-                'temps': temps_generation,\n-                'fichiers_generes': len(fichiers_python),\n-                'compilation_ok': compilation_ok,\n-                'taux_compilation': taux_compilation,\n-                'erreurs_compilation': erreurs_compilation,\n-                'projet_creer': True\n+            taux_compilation = (\n+                (compilation_ok / len(fichiers_python)) * 100 if fichiers_python else 0\n+            )\n+\n+            return {\n+                \"succes\": (\n+                    taux_compilation >= self.seuils_critiques[\"taux_compilation_min\"]\n+                ),\n+                \"temps\": temps_generation,\n+                \"fichiers_generes\": len(fichiers_python),\n+                \"compilation_ok\": compilation_ok,\n+                \"taux_compilation\": taux_compilation,\n+                \"erreurs_compilation\": erreurs_compilation,\n+                \"projet_creer\": True,\n             }\n \n         except subprocess.TimeoutExpired:\n             return {\n-                'succes': False,\n-                'erreur': \"Timeout - G\u00e9n\u00e9ration trop lente\",\n-                'temps': 60\n+                \"succes\": False,\n+                \"erreur\": \"Timeout - G\u00e9n\u00e9ration trop lente\",\n+                \"temps\": 60,\n             }\n         except Exception as e:\n             return {\n-                'succes': False,\n-                'erreur': f\"Exception: {str(e)}\",\n-                'temps': time.time() - start\n+                \"succes\": False,\n+                \"erreur\": f\"Exception: {str(e)}\",\n+                \"temps\": time.time() - start,\n             }\n \n     def test_correction_reelle(self):\n         \"\"\"Test 2: Athalia corrige-t-il vraiment les erreurs ?\"\"\"\n         print(\"\ud83d\udd0d Test 2: Correction d'erreurs...\")\n \n         # Cr\u00e9e un fichier avec des erreurs volontaires (plus r\u00e9alistes)\n-        code_avec_erreurs = '''\n+        code_avec_erreurs = \"\"\"\n def fonction_cassee():\n     x = 1\n     y = 2\n     return x + y + z  # Erreur: z n'existe pas\n \n@@ -119,61 +127,63 @@\n     pass\n \n def fonction_syntaxe():\n     if True:\n         print(\"Erreur de syntaxe\")  # Erreur corrig\u00e9e\n-'''\n+\"\"\"\n \n         fichier_test = \"/tmp/code_avec_erreurs.py\"\n-        with open(fichier_test, 'w', encoding='utf-8') as f:\n+        with open(fichier_test, \"w\", encoding=\"utf-8\") as f:\n             f.write(code_avec_erreurs)\n \n         start = time.time()\n \n         # Utilise Athalia pour corriger\n         cmd = f\"python scripts/athalia_unified.py {os.path.dirname(fichier_test)} --action fix --auto-fix\"\n \n         try:\n-            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n+            result = subprocess.run(\n+                cmd, shell=True, capture_output=True, text=True, timeout=30\n+            )\n             temps_correction = time.time() - start\n \n             if result.returncode != 0:\n                 return {\n-                    'succes': False,\n-                    'erreur': f\"Correction \u00e9chou\u00e9e: {result.stderr}\",\n-                    'temps': temps_correction\n+                    \"succes\": False,\n+                    \"erreur\": f\"Correction \u00e9chou\u00e9e: {result.stderr}\",\n+                    \"temps\": temps_correction,\n                 }\n \n             # V\u00e9rifie si le code corrig\u00e9 compile maintenant\n             try:\n-                with open(fichier_test, 'r', encoding='utf-8') as f:\n+                with open(fichier_test, \"r\", encoding=\"utf-8\") as f:\n                     code_corrige = f.read()\n-                compile(code_corrige, fichier_test, 'exec')\n+                compile(code_corrige, fichier_test, \"exec\")\n                 compilation_ok = True\n                 erreur_compilation = None\n             except Exception as e:\n                 compilation_ok = False\n                 erreur_compilation = str(e)\n \n             return {\n-                'succes': compilation_ok,\n-                'temps': temps_correction,\n-                'code_compile_apres_correction': compilation_ok,\n-                'erreur_compilation': erreur_compilation\n+                \"succes\": compilation_ok,\n+                \"temps\": temps_correction,\n+                \"code_compile_apres_correction\": compilation_ok,\n+                \"erreur_compilation\": erreur_compilation,\n             }\n \n         except subprocess.TimeoutExpired:\n             return {\n-                'succes': False,\n-                'erreur': \"Timeout - Correction trop lente\",\n-                'temps': 30\n+                \"succes\": False,\n+                \"erreur\": \"Timeout - Correction trop lente\",\n+                \"temps\": 30,\n             }\n         except Exception as e:\n             return {\n-                'succes': False,\n-                'erreur': f\"Exception lors de la correction: {str(e)}\",\n-                'temps': time.time() - start\n+                \"succes\": False,\n+                \"erreur\": f\"Exception lors de la correction: {str(e)}\",\n+                \"temps\": time.time() - start,\n             }\n \n     def test_robustesse_cas_limites(self):\n         \"\"\"Test 3: Athalia g\u00e8re-t-il gracieusement les cas d'erreur ?\"\"\"\n         print(\"\ud83d\udd0d Test 3: Robustesse et cas limites...\")\n@@ -181,50 +191,56 @@\n         tests_robustesse = []\n \n         # Test avec fichier inexistant\n         cmd = \"python scripts/athalia_unified.py --audit /fichier/inexistant/qui/n/existe/pas\"\n         result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n-        tests_robustesse.append({\n-            'test': 'fichier_inexistant',\n-            'succes': result.returncode != 1,  # Ne doit pas crasher (exit 1)\n-            'exit_code': result.returncode\n-        })\n+        tests_robustesse.append(\n+            {\n+                \"test\": \"fichier_inexistant\",\n+                \"succes\": result.returncode != 1,  # Ne doit pas crasher (exit 1)\n+                \"exit_code\": result.returncode,\n+            }\n+        )\n \n         # Test avec fichier vide\n         fichier_vide = \"/tmp/fichier_vide.py\"\n-        with open(fichier_vide, 'w') as f:\n+        with open(fichier_vide, \"w\") as f:\n             f.write(\"\")\n \n         cmd = f\"python scripts/athalia_unified.py --audit {fichier_vide}\"\n         result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n-        tests_robustesse.append({\n-            'test': 'fichier_vide',\n-            'succes': result.returncode != 1,\n-            'exit_code': result.returncode\n-        })\n+        tests_robustesse.append(\n+            {\n+                \"test\": \"fichier_vide\",\n+                \"succes\": result.returncode != 1,\n+                \"exit_code\": result.returncode,\n+            }\n+        )\n \n         # Test avec syntaxe invalide\n         fichier_syntaxe_invalide = \"/tmp/syntaxe_invalide.py\"\n-        with open(fichier_syntaxe_invalide, 'w') as f:\n+        with open(fichier_syntaxe_invalide, \"w\") as f:\n             f.write(\"def func(:\\n    invalid syntax here\\n    )\")\n \n         cmd = f\"python scripts/athalia_unified.py --audit {fichier_syntaxe_invalide}\"\n         result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n-        tests_robustesse.append({\n-            'test': 'syntaxe_invalide',\n-            'succes': result.returncode != 1,\n-            'exit_code': result.returncode\n-        })\n+        tests_robustesse.append(\n+            {\n+                \"test\": \"syntaxe_invalide\",\n+                \"succes\": result.returncode != 1,\n+                \"exit_code\": result.returncode,\n+            }\n+        )\n \n         # Calcul du taux de succ\u00e8s\n-        succes = sum(1 for t in tests_robustesse if t['succes'])\n+        succes = sum(1 for t in tests_robustesse if t[\"succes\"])\n         taux_robustesse = (succes / len(tests_robustesse)) * 100\n \n         return {\n-            'succes': taux_robustesse >= 80,  # 80% des cas doivent \u00eatre g\u00e9r\u00e9s\n-            'taux_robustesse': taux_robustesse,\n-            'tests_detail': tests_robustesse\n+            \"succes\": taux_robustesse >= 80,  # 80% des cas doivent \u00eatre g\u00e9r\u00e9s\n+            \"taux_robustesse\": taux_robustesse,\n+            \"tests_detail\": tests_robustesse,\n         }\n \n     def test_performance_benchmark(self):\n         \"\"\"Test 4: Performance vs solution manuelle\"\"\"\n         print(\"\ud83d\udd0d Test 4: Benchmark de performance...\")\n@@ -233,40 +249,50 @@\n         start = time.time()\n         projet_benchmark = \"/tmp/benchmark_test\"\n         os.makedirs(projet_benchmark, exist_ok=True)\n \n         # Cr\u00e9e un fichier Python simple pour benchmark\n-        with open(f\"{projet_benchmark}/main.py\", 'w') as f:\n-            f.write(\"\"\"def benchmark():\n+        with open(f\"{projet_benchmark}/main.py\", \"w\") as f:\n+            f.write(\n+                \"\"\"def benchmark():\n     return \"test\"\n \n if __name__ == \"__main__\":\n     benchmark()\n-\"\"\")\n+\"\"\"\n+            )\n \n         cmd = f\"python scripts/athalia_unified.py {projet_benchmark} --action complete\"\n-        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n+        result = subprocess.run(\n+            cmd, shell=True, capture_output=True, text=True, timeout=60\n+        )\n         temps_athalia = time.time() - start\n \n         if result.returncode != 0:\n             return {\n-                'succes': False,\n-                'erreur': f\"Benchmark \u00e9chou\u00e9: {result.stderr}\",\n-                'temps_athalia': temps_athalia\n+                \"succes\": False,\n+                \"erreur\": f\"Benchmark \u00e9chou\u00e9: {result.stderr}\",\n+                \"temps_athalia\": temps_athalia,\n             }\n \n         # Estimation du temps manuel (cr\u00e9er un projet \u00e9quivalent)\n-        temps_estime_manuel = 300  # 5 minutes pour cr\u00e9er un projet \u00e9quivalent manuellement\n+        temps_estime_manuel = (\n+            300  # 5 minutes pour cr\u00e9er un projet \u00e9quivalent manuellement\n+        )\n \n         gain_temps = temps_estime_manuel / temps_athalia if temps_athalia > 0 else 0\n \n         return {\n-            'succes': temps_athalia <= self.seuils_critiques['temps_max_generation'],\n-            'temps_athalia': temps_athalia,\n-            'temps_estime_manuel': temps_estime_manuel,\n-            'gain_temps': gain_temps,\n-            'efficacite': 'EXCELLENTE' if gain_temps > 10 else 'BONNE' if gain_temps > 5 else 'MOYENNE'\n+            \"succes\": temps_athalia <= self.seuils_critiques[\"temps_max_generation\"],\n+            \"temps_athalia\": temps_athalia,\n+            \"temps_estime_manuel\": temps_estime_manuel,\n+            \"gain_temps\": gain_temps,\n+            \"efficacite\": (\n+                \"EXCELLENTE\"\n+                if gain_temps > 10\n+                else \"BONNE\" if gain_temps > 5 else \"MOYENNE\"\n+            ),\n         }\n \n     def test_qualite_code_genere(self):\n         \"\"\"Test 5: Qualit\u00e9 objective du code g\u00e9n\u00e9r\u00e9\"\"\"\n         print(\"\ud83d\udd0d Test 5: Qualit\u00e9 du code g\u00e9n\u00e9r\u00e9...\")\n@@ -274,105 +300,121 @@\n         # G\u00e9n\u00e8re un projet pour analyse\n         projet_qualite = \"/tmp/projet_qualite\"\n         os.makedirs(projet_qualite, exist_ok=True)\n \n         # Cr\u00e9e un fichier Python pour analyse de qualit\u00e9\n-        with open(f\"{projet_qualite}/main.py\", 'w') as f:\n-            f.write(\"\"\"def qualite_test():\n+        with open(f\"{projet_qualite}/main.py\", \"w\") as f:\n+            f.write(\n+                \"\"\"def qualite_test():\n     return \"qualite\"\n \n if __name__ == \"__main__\":\n     qualite_test()\n-\"\"\")\n+\"\"\"\n+            )\n \n         cmd = f\"python scripts/athalia_unified.py {projet_qualite} --action complete\"\n \n-        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n+        result = subprocess.run(\n+            cmd, shell=True, capture_output=True, text=True, timeout=60\n+        )\n \n         if result.returncode != 0:\n             return {\n-                'succes': False,\n-                'erreur': \"Impossible de g\u00e9n\u00e9rer le projet pour analyse\"\n+                \"succes\": False,\n+                \"erreur\": \"Impossible de g\u00e9n\u00e9rer le projet pour analyse\",\n             }\n \n         # Analyse avec pylint si disponible\n         try:\n             cmd_pylint = f\"python -m pylint {projet_qualite} --output-format=json\"\n-            result_pylint = subprocess.run(cmd_pylint, shell=True, capture_output=True, text=True)\n+            result_pylint = subprocess.run(\n+                cmd_pylint, shell=True, capture_output=True, text=True\n+            )\n \n             if result_pylint.returncode == 0:\n                 try:\n                     pylint_data = json.loads(result_pylint.stdout)\n-                    score_pylint = pylint_data.get('score', 0)\n-                    erreurs = len([e for e in pylint_data.get('errors', []) if e.get('type') == 'error'])\n-                    warnings = len([w for w in pylint_data.get('warnings', [])])\n+                    score_pylint = pylint_data.get(\"score\", 0)\n+                    erreurs = len(\n+                        [\n+                            e\n+                            for e in pylint_data.get(\"errors\", [])\n+                            if e.get(\"type\") == \"error\"\n+                        ]\n+                    )\n+                    warnings = len([w for w in pylint_data.get(\"warnings\", [])])\n \n                     return {\n-                        'succes': score_pylint >= 7.0,  # Score pylint minimum\n-                        'score_pylint': score_pylint,\n-                        'erreurs': erreurs,\n-                        'warnings': warnings,\n-                        'qualite': 'EXCELLENTE' if score_pylint >= 9.0 else 'BONNE' if score_pylint >= 7.0 else 'MOYENNE'\n+                        \"succes\": score_pylint >= 7.0,  # Score pylint minimum\n+                        \"score_pylint\": score_pylint,\n+                        \"erreurs\": erreurs,\n+                        \"warnings\": warnings,\n+                        \"qualite\": (\n+                            \"EXCELLENTE\"\n+                            if score_pylint >= 9.0\n+                            else \"BONNE\" if score_pylint >= 7.0 else \"MOYENNE\"\n+                        ),\n                     }\n                 except json.JSONDecodeError:\n                     pass\n         except Exception:\n             pass\n \n         # Fallback: analyse basique\n-        fichiers_python = list(Path(projet_qualite).glob('**/*.py'))\n+        fichiers_python = list(Path(projet_qualite).glob(\"**/*.py\"))\n         total_lignes = 0\n         fonctions = 0\n         classes = 0\n \n         for py_file in fichiers_python:\n             try:\n-                with open(py_file, 'r', encoding='utf-8') as f:\n+                with open(py_file, \"r\", encoding=\"utf-8\") as f:\n                     contenu = f.read()\n-                    lignes = contenu.split('\\n')\n+                    lignes = contenu.split(\"\\n\")\n                     total_lignes += len(lignes)\n-                    fonctions += contenu.count('def ')\n-                    classes += contenu.count('class ')\n+                    fonctions += contenu.count(\"def \")\n+                    classes += contenu.count(\"class \")\n             except Exception:\n                 pass\n \n         return {\n-            'succes': total_lignes > 0,  # Au moins du code g\u00e9n\u00e9r\u00e9\n-            'lignes_code': total_lignes,\n-            'fonctions': fonctions,\n-            'classes': classes,\n-            'qualite': 'ANALYSE_BASIQUE'\n+            \"succes\": total_lignes > 0,  # Au moins du code g\u00e9n\u00e9r\u00e9\n+            \"lignes_code\": total_lignes,\n+            \"fonctions\": fonctions,\n+            \"classes\": classes,\n+            \"qualite\": \"ANALYSE_BASIQUE\",\n         }\n \n     def validation_complete(self):\n         \"\"\"Validation compl\u00e8te objective\"\"\"\n         print(\"\ud83d\ude80 D\u00e9marrage de la validation objective d'Athalia/Arkalia\")\n         print(\"=\" * 60)\n \n         tests = {\n-            'generation_compilation': self.test_generation_et_compilation,\n-            'correction_erreurs': self.test_correction_reelle,\n-            'robustesse': self.test_robustesse_cas_limites,\n-            'performance': self.test_performance_benchmark,\n-            'qualite_code': self.test_qualite_code_genere\n+            \"generation_compilation\": self.test_generation_et_compilation,\n+            \"correction_erreurs\": self.test_correction_reelle,\n+            \"robustesse\": self.test_robustesse_cas_limites,\n+            \"performance\": self.test_performance_benchmark,\n+            \"qualite_code\": self.test_qualite_code_genere,\n         }\n \n         resultats = {}\n         temps_total_start = time.time()\n \n         for nom, test_func in tests.items():\n             print(f\"\\n\ud83d\udd0d Ex\u00e9cution du test: {nom}\")\n             try:\n                 resultats[nom] = test_func()\n-                status = \"\u2705 SUCC\u00c8S\" if resultats[nom].get('succes') else \"\u274c \u00c9CHEC\"\n+                status = \"\u2705 SUCC\u00c8S\" if resultats[nom].get(\"succes\") else \"\u274c \u00c9CHEC\"\n                 print(f\"   {status}\")\n \n-                if not resultats[nom].get('succes'):\n+                if not resultats[nom].get(\"succes\"):\n                     print(f\"   Erreur: {resultats[nom].get('erreur', 'Inconnue')}\")\n \n             except Exception as e:\n-                resultats[nom] = {'succes': False, 'erreur': str(e)}\n+                resultats[nom] = {\"succes\": False, \"erreur\": str(e)}\n                 print(f\"   \u274c ERREUR: {str(e)}\")\n \n         temps_total = time.time() - temps_total_start\n \n         # G\u00e9n\u00e9ration du rapport\n@@ -381,11 +423,11 @@\n         # Sauvegarde du rapport\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         os.makedirs(\"logs\", exist_ok=True)\n         rapport_file = f\"logs/rapport_validation_objective_{timestamp}.md\"\n \n-        with open(rapport_file, 'w', encoding='utf-8') as f:\n+        with open(rapport_file, \"w\", encoding=\"utf-8\") as f:\n             f.write(rapport)\n \n         print(f\"\\n\ud83d\udcca Rapport sauvegard\u00e9: {rapport_file}\")\n         print(\"\\n\" + \"=\" * 60)\n         print(rapport)\n@@ -394,11 +436,11 @@\n \n     def generer_rapport_objectif(self, resultats, temps_total):\n         \"\"\"G\u00e9n\u00e8re un rapport objectif et d\u00e9taill\u00e9\"\"\"\n \n         # Calcul des m\u00e9triques globales\n-        tests_succes = sum(1 for r in resultats.values() if r.get('succes', False))\n+        tests_succes = sum(1 for r in resultats.values() if r.get(\"succes\", False))\n         taux_succes = (tests_succes / len(resultats)) * 100\n \n         # D\u00e9termination du verdict\n         if taux_succes >= 90:\n             verdict = \"\ud83c\udf89 EXCELLENT - Athalia est tr\u00e8s fiable\"\n@@ -425,53 +467,65 @@\n ## \ud83d\udcc8 R\u00e9sultats D\u00e9taill\u00e9s\n \n \"\"\"\n \n         for nom, resultat in resultats.items():\n-            status = \"\u2705 SUCC\u00c8S\" if resultat.get('succes') else \"\u274c \u00c9CHEC\"\n+            status = \"\u2705 SUCC\u00c8S\" if resultat.get(\"succes\") else \"\u274c \u00c9CHEC\"\n             rapport += f\"### {nom.replace('_', ' ').title()}\\n\"\n             rapport += f\"**Statut:** {status}\\n\\n\"\n \n             # D\u00e9tails sp\u00e9cifiques selon le test\n-            if nom == 'generation_compilation':\n-                if resultat.get('succes'):\n-                    rapport += f\"- \u23f1\ufe0f Temps de g\u00e9n\u00e9ration: {resultat.get('temps', 0):.1f}s\\n\"\n+            if nom == \"generation_compilation\":\n+                if resultat.get(\"succes\"):\n+                    rapport += (\n+                        f\"- \u23f1\ufe0f Temps de g\u00e9n\u00e9ration: {resultat.get('temps', 0):.1f}s\\n\"\n+                    )\n                     rapport += f\"- \ud83d\udcc1 Fichiers g\u00e9n\u00e9r\u00e9s: {resultat.get('fichiers_generes', 0)}\\n\"\n                     rapport += f\"- \u2705 Taux de compilation: {resultat.get('taux_compilation', 0):.1f}%\\n\"\n                 else:\n                     rapport += f\"- \u274c Erreur: {resultat.get('erreur', 'Inconnue')}\\n\"\n \n-            elif nom == 'correction_erreurs':\n-                if resultat.get('succes'):\n-                    rapport += f\"- \u23f1\ufe0f Temps de correction: {resultat.get('temps', 0):.1f}s\\n\"\n+            elif nom == \"correction_erreurs\":\n+                if resultat.get(\"succes\"):\n+                    rapport += (\n+                        f\"- \u23f1\ufe0f Temps de correction: {resultat.get('temps', 0):.1f}s\\n\"\n+                    )\n                     rapport += \"- \u2705 Code compile apr\u00e8s correction: OUI\\n\"\n                 else:\n                     rapport += f\"- \u274c Erreur: {resultat.get('erreur', 'Inconnue')}\\n\"\n \n-            elif nom == 'robustesse':\n+            elif nom == \"robustesse\":\n                 rapport += f\"- \ud83d\udee1\ufe0f Taux de robustesse: {resultat.get('taux_robustesse', 0):.1f}%\\n\"\n-                for test in resultat.get('tests_detail', []):\n-                    status_test = \"\u2705\" if test['succes'] else \"\u274c\"\n+                for test in resultat.get(\"tests_detail\", []):\n+                    status_test = \"\u2705\" if test[\"succes\"] else \"\u274c\"\n                     rapport += f\"- {status_test} {test['test']}: exit code {test['exit_code']}\\n\"\n \n-            elif nom == 'performance':\n-                if resultat.get('succes'):\n-                    rapport += f\"- \u23f1\ufe0f Temps Athalia: {resultat.get('temps_athalia', 0):.1f}s\\n\"\n+            elif nom == \"performance\":\n+                if resultat.get(\"succes\"):\n+                    rapport += (\n+                        f\"- \u23f1\ufe0f Temps Athalia: {resultat.get('temps_athalia', 0):.1f}s\\n\"\n+                    )\n                     rapport += f\"- \u23f1\ufe0f Temps estim\u00e9 manuel: {resultat.get('temps_estime_manuel', 0):.1f}s\\n\"\n-                    rapport += f\"- \ud83d\ude80 Gain de temps: {resultat.get('gain_temps', 0):.1f}x\\n\"\n+                    rapport += (\n+                        f\"- \ud83d\ude80 Gain de temps: {resultat.get('gain_temps', 0):.1f}x\\n\"\n+                    )\n                     rapport += f\"- \ud83d\udcca Efficacit\u00e9: {resultat.get('efficacite', 'N/A')}\\n\"\n                 else:\n                     rapport += f\"- \u274c Erreur: {resultat.get('erreur', 'Inconnue')}\\n\"\n \n-            elif nom == 'qualite_code':\n-                if resultat.get('score_pylint'):\n-                    rapport += f\"- \ud83d\udcca Score pylint: {resultat.get('score_pylint', 0):.1f}/10\\n\"\n+            elif nom == \"qualite_code\":\n+                if resultat.get(\"score_pylint\"):\n+                    rapport += (\n+                        f\"- \ud83d\udcca Score pylint: {resultat.get('score_pylint', 0):.1f}/10\\n\"\n+                    )\n                     rapport += f\"- \u274c Erreurs: {resultat.get('erreurs', 0)}\\n\"\n                     rapport += f\"- \u26a0\ufe0f Warnings: {resultat.get('warnings', 0)}\\n\"\n                     rapport += f\"- \ud83d\udcc8 Qualit\u00e9: {resultat.get('qualite', 'N/A')}\\n\"\n                 else:\n-                    rapport += f\"- \ud83d\udcdd Lignes de code: {resultat.get('lignes_code', 0)}\\n\"\n+                    rapport += (\n+                        f\"- \ud83d\udcdd Lignes de code: {resultat.get('lignes_code', 0)}\\n\"\n+                    )\n                     rapport += f\"- \ud83d\udd27 Fonctions: {resultat.get('fonctions', 0)}\\n\"\n                     rapport += f\"- \ud83c\udfd7\ufe0f Classes: {resultat.get('classes', 0)}\\n\"\n \n             rapport += \"\\n\"\n \n@@ -490,17 +544,19 @@\n ## \ud83d\udd0d Points d'Attention\n \n \"\"\"\n \n         # Ajoute des points d'attention sp\u00e9cifiques\n-        if resultats.get('generation_compilation', {}).get('taux_compilation', 0) < 90:\n-            rapport += \"- \u26a0\ufe0f Le code g\u00e9n\u00e9r\u00e9 ne compile pas toujours (am\u00e9lioration n\u00e9cessaire)\\n\"\n-\n-        if resultats.get('performance', {}).get('gain_temps', 0) < 5:\n+        if resultats.get(\"generation_compilation\", {}).get(\"taux_compilation\", 0) < 90:\n+            rapport += (\n+                \"- \u26a0\ufe0f Le code g\u00e9n\u00e9r\u00e9 ne compile pas toujours (am\u00e9lioration n\u00e9cessaire)\\n\"\n+            )\n+\n+        if resultats.get(\"performance\", {}).get(\"gain_temps\", 0) < 5:\n             rapport += \"- \u26a0\ufe0f Gain de temps limit\u00e9 (optimisation recommand\u00e9e)\\n\"\n \n-        if resultats.get('robustesse', {}).get('taux_robustesse', 0) < 90:\n+        if resultats.get(\"robustesse\", {}).get(\"taux_robustesse\", 0) < 90:\n             rapport += \"- \u26a0\ufe0f Robustesse insuffisante (gestion d'erreurs \u00e0 am\u00e9liorer)\\n\"\n \n         rapport += f\"\"\"\n ## \ud83c\udf89 Conclusion\n Ce rapport est bas\u00e9 sur des **tests objectifs et mesurables**. Les r\u00e9sultats ne peuvent pas mentir.\n@@ -517,16 +573,16 @@\n if __name__ == \"__main__\":\n     validator = ValidationObjective()\n     resultats = validator.validation_complete()\n \n     # Affichage du score final\n-    tests_succes = sum(1 for r in resultats.values() if r.get('succes', False))\n+    tests_succes = sum(1 for r in resultats.values() if r.get(\"succes\", False))\n     taux_succes = (tests_succes / len(resultats)) * 100\n \n     print(f\"\\n\ud83c\udfaf SCORE FINAL: {taux_succes:.1f}%\")\n \n     if taux_succes >= 85:\n         print(\"\ud83c\udf89 TON OUTIL EST FIABLE ! Tu peux lui faire confiance.\")\n     elif taux_succes >= 70:\n         print(\"\u2705 Ton outil est bon avec quelques am\u00e9liorations mineures.\")\n     else:\n-        print(\"\u26a0\ufe0f Ton outil a des probl\u00e8mes \u00e0 corriger avant utilisation en production.\") \n\\ No newline at end of file\n+        print(\"\u26a0\ufe0f Ton outil a des probl\u00e8mes \u00e0 corriger avant utilisation en production.\")\n--- /Volumes/T7/athalia-dev-setup/tools/analysis/audit_complet_dossiers.py\t2025-07-29 17:56:19.430000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tools/analysis/audit_complet_dossiers.py\t2025-07-29 18:12:21.897252+00:00\n@@ -13,10 +13,11 @@\n \n \n @dataclass\n class DossierInfo:\n     \"\"\"Informations sur un dossier\"\"\"\n+\n     path: Path\n     nom: str\n     type_dossier: str  # 'core', 'tests', 'docs', 'tools', 'archive', etc.\n     fichiers_python: List[Path]\n     fichiers_md: List[Path]\n@@ -28,10 +29,11 @@\n \n \n @dataclass\n class ModuleInfo:\n     \"\"\"Informations sur un module Python\"\"\"\n+\n     path: Path\n     nom: str\n     taille: int\n     fonctions: List[str]\n     classes: List[str]\n@@ -43,10 +45,11 @@\n \n \n @dataclass\n class AuditResult:\n     \"\"\"R\u00e9sultat d'audit pour un dossier\"\"\"\n+\n     dossier: DossierInfo\n     modules: List[ModuleInfo]\n     score_utilite: float  # 0-10\n     score_implementation: float  # 0-10\n     score_tests: float  # 0-10\n@@ -61,11 +64,16 @@\n     \"\"\"Audit complet de tous les dossiers du projet\"\"\"\n \n     def __init__(self, root_path: Optional[str] = None):\n         self.root_path = Path(root_path) if root_path else Path.cwd()\n         self.dossiers_principaux = [\n-            \"athalia_core\", \"tests\", \"docs\", \"tools\", \"scripts\", \"config\"\n+            \"athalia_core\",\n+            \"tests\",\n+            \"docs\",\n+            \"tools\",\n+            \"scripts\",\n+            \"config\",\n         ]\n \n     def analyser_tous_dossiers(self) -> List[AuditResult]:\n         \"\"\"Analyser tous les dossiers principaux\"\"\"\n         results = []\n@@ -97,59 +105,70 @@\n \n         # Chercher dans athalia_core\n         core_path = self.root_path / \"athalia_core\"\n         if core_path.exists():\n             for item in core_path.iterdir():\n-                if item.is_dir() and not item.name.startswith('.'):\n+                if item.is_dir() and not item.name.startswith(\".\"):\n                     # V\u00e9rifier s'il contient des fichiers Python\n                     if list(item.rglob(\"*.py\")):\n                         sous_dossiers_caches.append(item)\n \n         # Chercher dans tests\n         tests_path = self.root_path / \"tests\"\n         if tests_path.exists():\n             for item in tests_path.iterdir():\n-                if item.is_dir() and not item.name.startswith('.'):\n+                if item.is_dir() and not item.name.startswith(\".\"):\n                     if list(item.rglob(\"*.py\")):\n                         sous_dossiers_caches.append(item)\n \n         # Chercher dans tools\n         tools_path = self.root_path / \"tools\"\n         if tools_path.exists():\n             for item in tools_path.iterdir():\n-                if item.is_dir() and not item.name.startswith('.'):\n+                if item.is_dir() and not item.name.startswith(\".\"):\n                     if list(item.rglob(\"*.py\")):\n                         sous_dossiers_caches.append(item)\n \n         return sous_dossiers_caches\n \n-    def _analyser_dossier_complet(self, dossier_path: Path, nom_dossier: str) -> Optional[AuditResult]:\n+    def _analyser_dossier_complet(\n+        self, dossier_path: Path, nom_dossier: str\n+    ) -> Optional[AuditResult]:\n         \"\"\"Analyser un dossier complet\"\"\"\n         try:\n             # Informations du dossier\n             dossier_info = self._analyser_dossier_info(dossier_path, nom_dossier)\n \n             # Analyser les modules Python\n             modules = []\n             for py_file in dossier_path.rglob(\"*.py\"):\n-                if py_file.name != \"__init__.py\" and not py_file.name.startswith('._'):\n+                if py_file.name != \"__init__.py\" and not py_file.name.startswith(\"._\"):\n                     module_info = self._analyser_module(py_file)\n                     if module_info:\n                         modules.append(module_info)\n \n             # Calculer les scores\n             score_utilite = self._calculer_score_utilite(dossier_info, modules)\n             score_implementation = self._calculer_score_implementation(modules)\n             score_tests = self._calculer_score_tests(dossier_info, modules)\n-            score_documentation = self._calculer_score_documentation(dossier_info, modules)\n+            score_documentation = self._calculer_score_documentation(\n+                dossier_info, modules\n+            )\n             score_integration = self._calculer_score_integration(modules)\n \n-            score_total = (score_utilite + score_implementation + score_tests +\n-                          score_documentation + score_integration) / 5\n+            score_total = (\n+                score_utilite\n+                + score_implementation\n+                + score_tests\n+                + score_documentation\n+                + score_integration\n+            ) / 5\n \n             # G\u00e9n\u00e9rer recommandations\n-            recommandations = self._generer_recommandations(dossier_info, modules, score_total)\n+            recommandations = self._generer_recommandations(\n+                dossier_info, modules, score_total\n+            )\n \n             # Chercher des p\u00e9pites\n             pepites = self._chercher_pepites(dossier_info, modules)\n \n             return AuditResult(\n@@ -160,22 +179,26 @@\n                 score_tests=score_tests,\n                 score_documentation=score_documentation,\n                 score_integration=score_integration,\n                 score_total=score_total,\n                 recommandations=recommandations,\n-                pepites_trouvees=pepites\n+                pepites_trouvees=pepites,\n             )\n \n         except Exception as e:\n             print(f\"\u26a0\ufe0f Erreur lors de l'analyse de {dossier_path}: {e}\")\n             return None\n \n-    def _analyser_dossier_info(self, dossier_path: Path, nom_dossier: str) -> DossierInfo:\n+    def _analyser_dossier_info(\n+        self, dossier_path: Path, nom_dossier: str\n+    ) -> DossierInfo:\n         \"\"\"Analyser les informations d'un dossier\"\"\"\n         fichiers_python = list(dossier_path.rglob(\"*.py\"))\n         fichiers_md = list(dossier_path.rglob(\"*.md\"))\n-        fichiers_yaml = list(dossier_path.rglob(\"*.yaml\")) + list(dossier_path.rglob(\"*.yml\"))\n+        fichiers_yaml = list(dossier_path.rglob(\"*.yaml\")) + list(\n+            dossier_path.rglob(\"*.yml\")\n+        )\n         fichiers_json = list(dossier_path.rglob(\"*.json\"))\n         sous_dossiers = [item for item in dossier_path.iterdir() if item.is_dir()]\n \n         # Calculer la taille totale\n         taille_totale = sum(f.stat().st_size for f in fichiers_python if f.exists())\n@@ -195,11 +218,13 @@\n             type_dossier = \"config\"\n         else:\n             type_dossier = \"autre\"\n \n         # G\u00e9n\u00e9rer une description\n-        description = f\"Dossier {type_dossier} avec {len(fichiers_python)} fichiers Python\"\n+        description = (\n+            f\"Dossier {type_dossier} avec {len(fichiers_python)} fichiers Python\"\n+        )\n \n         return DossierInfo(\n             path=dossier_path,\n             nom=nom_dossier,\n             type_dossier=type_dossier,\n@@ -207,17 +232,17 @@\n             fichiers_md=fichiers_md,\n             fichiers_yaml=fichiers_yaml,\n             fichiers_json=fichiers_json,\n             sous_dossiers=sous_dossiers,\n             taille_totale=taille_totale,\n-            description=description\n+            description=description,\n         )\n \n     def _analyser_module(self, file_path: Path) -> Optional[ModuleInfo]:\n         \"\"\"Analyser un module Python\"\"\"\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n \n             # Parse le code Python\n             tree = ast.parse(content)\n             taille = len(content)\n@@ -246,11 +271,13 @@\n             # Chercher les tests et documentation associ\u00e9s\n             tests_associes = self._chercher_tests_associes(file_path)\n             documentation_associee = self._chercher_documentation_associee(file_path)\n \n             # V\u00e9rifier l'int\u00e9gration avec l'orchestrateur\n-            integration_orchestrateur = self._verifier_integration_orchestrateur(content, imports)\n+            integration_orchestrateur = self._verifier_integration_orchestrateur(\n+                content, imports\n+            )\n \n             return ModuleInfo(\n                 path=file_path,\n                 nom=file_path.name,\n                 taille=taille,\n@@ -258,11 +285,11 @@\n                 classes=classes,\n                 imports=imports,\n                 docstring=docstring,\n                 tests_associes=tests_associes,\n                 documentation_associee=documentation_associee,\n-                integration_orchestrateur=integration_orchestrateur\n+                integration_orchestrateur=integration_orchestrateur,\n             )\n \n         except Exception as e:\n             print(f\"\u26a0\ufe0f Erreur lors de l'analyse de {file_path}: {e}\")\n             return None\n@@ -294,28 +321,38 @@\n             for doc_file in docs_path.rglob(f\"*{nom_module}*.md\"):\n                 docs.append(str(doc_file))\n \n         return docs\n \n-    def _verifier_integration_orchestrateur(self, content: str, imports: List[str]) -> bool:\n+    def _verifier_integration_orchestrateur(\n+        self, content: str, imports: List[str]\n+    ) -> bool:\n         \"\"\"V\u00e9rifier si le module s'int\u00e8gre avec l'orchestrateur principal\"\"\"\n         # V\u00e9rifier les imports d'Athalia\n         athalia_imports = [imp for imp in imports if \"athalia\" in imp.lower()]\n         return len(athalia_imports) > 0\n \n-    def _calculer_score_utilite(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> float:\n+    def _calculer_score_utilite(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> float:\n         \"\"\"Calculer le score d'utilit\u00e9 du dossier\"\"\"\n         if not modules:\n             return 0.0\n \n         # Crit\u00e8res d'utilit\u00e9\n         nb_fonctions = sum(len(m.fonctions) for m in modules)\n         nb_classes = sum(len(m.classes) for m in modules)\n-        nb_imports_athalia = sum(len([imp for imp in m.imports if \"athalia\" in imp.lower()]) for m in modules)\n+        nb_imports_athalia = sum(\n+            len([imp for imp in m.imports if \"athalia\" in imp.lower()]) for m in modules\n+        )\n \n         # Score bas\u00e9 sur la complexit\u00e9 et l'int\u00e9gration\n-        score = min(10.0, (nb_fonctions * 0.5 + nb_classes * 1.0 + nb_imports_athalia * 0.3) / len(modules))\n+        score = min(\n+            10.0,\n+            (nb_fonctions * 0.5 + nb_classes * 1.0 + nb_imports_athalia * 0.3)\n+            / len(modules),\n+        )\n \n         return round(score, 2)\n \n     def _calculer_score_implementation(self, modules: List[ModuleInfo]) -> float:\n         \"\"\"Calculer le score d'impl\u00e9mentation\"\"\"\n@@ -324,26 +361,34 @@\n \n         scores = []\n         for module in modules:\n             # Score bas\u00e9 sur la taille et la complexit\u00e9\n             if module.taille > 0:\n-                score = min(10.0, (len(module.fonctions) + len(module.classes) * 2) / (module.taille / 1000))\n+                score = min(\n+                    10.0,\n+                    (len(module.fonctions) + len(module.classes) * 2)\n+                    / (module.taille / 1000),\n+                )\n                 scores.append(score)\n \n         return round(sum(scores) / len(scores), 2) if scores else 0.0\n \n-    def _calculer_score_tests(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> float:\n+    def _calculer_score_tests(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> float:\n         \"\"\"Calculer le score de tests\"\"\"\n         if not modules:\n             return 0.0\n \n         modules_avec_tests = sum(1 for m in modules if m.tests_associes)\n         score = (modules_avec_tests / len(modules)) * 10.0\n \n         return round(score, 2)\n \n-    def _calculer_score_documentation(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> float:\n+    def _calculer_score_documentation(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> float:\n         \"\"\"Calculer le score de documentation\"\"\"\n         if not modules:\n             return 0.0\n \n         # Score bas\u00e9 sur la documentation des modules\n@@ -363,11 +408,13 @@\n         modules_integres = sum(1 for m in modules if m.integration_orchestrateur)\n         score = (modules_integres / len(modules)) * 10.0\n \n         return round(score, 2)\n \n-    def _generer_recommandations(self, dossier_info: DossierInfo, modules: List[ModuleInfo], score_total: float) -> List[str]:\n+    def _generer_recommandations(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo], score_total: float\n+    ) -> List[str]:\n         \"\"\"G\u00e9n\u00e9rer des recommandations d'am\u00e9lioration\"\"\"\n         recommandations = []\n \n         if score_total < 5.0:\n             recommandations.append(\"\ud83d\udd34 Am\u00e9lioration majeure n\u00e9cessaire\")\n@@ -378,29 +425,39 @@\n \n         # Recommandations sp\u00e9cifiques\n         if len(modules) > 0:\n             modules_sans_tests = [m for m in modules if not m.tests_associes]\n             if modules_sans_tests:\n-                recommandations.append(f\"\ud83d\udcdd Ajouter des tests pour {len(modules_sans_tests)} modules\")\n+                recommandations.append(\n+                    f\"\ud83d\udcdd Ajouter des tests pour {len(modules_sans_tests)} modules\"\n+                )\n \n             modules_sans_docs = [m for m in modules if not m.docstring]\n             if modules_sans_docs:\n-                recommandations.append(f\"\ud83d\udcda Ajouter de la documentation pour {len(modules_sans_docs)} modules\")\n+                recommandations.append(\n+                    f\"\ud83d\udcda Ajouter de la documentation pour {len(modules_sans_docs)} modules\"\n+                )\n \n         return recommandations\n \n-    def _chercher_pepites(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> List[str]:\n+    def _chercher_pepites(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> List[str]:\n         \"\"\"Chercher des p\u00e9pites (fonctionnalit\u00e9s int\u00e9ressantes)\"\"\"\n         pepites = []\n \n         for module in modules:\n             # Chercher des patterns int\u00e9ressants\n             if len(module.fonctions) > 10:\n-                pepites.append(f\"\ud83d\ude80 Module {module.nom}: {len(module.fonctions)} fonctions (tr\u00e8s actif)\")\n+                pepites.append(\n+                    f\"\ud83d\ude80 Module {module.nom}: {len(module.fonctions)} fonctions (tr\u00e8s actif)\"\n+                )\n \n             if len(module.classes) > 5:\n-                pepites.append(f\"\ud83c\udfd7\ufe0f Module {module.nom}: {len(module.classes)} classes (architecture complexe)\")\n+                pepites.append(\n+                    f\"\ud83c\udfd7\ufe0f Module {module.nom}: {len(module.classes)} classes (architecture complexe)\"\n+                )\n \n             if module.integration_orchestrateur:\n                 pepites.append(f\"\ud83d\udd17 Module {module.nom}: Int\u00e9gr\u00e9 avec l'orchestrateur\")\n \n         return pepites\n@@ -460,11 +517,13 @@\n \"\"\"\n \n             if score_moyen < 5.0:\n                 rapport += \"\ud83d\udd34 **Action critique n\u00e9cessaire** - Le projet n\u00e9cessite une refonte majeure\"\n             elif score_moyen < 7.0:\n-                rapport += \"\ud83d\udfe1 **Am\u00e9lioration recommand\u00e9e** - Quelques ajustements n\u00e9cessaires\"\n+                rapport += (\n+                    \"\ud83d\udfe1 **Am\u00e9lioration recommand\u00e9e** - Quelques ajustements n\u00e9cessaires\"\n+                )\n             else:\n                 rapport += \"\ud83d\udfe2 **Excellent \u00e9tat** - Le projet est bien structur\u00e9\"\n \n         return rapport\n \n@@ -476,15 +535,15 @@\n \n     # Sauvegarder le rapport\n     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n     rapport_file = f\"rapport_audit_complet_{timestamp}.md\"\n \n-    with open(rapport_file, 'w', encoding='utf-8') as f:\n+    with open(rapport_file, \"w\", encoding=\"utf-8\") as f:\n         f.write(rapport)\n \n     print(f\"\ud83d\udcc4 Rapport sauvegard\u00e9: {rapport_file}\")\n     print(\"\\n\" + \"=\" * 60)\n     print(rapport)\n \n \n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n--- /Volumes/T7/athalia-dev-setup/tests/audit_complet_dossiers.py\t2025-07-29 17:56:10.820000+00:00\n+++ /Volumes/T7/athalia-dev-setup/tests/audit_complet_dossiers.py\t2025-07-29 18:12:21.905927+00:00\n@@ -13,13 +13,15 @@\n from dataclasses import dataclass, field\n \n # Ajouter le r\u00e9pertoire parent au path\n sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n \n+\n @dataclass\n class DossierInfo:\n     \"\"\"Informations sur un dossier\"\"\"\n+\n     path: Path\n     nom: str\n     type_dossier: str  # 'core', 'tests', 'docs', 'tools', 'archive', etc.\n     fichiers_python: List[Path]\n     fichiers_md: List[Path]\n@@ -27,13 +29,15 @@\n     fichiers_json: List[Path]\n     sous_dossiers: List[Path]\n     taille_totale: int\n     description: str = \"\"\n \n+\n @dataclass\n class ModuleInfo:\n     \"\"\"Informations sur un module Python\"\"\"\n+\n     path: Path\n     nom: str\n     taille: int\n     fonctions: List[str]\n     classes: List[str]\n@@ -41,13 +45,15 @@\n     docstring: str = \"\"\n     tests_associes: List[str] = field(default_factory=list)\n     documentation_associee: List[str] = field(default_factory=list)\n     integration_orchestrateur: bool = False\n \n+\n @dataclass\n class AuditResult:\n     \"\"\"R\u00e9sultat d'audit pour un dossier\"\"\"\n+\n     dossier: DossierInfo\n     modules: List[ModuleInfo]\n     score_utilite: float  # 0-10\n     score_implementation: float  # 0-10\n     score_tests: float  # 0-10\n@@ -55,27 +61,28 @@\n     score_integration: float  # 0-10\n     score_total: float  # 0-10\n     recommandations: List[str] = field(default_factory=list)\n     pepites_trouvees: List[str] = field(default_factory=list)\n \n+\n class AuditCompletDossiers:\n     \"\"\"Auditeur complet des dossiers et sous-dossiers\"\"\"\n-    \n+\n     def __init__(self, root_path: str = None):\n         self.root_path = Path(root_path or Path.cwd())\n         self.dossiers = []\n         self.results = []\n-        \n+\n     def analyser_tous_dossiers(self) -> List[AuditResult]:\n         \"\"\"Analyser tous les dossiers et sous-dossiers\"\"\"\n         print(\"\ud83d\udd0d ANALYSE COMPL\u00c8TE DE TOUS LES DOSSIERS\")\n         print(\"=\" * 50)\n-        \n+\n         # Dossiers principaux \u00e0 analyser\n         dossiers_principaux = [\n             \"athalia_core\",\n-            \"tests\", \n+            \"tests\",\n             \"docs\",\n             \"tools\",\n             \"demos\",\n             \"scripts\",\n             \"setup\",\n@@ -85,124 +92,139 @@\n             \"archive\",\n             \"projects\",\n             \"mon-projet\",\n             \"prompts\",\n             \"templates\",\n-            \"bin\"\n+            \"bin\",\n         ]\n-        \n+\n         results = []\n-        \n+\n         for dossier_nom in dossiers_principaux:\n             dossier_path = self.root_path / dossier_nom\n             if dossier_path.exists():\n                 print(f\"\\n\ud83d\udcc1 ANALYSE DU DOSSIER : {dossier_nom}\")\n                 result = self._analyser_dossier_complet(dossier_path, dossier_nom)\n                 if result:\n                     results.append(result)\n-        \n+\n         # Analyser les sous-dossiers cach\u00e9s\n         print(\"\\n\ud83d\udd0d ANALYSE DES SOUS-DOSSIERS CACH\u00c9S\")\n         sous_dossiers_caches = self._trouver_sous_dossiers_caches()\n         for sous_dossier in sous_dossiers_caches:\n             print(f\"\\n\ud83d\udcc1 ANALYSE DU SOUS-DOSSIER CACH\u00c9 : {sous_dossier}\")\n             result = self._analyser_dossier_complet(sous_dossier, sous_dossier.name)\n             if result:\n                 results.append(result)\n-        \n+\n         self.results = results\n         return results\n-    \n+\n     def _trouver_sous_dossiers_caches(self) -> List[Path]:\n         \"\"\"Trouver les sous-dossiers cach\u00e9s qui pourraient contenir des p\u00e9pites\"\"\"\n         sous_dossiers_caches = []\n-        \n+\n         # Chercher dans athalia_core\n         core_path = self.root_path / \"athalia_core\"\n         if core_path.exists():\n             for item in core_path.iterdir():\n-                if item.is_dir() and not item.name.startswith('.'):\n+                if item.is_dir() and not item.name.startswith(\".\"):\n                     # V\u00e9rifier s'il contient des fichiers Python\n                     if list(item.rglob(\"*.py\")):\n                         sous_dossiers_caches.append(item)\n-        \n+\n         # Chercher dans tests\n         tests_path = self.root_path / \"tests\"\n         if tests_path.exists():\n             for item in tests_path.iterdir():\n-                if item.is_dir() and not item.name.startswith('.'):\n+                if item.is_dir() and not item.name.startswith(\".\"):\n                     if list(item.rglob(\"*.py\")):\n                         sous_dossiers_caches.append(item)\n-        \n+\n         # Chercher dans tools\n         tools_path = self.root_path / \"tools\"\n         if tools_path.exists():\n             for item in tools_path.iterdir():\n-                if item.is_dir() and not item.name.startswith('.'):\n+                if item.is_dir() and not item.name.startswith(\".\"):\n                     if list(item.rglob(\"*.py\")):\n                         sous_dossiers_caches.append(item)\n-        \n+\n         return sous_dossiers_caches\n-    \n-    def _analyser_dossier_complet(self, dossier_path: Path, nom_dossier: str) -> Optional[AuditResult]:\n+\n+    def _analyser_dossier_complet(\n+        self, dossier_path: Path, nom_dossier: str\n+    ) -> Optional[AuditResult]:\n         \"\"\"Analyser un dossier complet\"\"\"\n         try:\n             # Informations du dossier\n             dossier_info = self._analyser_dossier_info(dossier_path, nom_dossier)\n-            \n+\n             # Analyser les modules Python\n             modules = []\n             for py_file in dossier_path.rglob(\"*.py\"):\n-                if py_file.name != \"__init__.py\" and not py_file.name.startswith('._'):\n+                if py_file.name != \"__init__.py\" and not py_file.name.startswith(\"._\"):\n                     module_info = self._analyser_module(py_file)\n                     if module_info:\n                         modules.append(module_info)\n-            \n+\n             # Calculer les scores\n             score_utilite = self._calculer_score_utilite(dossier_info, modules)\n             score_implementation = self._calculer_score_implementation(modules)\n             score_tests = self._calculer_score_tests(dossier_info, modules)\n-            score_documentation = self._calculer_score_documentation(dossier_info, modules)\n+            score_documentation = self._calculer_score_documentation(\n+                dossier_info, modules\n+            )\n             score_integration = self._calculer_score_integration(modules)\n-            \n-            score_total = (score_utilite + score_implementation + score_tests + \n-                          score_documentation + score_integration) / 5\n-            \n+\n+            score_total = (\n+                score_utilite\n+                + score_implementation\n+                + score_tests\n+                + score_documentation\n+                + score_integration\n+            ) / 5\n+\n             # G\u00e9n\u00e9rer recommandations\n-            recommandations = self._generer_recommandations(dossier_info, modules, score_total)\n-            \n+            recommandations = self._generer_recommandations(\n+                dossier_info, modules, score_total\n+            )\n+\n             # Chercher des p\u00e9pites\n             pepites = self._chercher_pepites(dossier_info, modules)\n-            \n+\n             return AuditResult(\n                 dossier=dossier_info,\n                 modules=modules,\n                 score_utilite=score_utilite,\n                 score_implementation=score_implementation,\n                 score_tests=score_tests,\n                 score_documentation=score_documentation,\n                 score_integration=score_integration,\n                 score_total=score_total,\n                 recommandations=recommandations,\n-                pepites_trouvees=pepites\n-            )\n-            \n+                pepites_trouvees=pepites,\n+            )\n+\n         except Exception as e:\n             print(f\"\u26a0\ufe0f Erreur lors de l'analyse de {dossier_path}: {e}\")\n             return None\n-    \n-    def _analyser_dossier_info(self, dossier_path: Path, nom_dossier: str) -> DossierInfo:\n+\n+    def _analyser_dossier_info(\n+        self, dossier_path: Path, nom_dossier: str\n+    ) -> DossierInfo:\n         \"\"\"Analyser les informations d'un dossier\"\"\"\n         fichiers_python = list(dossier_path.rglob(\"*.py\"))\n         fichiers_md = list(dossier_path.rglob(\"*.md\"))\n-        fichiers_yaml = list(dossier_path.rglob(\"*.yaml\")) + list(dossier_path.rglob(\"*.yml\"))\n+        fichiers_yaml = list(dossier_path.rglob(\"*.yaml\")) + list(\n+            dossier_path.rglob(\"*.yml\")\n+        )\n         fichiers_json = list(dossier_path.rglob(\"*.json\"))\n         sous_dossiers = [item for item in dossier_path.iterdir() if item.is_dir()]\n-        \n+\n         # Calculer la taille totale\n         taille_totale = sum(f.stat().st_size for f in fichiers_python if f.exists())\n-        \n+\n         # D\u00e9terminer le type de dossier\n         if \"core\" in nom_dossier.lower():\n             type_dossier = \"core\"\n         elif \"test\" in nom_dossier.lower():\n             type_dossier = \"tests\"\n@@ -218,362 +240,416 @@\n             type_dossier = \"config\"\n         elif \"archive\" in nom_dossier.lower():\n             type_dossier = \"archive\"\n         else:\n             type_dossier = \"other\"\n-        \n+\n         return DossierInfo(\n             path=dossier_path,\n             nom=nom_dossier,\n             type_dossier=type_dossier,\n             fichiers_python=fichiers_python,\n             fichiers_md=fichiers_md,\n             fichiers_yaml=fichiers_yaml,\n             fichiers_json=fichiers_json,\n             sous_dossiers=sous_dossiers,\n-            taille_totale=taille_totale\n+            taille_totale=taille_totale,\n         )\n-    \n+\n     def _analyser_module(self, file_path: Path) -> Optional[ModuleInfo]:\n         \"\"\"Analyser un module Python\"\"\"\n         try:\n-            with open(file_path, 'r', encoding='utf-8') as f:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                 content = f.read()\n-            \n+\n             # Parser le code\n             tree = ast.parse(content)\n-            \n+\n             # Extraire les fonctions\n             fonctions = []\n             for node in ast.walk(tree):\n                 if isinstance(node, ast.FunctionDef):\n                     fonctions.append(node.name)\n-            \n+\n             # Extraire les classes\n             classes = []\n             for node in ast.walk(tree):\n                 if isinstance(node, ast.ClassDef):\n                     classes.append(node.name)\n-            \n+\n             # Extraire les imports\n             imports = []\n             for node in ast.walk(tree):\n                 if isinstance(node, ast.Import):\n                     for alias in node.names:\n                         imports.append(alias.name)\n                 elif isinstance(node, ast.ImportFrom):\n                     module = node.module or \"\"\n                     for alias in node.names:\n                         imports.append(f\"{module}.{alias.name}\")\n-            \n+\n             # Extraire le docstring\n             docstring = \"\"\n             for node in ast.walk(tree):\n                 if isinstance(node, ast.Module) and node.body:\n                     first_node = node.body[0]\n-                    if isinstance(first_node, ast.Expr) and isinstance(first_node.value, ast.Str):\n-                        docstring = first_node.value.s.split('\\n')[0]\n+                    if isinstance(first_node, ast.Expr) and isinstance(\n+                        first_node.value, ast.Str\n+                    ):\n+                        docstring = first_node.value.s.split(\"\\n\")[0]\n                         break\n-            \n+\n             # Chercher les tests associ\u00e9s\n             tests_associes = self._chercher_tests_associes(file_path)\n-            \n+\n             # Chercher la documentation associ\u00e9e\n             documentation_associee = self._chercher_documentation_associee(file_path)\n-            \n+\n             # V\u00e9rifier l'int\u00e9gration avec l'orchestrateur\n-            integration_orchestrateur = self._verifier_integration_orchestrateur(content, imports)\n-            \n+            integration_orchestrateur = self._verifier_integration_orchestrateur(\n+                content, imports\n+            )\n+\n             return ModuleInfo(\n                 path=file_path,\n                 nom=file_path.stem,\n-                taille=len(content.split('\\n')),\n+                taille=len(content.split(\"\\n\")),\n                 fonctions=fonctions,\n                 classes=classes,\n                 imports=imports,\n                 docstring=docstring,\n                 tests_associes=tests_associes,\n                 documentation_associee=documentation_associee,\n-                integration_orchestrateur=integration_orchestrateur\n-            )\n-            \n+                integration_orchestrateur=integration_orchestrateur,\n+            )\n+\n         except Exception as e:\n             print(f\"\u26a0\ufe0f Erreur lors de l'analyse de {file_path}: {e}\")\n             return None\n-    \n+\n     def _chercher_tests_associes(self, file_path: Path) -> List[str]:\n         \"\"\"Chercher les tests associ\u00e9s \u00e0 un module\"\"\"\n         tests = []\n         nom_module = file_path.stem\n-        \n+\n         # Chercher dans le dossier tests\n         tests_path = self.root_path / \"tests\"\n         if tests_path.exists():\n             for test_file in tests_path.rglob(f\"test_{nom_module}*.py\"):\n                 tests.append(str(test_file.relative_to(self.root_path)))\n             for test_file in tests_path.rglob(f\"*{nom_module}*test*.py\"):\n                 tests.append(str(test_file.relative_to(self.root_path)))\n-        \n+\n         return tests\n-    \n+\n     def _chercher_documentation_associee(self, file_path: Path) -> List[str]:\n         \"\"\"Chercher la documentation associ\u00e9e \u00e0 un module\"\"\"\n         docs = []\n         nom_module = file_path.stem\n-        \n+\n         # Chercher dans le dossier docs\n         docs_path = self.root_path / \"docs\"\n         if docs_path.exists():\n             for doc_file in docs_path.rglob(f\"*{nom_module}*.md\"):\n                 docs.append(str(doc_file.relative_to(self.root_path)))\n-        \n+\n         return docs\n-    \n-    def _verifier_integration_orchestrateur(self, content: str, imports: List[str]) -> bool:\n+\n+    def _verifier_integration_orchestrateur(\n+        self, content: str, imports: List[str]\n+    ) -> bool:\n         \"\"\"V\u00e9rifier si le module est int\u00e9gr\u00e9 dans l'orchestrateur\"\"\"\n         # V\u00e9rifier les imports de l'orchestrateur\n         if \"unified_orchestrator\" in imports or \"intelligent_analyzer\" in imports:\n             return True\n-        \n+\n         # V\u00e9rifier dans le contenu\n-        if \"orchestrator\" in content.lower() or \"intelligent_analyzer\" in content.lower():\n+        if (\n+            \"orchestrator\" in content.lower()\n+            or \"intelligent_analyzer\" in content.lower()\n+        ):\n             return True\n-        \n+\n         return False\n-    \n-    def _calculer_score_utilite(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> float:\n+\n+    def _calculer_score_utilite(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> float:\n         \"\"\"Calculer le score d'utilit\u00e9\"\"\"\n         score = 0.0\n-        \n+\n         # Points pour les fichiers Python\n         if dossier_info.fichiers_python:\n             score += min(len(dossier_info.fichiers_python) * 0.5, 5.0)\n-        \n+\n         # Points pour la taille\n         if dossier_info.taille_totale > 0:\n             score += min(dossier_info.taille_totale / 1000, 2.0)\n-        \n+\n         # Points pour les types de fichiers\n         if dossier_info.fichiers_md:\n             score += 1.0\n         if dossier_info.fichiers_yaml:\n             score += 1.0\n         if dossier_info.fichiers_json:\n             score += 1.0\n-        \n+\n         return min(score, 10.0)\n-    \n+\n     def _calculer_score_implementation(self, modules: List[ModuleInfo]) -> float:\n         \"\"\"Calculer le score d'impl\u00e9mentation\"\"\"\n         if not modules:\n             return 0.0\n-        \n+\n         score = 0.0\n-        \n+\n         for module in modules:\n             # Points pour les fonctions\n             score += min(len(module.fonctions) * 0.2, 2.0)\n-            \n+\n             # Points pour les classes\n             score += min(len(module.classes) * 0.3, 2.0)\n-            \n+\n             # Points pour la taille\n             score += min(module.taille / 100, 2.0)\n-            \n+\n             # Points pour le docstring\n             if module.docstring:\n                 score += 1.0\n-        \n+\n         return min(score / len(modules), 10.0)\n-    \n-    def _calculer_score_tests(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> float:\n+\n+    def _calculer_score_tests(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> float:\n         \"\"\"Calculer le score des tests\"\"\"\n         score = 0.0\n-        \n+\n         # Compter les tests associ\u00e9s\n         total_tests = sum(len(module.tests_associes) for module in modules)\n         if total_tests > 0:\n             score += min(total_tests * 2.0, 5.0)\n-        \n+\n         # Points pour les fichiers de test dans le dossier\n-        test_files = [f for f in dossier_info.fichiers_python if \"test\" in f.name.lower()]\n+        test_files = [\n+            f for f in dossier_info.fichiers_python if \"test\" in f.name.lower()\n+        ]\n         score += min(len(test_files) * 1.0, 5.0)\n-        \n+\n         return min(score, 10.0)\n-    \n-    def _calculer_score_documentation(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> float:\n+\n+    def _calculer_score_documentation(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> float:\n         \"\"\"Calculer le score de documentation\"\"\"\n         score = 0.0\n-        \n+\n         # Points pour les fichiers markdown\n         score += min(len(dossier_info.fichiers_md) * 1.0, 5.0)\n-        \n+\n         # Points pour la documentation associ\u00e9e\n         total_docs = sum(len(module.documentation_associee) for module in modules)\n         score += min(total_docs * 1.0, 3.0)\n-        \n+\n         # Points pour les docstrings\n         modules_avec_docstring = sum(1 for module in modules if module.docstring)\n         if modules:\n             score += (modules_avec_docstring / len(modules)) * 2.0\n-        \n+\n         return min(score, 10.0)\n-    \n+\n     def _calculer_score_integration(self, modules: List[ModuleInfo]) -> float:\n         \"\"\"Calculer le score d'int\u00e9gration\"\"\"\n         if not modules:\n             return 0.0\n-        \n-        modules_integres = sum(1 for module in modules if module.integration_orchestrateur)\n+\n+        modules_integres = sum(\n+            1 for module in modules if module.integration_orchestrateur\n+        )\n         return (modules_integres / len(modules)) * 10.0\n-    \n-    def _generer_recommandations(self, dossier_info: DossierInfo, modules: List[ModuleInfo], score_total: float) -> List[str]:\n+\n+    def _generer_recommandations(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo], score_total: float\n+    ) -> List[str]:\n         \"\"\"G\u00e9n\u00e9rer des recommandations\"\"\"\n         recommandations = []\n-        \n+\n         if score_total < 5.0:\n             recommandations.append(\"\u26a0\ufe0f Score faible - N\u00e9cessite une am\u00e9lioration\")\n-        \n+\n         if not modules:\n             recommandations.append(\"\ud83d\udcdd Aucun module Python trouv\u00e9 - V\u00e9rifier l'utilit\u00e9\")\n-        \n+\n         if not dossier_info.fichiers_md:\n-            recommandations.append(\"\ud83d\udcda Aucune documentation trouv\u00e9e - Ajouter des fichiers .md\")\n-        \n+            recommandations.append(\n+                \"\ud83d\udcda Aucune documentation trouv\u00e9e - Ajouter des fichiers .md\"\n+            )\n+\n         total_tests = sum(len(module.tests_associes) for module in modules)\n         if total_tests == 0 and modules:\n             recommandations.append(\"\ud83e\uddea Aucun test associ\u00e9 - Cr\u00e9er des tests\")\n-        \n+\n         modules_sans_docstring = [m for m in modules if not m.docstring]\n         if modules_sans_docstring:\n-            recommandations.append(f\"\ud83d\udcd6 {len(modules_sans_docstring)} modules sans docstring - Ajouter de la documentation\")\n-        \n+            recommandations.append(\n+                f\"\ud83d\udcd6 {len(modules_sans_docstring)} modules sans docstring - Ajouter de la documentation\"\n+            )\n+\n         modules_non_integres = [m for m in modules if not m.integration_orchestrateur]\n         if modules_non_integres:\n-            recommandations.append(f\"\ud83d\udd17 {len(modules_non_integres)} modules non int\u00e9gr\u00e9s - Consid\u00e9rer l'int\u00e9gration\")\n-        \n+            recommandations.append(\n+                f\"\ud83d\udd17 {len(modules_non_integres)} modules non int\u00e9gr\u00e9s - Consid\u00e9rer l'int\u00e9gration\"\n+            )\n+\n         return recommandations\n-    \n-    def _chercher_pepites(self, dossier_info: DossierInfo, modules: List[ModuleInfo]) -> List[str]:\n+\n+    def _chercher_pepites(\n+        self, dossier_info: DossierInfo, modules: List[ModuleInfo]\n+    ) -> List[str]:\n         \"\"\"Chercher des p\u00e9pites dans le dossier\"\"\"\n         pepites = []\n-        \n+\n         # Chercher des modules avec beaucoup de fonctionnalit\u00e9s\n         for module in modules:\n             if len(module.fonctions) > 10 or len(module.classes) > 5:\n-                pepites.append(f\"\ud83d\udc8e Module riche : {module.nom} ({len(module.fonctions)} fonctions, {len(module.classes)} classes)\")\n-        \n+                pepites.append(\n+                    f\"\ud83d\udc8e Module riche : {module.nom} ({len(module.fonctions)} fonctions, {len(module.classes)} classes)\"\n+                )\n+\n         # Chercher des modules avec des noms int\u00e9ressants\n         for module in modules:\n             nom_lower = module.nom.lower()\n-            if any(mot in nom_lower for mot in [\"intelligent\", \"advanced\", \"smart\", \"ai\", \"ml\", \"neural\", \"deep\"]):\n+            if any(\n+                mot in nom_lower\n+                for mot in [\n+                    \"intelligent\",\n+                    \"advanced\",\n+                    \"smart\",\n+                    \"ai\",\n+                    \"ml\",\n+                    \"neural\",\n+                    \"deep\",\n+                ]\n+            ):\n                 pepites.append(f\"\ud83e\udde0 Module IA : {module.nom}\")\n-        \n+\n         # Chercher des modules avec beaucoup d'imports\n         for module in modules:\n             if len(module.imports) > 10:\n-                pepites.append(f\"\ud83d\udd17 Module complexe : {module.nom} ({len(module.imports)} imports)\")\n-        \n+                pepites.append(\n+                    f\"\ud83d\udd17 Module complexe : {module.nom} ({len(module.imports)} imports)\"\n+                )\n+\n         # Chercher des fichiers de configuration\n         if dossier_info.fichiers_yaml:\n-            pepites.append(f\"\u2699\ufe0f Configuration : {len(dossier_info.fichiers_yaml)} fichiers YAML\")\n-        \n+            pepites.append(\n+                f\"\u2699\ufe0f Configuration : {len(dossier_info.fichiers_yaml)} fichiers YAML\"\n+            )\n+\n         return pepites\n-    \n+\n     def generer_rapport(self) -> str:\n         \"\"\"G\u00e9n\u00e9rer un rapport complet\"\"\"\n         rapport = []\n         rapport.append(\"# \ud83d\udd0d AUDIT COMPLET DOSSIERS ET SOUS-DOSSIERS\")\n         rapport.append(\"=\" * 60)\n         rapport.append(f\"**Date** : {Path.cwd().name}\")\n         rapport.append(f\"**Total dossiers analys\u00e9s** : {len(self.results)}\")\n         rapport.append(\"\")\n-        \n+\n         # R\u00e9sum\u00e9 global\n         scores_totaux = [r.score_total for r in self.results]\n         if scores_totaux:\n-            rapport.append(f\"**Score moyen global** : {sum(scores_totaux) / len(scores_totaux):.2f}/10\")\n+            rapport.append(\n+                f\"**Score moyen global** : {sum(scores_totaux) / len(scores_totaux):.2f}/10\"\n+            )\n             rapport.append(f\"**Meilleur score** : {max(scores_totaux):.2f}/10\")\n             rapport.append(f\"**Pire score** : {min(scores_totaux):.2f}/10\")\n         rapport.append(\"\")\n-        \n+\n         # D\u00e9tails par dossier\n         for result in sorted(self.results, key=lambda x: x.score_total, reverse=True):\n             rapport.append(f\"## \ud83d\udcc1 {result.dossier.nom}\")\n             rapport.append(f\"**Type** : {result.dossier.type_dossier}\")\n             rapport.append(f\"**Score total** : {result.score_total:.2f}/10\")\n             rapport.append(f\"**Modules Python** : {len(result.modules)}\")\n             rapport.append(f\"**Fichiers MD** : {len(result.dossier.fichiers_md)}\")\n             rapport.append(f\"**Sous-dossiers** : {len(result.dossier.sous_dossiers)}\")\n             rapport.append(\"\")\n-            \n+\n             # Scores d\u00e9taill\u00e9s\n             rapport.append(\"### \ud83d\udcca Scores d\u00e9taill\u00e9s\")\n             rapport.append(f\"- **Utilit\u00e9** : {result.score_utilite:.2f}/10\")\n-            rapport.append(f\"- **Impl\u00e9mentation** : {result.score_implementation:.2f}/10\")\n+            rapport.append(\n+                f\"- **Impl\u00e9mentation** : {result.score_implementation:.2f}/10\"\n+            )\n             rapport.append(f\"- **Tests** : {result.score_tests:.2f}/10\")\n             rapport.append(f\"- **Documentation** : {result.score_documentation:.2f}/10\")\n             rapport.append(f\"- **Int\u00e9gration** : {result.score_integration:.2f}/10\")\n             rapport.append(\"\")\n-            \n+\n             # P\u00e9pites trouv\u00e9es\n             if result.pepites_trouvees:\n                 rapport.append(\"### \ud83d\udc8e P\u00e9pites trouv\u00e9es\")\n                 for pepite in result.pepites_trouvees:\n                     rapport.append(f\"- {pepite}\")\n                 rapport.append(\"\")\n-            \n+\n             # Recommandations\n             if result.recommandations:\n                 rapport.append(\"### \ud83c\udfaf Recommandations\")\n                 for rec in result.recommandations:\n                     rapport.append(f\"- {rec}\")\n                 rapport.append(\"\")\n-            \n+\n             # Modules principaux\n             if result.modules:\n                 rapport.append(\"### \ud83d\udce6 Modules principaux\")\n                 for module in result.modules[:5]:  # Top 5\n-                    rapport.append(f\"- **{module.nom}** : {len(module.fonctions)} fonctions, {len(module.classes)} classes\")\n+                    rapport.append(\n+                        f\"- **{module.nom}** : {len(module.fonctions)} fonctions, {len(module.classes)} classes\"\n+                    )\n                 rapport.append(\"\")\n-        \n+\n         return \"\\n\".join(rapport)\n+\n \n def main():\n     \"\"\"Fonction principale\"\"\"\n     print(\"\ud83d\udd0d AUDIT COMPLET DOSSIERS ET SOUS-DOSSIERS\")\n     print(\"=\" * 50)\n-    \n+\n     auditor = AuditCompletDossiers()\n     results = auditor.analyser_tous_dossiers()\n-    \n+\n     # G\u00e9n\u00e9rer le rapport\n     rapport = auditor.generer_rapport()\n-    \n+\n     # Sauvegarder le rapport\n     rapport_path = Path(\"audit_complet_dossiers.md\")\n-    with open(rapport_path, 'w', encoding='utf-8') as f:\n+    with open(rapport_path, \"w\", encoding=\"utf-8\") as f:\n         f.write(rapport)\n-    \n+\n     print(f\"\\n\u2705 Rapport sauvegard\u00e9 dans : {rapport_path}\")\n-    \n+\n     # Afficher un r\u00e9sum\u00e9\n     print(\"\\n\ud83d\udcca R\u00c9SUM\u00c9 EX\u00c9CUTIF:\")\n     print(f\"  \ud83d\udcc1 Dossiers analys\u00e9s : {len(results)}\")\n     if results:\n         scores = [r.score_total for r in results]\n         print(f\"  \ud83d\udcc8 Score moyen : {sum(scores) / len(scores):.2f}/10\")\n         print(f\"  \ud83c\udfc6 Meilleur : {max(scores):.2f}/10\")\n         print(f\"  \u26a0\ufe0f Pire : {min(scores):.2f}/10\")\n-    \n+\n     # Afficher les p\u00e9pites\n     toutes_pepites = []\n     for result in results:\n         toutes_pepites.extend(result.pepites_trouvees)\n-    \n+\n     if toutes_pepites:\n         print(f\"\\n\ud83d\udc8e P\u00c9PITES TROUV\u00c9ES ({len(toutes_pepites)}):\")\n         for pepite in toutes_pepites[:10]:  # Top 10\n             print(f\"  - {pepite}\")\n \n+\n if __name__ == \"__main__\":\n-    main() \n\\ No newline at end of file\n+    main()\n",
      "error": "would reformat /Volumes/T7/athalia-dev-setup/athalia_core/__init__.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/agents/unified_agent.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/analytics.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/agents/context_prompt.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/advanced_analytics.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/dashboard_unified.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/audit.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/user_profiles_advanced.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/ast_analyzer.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_server.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_engine.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust_broken.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/auto_cicd.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/architecture_analyzer.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/classification/__init__.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/auto_correction_advanced.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/ci.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/cleanup.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_types.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_classifier.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/cli.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/dashboard.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/auto_cleaner.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/audit_distiller.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/cache_manager.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/correction_distiller.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/quality_scorer.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/code_linter.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/adaptive_distillation.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/code_genetics.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/multimodal_distiller.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/predictive_cache.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/auto_tester.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/distillation/response_distiller.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/auto_documenter.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/error_codes.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/error_handling.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/config_manager.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/generation.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/onboarding.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/generation_simple.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_analyzer.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/ready_check.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/robotics/__init__.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/main.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/plugins_validator.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/correction_optimizer.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_memory.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/performance_analyzer.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/project_importer.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/logger_advanced.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/robotics/docker_robotics.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/robotics/reachy_auditor.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/security.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/templates/__init__.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/robotics/ros2_validator.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/pattern_detector.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/templates/artistic_templates.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/templates/base_templates.py\nwould reformat /Volumes/T7/athalia-dev-setup/bin/ath-audit.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/robotics/robotics_ci.py\nwould reformat /Volumes/T7/athalia-dev-setup/bin/ath-build.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_auditor.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/robotics/rust_analyzer.py\nwould reformat /Volumes/T7/athalia-dev-setup/bin/ath-lint.py\nwould reformat /Volumes/T7/athalia-dev-setup/plugins/__init__.py\nwould reformat /Volumes/T7/athalia-dev-setup/bin/ath-test.py\nwould reformat /Volumes/T7/athalia-dev-setup/plugins/export_docker_plugin.py\nwould reformat /Volumes/T7/athalia-dev-setup/bin/ath-coverage.py\nwould reformat /Volumes/T7/athalia-dev-setup/plugins/hello_plugin.py\nwould reformat /Volumes/T7/athalia-dev-setup/plugins/plugins_manager.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/security_auditor.py\nwould reformat /Volumes/T7/athalia-dev-setup/plugins/plugins_validator.py\nwould reformat /Volumes/T7/athalia-dev-setup/tests/__init__.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_unified.py\nwould reformat /Volumes/T7/athalia-dev-setup/tests/debug_correction.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/robotics_ci.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/ros2_validator.py\nwould reformat /Volumes/T7/athalia-dev-setup/tests/correction_chai\u0302nes.py\nwould reformat /Volumes/T7/athalia-dev-setup/tests/correction_finale.py\nwould reformat /Volumes/T7/athalia-dev-setup/tools/analysis/verification_integration_simple.py\nwould reformat /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py\nwould reformat /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_documentation.py\nwould reformat /Volumes/T7/athalia-dev-setup/tests/optimize_performance.py\nwould reformat /Volumes/T7/athalia-dev-setup/scripts/validation_continue.py\nwould reformat /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_archives.py\nwould reformat /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_old_data.py\nwould reformat /Volumes/T7/athalia-dev-setup/tests/audit.py\nwould reformat /Volumes/T7/athalia-dev-setup/scripts/validation_objective.py\nwould reformat /Volumes/T7/athalia-dev-setup/tools/analysis/audit_complet_dossiers.py\nwould reformat /Volumes/T7/athalia-dev-setup/tests/audit_complet_dossiers.py\n\nOh no! \ud83d\udca5 \ud83d\udc94 \ud83d\udca5\n91 files would be reformatted, 14 files would be left unchanged.\n",
      "duration": 1.8536388874053955,
      "critical": false
    },
    {
      "name": "isort",
      "description": "Organisation des imports",
      "success": false,
      "output": "--- /Volumes/T7/athalia-dev-setup/athalia_unified.py:before\t2025-07-29 19:56:18.640000\n+++ /Volumes/T7/athalia-dev-setup/athalia_unified.py:after\t2025-07-29 20:12:22.131570\n@@ -5,10 +5,10 @@\n Interface unifi\u00e9e pour tous les modules Athalia\n \"\"\"\n \n+import argparse\n+import logging\n import os\n import sys\n-import argparse\n-import logging\n \n # Configuration du logging\n logging.basicConfig(\n--- /Volumes/T7/athalia-dev-setup/tests/__init__.py:before\t2025-07-29 20:02:53.970000\n+++ /Volumes/T7/athalia-dev-setup/tests/__init__.py:after\t2025-07-29 20:12:22.160859\n@@ -6,6 +6,7 @@\n \"\"\"\n \n from pathlib import Path\n+\n \n def _protect_test_directory():\n     \"\"\"Prot\u00e8ge le r\u00e9pertoire tests contre la cr\u00e9ation automatique de fichiers.\"\"\"\n--- /Volumes/T7/athalia-dev-setup/tests/test_cleanup.py:before\t2025-07-29 19:56:10.500000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_cleanup.py:after\t2025-07-29 20:12:22.162998\n@@ -3,8 +3,9 @@\n \"\"\"\n Tests pour le module cleanup\n \"\"\"\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n \n \n def test_clean_old_tests_and_caches(tmp_path):\n--- /Volumes/T7/athalia-dev-setup/tests/audit.py:before\t2025-07-29 19:56:10.600000\n+++ /Volumes/T7/athalia-dev-setup/tests/audit.py:after\t2025-07-29 20:12:22.165539\n@@ -5,12 +5,13 @@\n Analyse le code, d\u00e9tecte la dette technique, et propose des am\u00e9liorations.\n \"\"\"\n \n-from typing import Dict, List, Any\n+import ast\n+import builtins\n import json\n+import logging\n import os\n-import ast\n-import logging\n-import builtins\n+from typing import Any, Dict, List\n+\n \n class ProjectAuditor:\n     \"\"\"Auditeur intelligent de projets g\u00e9n\u00e9r\u00e9s.\"\"\"\n--- /Volumes/T7/athalia-dev-setup/tests/test_i18n.py:before\t2025-07-29 19:56:10.710000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_i18n.py:after\t2025-07-29 20:12:22.167076\n@@ -3,8 +3,9 @@\n \"\"\"\n Tests pour le module i18n\n \"\"\"\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n \n \n def test_i18n_module_import():\n@@ -39,8 +40,8 @@\n def test_translation_consistency():\n     \"\"\"Test de la coh\u00e9rence des traductions\"\"\"\n     try:\n-        from athalia_core.i18n import fr, en\n-        \n+        from athalia_core.i18n import en, fr\n+\n         # V\u00e9rifie que les deux modules ont les m\u00eames cl\u00e9s\n         fr_keys = set(fr.translations.keys())\n         en_keys = set(en.translations.keys())\n--- /Volumes/T7/athalia-dev-setup/tests/audit_complet_dossiers.py:before\t2025-07-29 19:56:10.820000\n+++ /Volumes/T7/athalia-dev-setup/tests/audit_complet_dossiers.py:after\t2025-07-29 20:12:22.169814\n@@ -6,11 +6,11 @@\n V\u00e9rifie : utilit\u00e9, impl\u00e9mentation, tests, documentation, int\u00e9gration.\n \"\"\"\n \n+import ast\n import sys\n+from dataclasses import dataclass, field\n from pathlib import Path\n-import ast\n from typing import List, Optional\n-from dataclasses import dataclass, field\n \n # Ajouter le r\u00e9pertoire parent au path\n sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n--- /Volumes/T7/athalia-dev-setup/tests/debug_correction.py:before\t2025-07-29 19:56:10.920000\n+++ /Volumes/T7/athalia-dev-setup/tests/debug_correction.py:after\t2025-07-29 20:12:22.170673\n@@ -6,6 +6,7 @@\n \n import sys\n from pathlib import Path\n+\n from athalia_core.correction_optimizer import optimize_correction\n \n # Ajouter le r\u00e9pertoire parent au path\n--- /Volumes/T7/athalia-dev-setup/tests/correction_finale.py:before\t2025-07-29 19:56:11.130000\n+++ /Volumes/T7/athalia-dev-setup/tests/correction_finale.py:after\t2025-07-29 20:12:22.172225\n@@ -7,6 +7,7 @@\n \n import os\n import re\n+\n \n def corriger_fichier(file_path):\n     \"\"\"Corrige un fichier en rempla\u00e7ant les patterns probl\u00e9matiques\"\"\"\n--- /Volumes/T7/athalia-dev-setup/tests/test_plugin_complet.py:before\t2025-07-29 19:56:11.230000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_plugin_complet.py:after\t2025-07-29 20:12:22.174946\n@@ -4,13 +4,15 @@\n V\u00e9rifie tous les composants n\u00e9cessaires au fonctionnement\n \"\"\"\n \n+import json\n import os\n+import subprocess\n import sys\n-import json\n-import subprocess\n-import requests\n import time\n from pathlib import Path\n+\n+import requests\n+\n \n def print_status(message, status=\"\u2139\ufe0f\"):\n     print(f\"{status} {message}\")\n--- /Volumes/T7/athalia-dev-setup/tests/test_plugins.py:before\t2025-07-29 20:02:53.990000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_plugins.py:after\t2025-07-29 20:12:22.176172\n@@ -7,6 +7,7 @@\n import shutil\n import tempfile\n import unittest\n+\n \n # Simulation des fonctions pour les tests\n def list_plugins():\n--- /Volumes/T7/athalia-dev-setup/tests/test_adaptive_distillation.py:before\t2025-07-29 19:56:11.440000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_adaptive_distillation.py:after\t2025-07-29 20:12:22.177250\n@@ -1,6 +1,8 @@\n+import os\n import unittest\n-import os\n+\n from athalia_core.distillation.adaptive_distillation import AdaptiveDistiller\n+\n \n class TestAdaptiveDistiller(unittest.TestCase):\n     def setUp(self):\n--- /Volumes/T7/athalia-dev-setup/tests/optimize_performance.py:before\t2025-07-29 19:56:11.540000\n+++ /Volumes/T7/athalia-dev-setup/tests/optimize_performance.py:after\t2025-07-29 20:12:22.179016\n@@ -6,16 +6,15 @@\n Auteur: Athalia Team\n \"\"\"\n \n+# Standard library imports\n+import argparse\n import os\n+import subprocess\n import sys\n+import tempfile\n import time\n-import subprocess\n from pathlib import Path\n from typing import Dict, List\n-\n-# Standard library imports\n-import argparse\n-import tempfile\n \n # Third party imports\n try:\n--- /Volumes/T7/athalia-dev-setup/tests/test_advanced_analytics_unit.py:before\t2025-07-29 19:56:11.640000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_advanced_analytics_unit.py:after\t2025-07-29 20:12:22.180034\n@@ -1,6 +1,8 @@\n+import os\n import unittest\n-import os\n+\n from athalia_core.advanced_analytics import AdvancedAnalytics\n+\n \n class TestAdvancedAnalytics(unittest.TestCase):\n     def setUp(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_ai_robust_integration.py:before\t2025-07-29 20:02:53.980000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_ai_robust_integration.py:after\t2025-07-29 20:12:22.182880\n@@ -1,10 +1,12 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n import os\n-from athalia_core.ai_robust import RobustAI, AIModel, PromptContext\n+import tempfile\n import time\n+\n import pytest\n-import tempfile\n+\n+from athalia_core.ai_robust import AIModel, PromptContext, RobustAI\n \n # Import conditionnel de psutil\n try:\n--- /Volumes/T7/athalia-dev-setup/tests/test_athalia_simple.py:before\t2025-07-29 20:02:53.980000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_athalia_simple.py:after\t2025-07-29 20:12:22.183960\n@@ -3,14 +3,15 @@\n \"\"\"\n Tests simples pour Athalia\n \"\"\"\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n \n \n def test_athalia_core_import():\n     \"\"\"Test d'import du module core\"\"\"\n     try:\n-        from athalia_core import audit, cleanup, analytics\n+        from athalia_core import analytics, audit, cleanup\n         assert audit is not None\n         assert cleanup is not None\n         assert analytics is not None\n--- /Volumes/T7/athalia-dev-setup/tests/test_agent_network.py:before\t2025-07-29 19:56:12.480000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_agent_network.py:after\t2025-07-29 20:12:22.185598\n@@ -5,10 +5,10 @@\n Corrig\u00e9 apr\u00e8s consolidation des agents\n \"\"\"\n \n+import os\n+import sys\n import unittest\n-from unittest.mock import patch, MagicMock\n-import sys\n-import os\n+from unittest.mock import MagicMock, patch\n \n # Ajouter le chemin du projet\n sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n@@ -21,8 +21,14 @@\n         \"\"\"Test basique des agents unifi\u00e9s\"\"\"\n         try:\n             # Test d'import des nouveaux modules unifi\u00e9s\n-            from athalia_core.agents.unified_agent import UnifiedAgent, AuditAgent, CorrectionAgent, SynthesisAgent, QwenAgent\n-            \n+            from athalia_core.agents.unified_agent import (\n+                AuditAgent,\n+                CorrectionAgent,\n+                QwenAgent,\n+                SynthesisAgent,\n+                UnifiedAgent,\n+            )\n+\n             # Test de cr\u00e9ation d'agents\n             unified_agent = UnifiedAgent(\"test\")\n             audit_agent = AuditAgent()\n@@ -43,8 +49,7 @@\n         \"\"\"Test des imports d'agents unifi\u00e9s\"\"\"\n         try:\n             # Test des imports corrig\u00e9s\n-            from athalia_core.agents import context_prompt\n-            from athalia_core.agents import unified_agent\n+            from athalia_core.agents import context_prompt, unified_agent\n             \n             self.assertTrue(True, \"Tous les imports d'agents unifi\u00e9s fonctionnent\")\n             \n--- /Volumes/T7/athalia-dev-setup/tests/test_ai_robust.py:before\t2025-07-29 19:56:12.600000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_ai_robust.py:after\t2025-07-29 20:12:22.187439\n@@ -1,11 +1,12 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-from athalia_core.ai_robust import RobustAI, AIModel, PromptContext\n import os\n-\n-import pytest\n import tempfile\n from unittest.mock import patch\n+\n+import pytest\n+\n+from athalia_core.ai_robust import AIModel, PromptContext, RobustAI\n \n \"\"\"\n Tests pour le module dict_data'IA robuste.\n@@ -184,6 +185,7 @@\n \n def test_fallback_ia_qwen_mistral(monkeypatch):\n     from athalia_core.ai_robust import fallback_ia\n+\n     # Mock les requ\u00eates pour Qwen et Mistral\n     def mock_query_qwen(prompt):\n         return \"R\u00e9ponse Qwen\"\n--- /Volumes/T7/athalia-dev-setup/tests/test_continue_models.py:before\t2025-07-29 20:02:53.980000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_continue_models.py:after\t2025-07-29 20:12:22.188360\n@@ -4,6 +4,7 @@\n Test de pr\u00e9sence des mod\u00e8les dans la config Continue\n \"\"\"\n import os\n+\n import pytest\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/test_ai_robust_unit.py:before\t2025-07-29 19:56:12.790000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_ai_robust_unit.py:after\t2025-07-29 20:12:22.189279\n@@ -1,5 +1,7 @@\n import unittest\n+\n from athalia_core import ai_robust\n+\n \n class TestAiRobust(unittest.TestCase):\n     def test_robust_ai_instance(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_analytics.py:before\t2025-07-29 19:56:12.890000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_analytics.py:after\t2025-07-29 20:12:22.191550\n@@ -3,17 +3,18 @@\n \"\"\"\n Tests pour le module analytics\n \"\"\"\n-import pytest\n import tempfile\n from pathlib import Path\n+\n+import pytest\n \n # Import conditionnel du module analytics\n try:\n     from athalia_core.analytics import (\n-        analyze_project, \n-        generate_heatmap_data, \n+        analyze_project,\n+        generate_analytics_html,\n+        generate_heatmap_data,\n         generate_technical_debt_analysis,\n-        generate_analytics_html\n     )\n     ANALYTICS_AVAILABLE = True\n except ImportError:\n--- /Volumes/T7/athalia-dev-setup/tests/test_security_auditor_complete.py:before\t2025-07-29 19:56:12.990000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_security_auditor_complete.py:after\t2025-07-29 20:12:22.194396\n@@ -3,17 +3,20 @@\n Couverture : 100% des fonctionnalit\u00e9s de s\u00e9curit\u00e9\n Tests : 32 tests unitaires et d'int\u00e9gration\n \"\"\"\n+import base64\n+import hashlib\n+import json\n+import os\n+import re\n+import subprocess\n+import tempfile\n+from pathlib import Path\n+from unittest.mock import MagicMock, Mock, patch\n+\n import pytest\n-import tempfile\n-import os\n-import subprocess\n-import re\n-from pathlib import Path\n-from unittest.mock import Mock, patch, MagicMock\n-import json\n-import hashlib\n-import base64\n+\n from athalia_core.security_auditor import SecurityAuditor\n+\n \n class TestSecurityAuditor:\n     def setup_method(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_analytics_unit.py:before\t2025-07-29 19:56:13.110000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_analytics_unit.py:after\t2025-07-29 20:12:22.195885\n@@ -1,6 +1,8 @@\n import unittest\n from unittest.mock import patch\n+\n from athalia_core import analytics\n+\n \n class TestAnalytics(unittest.TestCase):\n     def setUp(self):\n@@ -11,8 +13,8 @@\n         ]\n \n     def test_generate_heatmap_data(self):\n+        import os\n         import tempfile\n-        import os\n         with tempfile.TemporaryDirectory() as tmpdir:\n             # Cr\u00e9er des fichiers de test\n             open(os.path.join(tmpdir, 'main.py'), 'w').close()\n@@ -23,8 +25,8 @@\n             self.assertIn('max_complexity', data)\n \n     def test_generate_technical_debt_analysis(self):\n+        import os\n         import tempfile\n-        import os\n         with tempfile.TemporaryDirectory() as tmpdir:\n             # Cr\u00e9er des fichiers de test\n             with open(os.path.join(tmpdir, 'main.py'), 'w') as f:\n@@ -35,8 +37,8 @@\n             self.assertIn('recommendations', data)\n \n     def test_generate_analytics_html(self):\n+        import os\n         import tempfile\n-        import os\n         with tempfile.TemporaryDirectory() as tmpdir:\n             # Cr\u00e9er des fichiers de test\n             open(os.path.join(tmpdir, 'main.py'), 'w').close()\n@@ -46,8 +48,8 @@\n             self.assertIn('Analytics', html)\n \n     def test_analyze_project(self):\n+        import os\n         import tempfile\n-        import os\n         with tempfile.TemporaryDirectory() as tmpdir:\n             # Cr\u00e9er des fichiers de test\n             open(os.path.join(tmpdir, 'main.py'), 'w').close()\n--- /Volumes/T7/athalia-dev-setup/tests/test_predictive_cache.py:before\t2025-07-29 19:56:13.230000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_predictive_cache.py:after\t2025-07-29 20:12:22.196837\n@@ -1,6 +1,8 @@\n+import time\n import unittest\n-import time\n+\n from athalia_core.distillation.predictive_cache import PredictiveCache\n+\n \n class TestPredictiveCache(unittest.TestCase):\n     def test_set_get(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_coverage_threshold.py:before\t2025-07-29 20:02:53.980000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_coverage_threshold.py:after\t2025-07-29 20:12:22.198419\n@@ -2,8 +2,9 @@\n Test de seuil de couverture de code\n V\u00e9rifie que la couverture de code est suffisante\n \"\"\"\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n \n \n class TestCoverageThreshold:\n--- /Volumes/T7/athalia-dev-setup/tests/test_code_linter_complete.py:before\t2025-07-29 19:56:13.470000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_code_linter_complete.py:after\t2025-07-29 20:12:22.201081\n@@ -3,15 +3,18 @@\n Couverture : 100% des fonctionnalit\u00e9s de linting\n Tests : 28 tests unitaires et d'int\u00e9gration\n \"\"\"\n-import pytest\n-import tempfile\n+import ast\n+import json\n import os\n import subprocess\n+import tempfile\n from pathlib import Path\n-from unittest.mock import Mock, patch, MagicMock\n-import json\n-import ast\n+from unittest.mock import MagicMock, Mock, patch\n+\n+import pytest\n+\n from athalia_core.code_linter import CodeLinter\n+\n \n class TestCodeLinter:\n     def setup_method(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_audit_agent.py:before\t2025-07-29 19:56:13.590000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_audit_agent.py:after\t2025-07-29 20:12:22.203385\n@@ -4,12 +4,12 @@\n Tests unitaires et d'int\u00e9gration pour AuditAgent\n \"\"\"\n \n-import unittest\n-import tempfile\n import os\n import sys\n-from unittest.mock import patch, MagicMock\n+import tempfile\n+import unittest\n from pathlib import Path\n+from unittest.mock import MagicMock, patch\n \n # Ajout du chemin du projet pour les imports\n sys.path.insert(0, str(Path(__file__).parent.parent))\n@@ -167,7 +167,7 @@\n     def test_agent_performance(self, mock_query_qwen):\n         \"\"\"Test de performance de l'agent\"\"\"\n         import time\n-        \n+\n         # Configuration du mock\n         mock_query_qwen.return_value = \"Test de performance\"\n         \n--- /Volumes/T7/athalia-dev-setup/tests/test_plugins_validator.py:before\t2025-07-29 19:56:11.970000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_plugins_validator.py:after\t2025-07-29 20:12:22.204360\n@@ -1,9 +1,9 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import os\n+import tempfile\n+\n from athalia_core.plugins_validator import validate_plugin\n-import os\n-\n-import tempfile\n \n \n def test_validate_plugin_ok():\n--- /Volumes/T7/athalia-dev-setup/tests/test_audit_intelligent.py:before\t2025-07-29 19:56:13.830000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_audit_intelligent.py:after\t2025-07-29 20:12:22.205974\n@@ -6,11 +6,12 @@\n import os\n import shutil\n import tempfile\n+\n import pytest\n \n try:\n+    from athalia_core.audit import audit_project_intelligent\n     from athalia_core.intelligent_auditor import IntelligentAuditor\n-    from athalia_core.audit import audit_project_intelligent\n     ProjectAuditor = IntelligentAuditor  # Alias pour compatibilit\u00e9\n     generate_audit_report = None\n except ImportError:\n--- /Volumes/T7/athalia-dev-setup/tests/test_project_importer.py:before\t2025-07-29 19:56:13.970000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_project_importer.py:after\t2025-07-29 20:12:22.206949\n@@ -1,9 +1,9 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import os\n+import tempfile\n+\n from athalia_core.project_importer import ProjectImporter\n-import os\n-\n-import tempfile\n \n \n def test_project_import_concept():\n--- /Volumes/T7/athalia-dev-setup/tests/test_auto_cicd_unit.py:before\t2025-07-29 19:56:14.090000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_auto_cicd_unit.py:after\t2025-07-29 20:12:22.208661\n@@ -1,8 +1,10 @@\n+import os\n+import tempfile\n import unittest\n-import tempfile\n-import os\n from pathlib import Path\n+\n from athalia_core.auto_cicd import AutoCICD, generate_github_ci_yaml\n+\n \n class TestAutoCICD(unittest.TestCase):\n     def setUp(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_hardcoded_paths.py:before\t2025-07-29 20:02:53.990000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_hardcoded_paths.py:after\t2025-07-29 20:12:22.210059\n@@ -4,10 +4,12 @@\n Tests pour d\u00e9tecter les chemins hardcod\u00e9s\n \"\"\"\n \n-import pytest\n+import os\n import re\n from pathlib import Path\n-import os\n+\n+import pytest\n+\n \n class TestHardcodedPaths:\n     \"\"\"Tests pour d\u00e9tecter les chemins hardcod\u00e9s\"\"\"\n--- /Volumes/T7/athalia-dev-setup/tests/test_auto_cleaner_unit.py:before\t2025-07-29 19:56:14.360000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_auto_cleaner_unit.py:after\t2025-07-29 20:12:22.211598\n@@ -1,8 +1,10 @@\n+import os\n+import tempfile\n import unittest\n-import tempfile\n-import os\n from pathlib import Path\n+\n from athalia_core.auto_cleaner import AutoCleaner\n+\n \n class TestAutoCleaner(unittest.TestCase):\n     def setUp(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_ready_check.py:before\t2025-07-29 19:56:14.480000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_ready_check.py:after\t2025-07-29 20:12:22.212606\n@@ -4,9 +4,10 @@\n Tests pour ready_check.py\n \"\"\"\n \n+import os\n+import sys\n+\n import pytest\n-import sys\n-import os\n \n # Ajouter le chemin du projet\n sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n--- /Volumes/T7/athalia-dev-setup/tests/test_auto_correction_advanced_complete.py:before\t2025-07-29 19:56:14.600000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_auto_correction_advanced_complete.py:after\t2025-07-29 20:12:22.215263\n@@ -4,20 +4,22 @@\n Tests unitaires et d'int\u00e9gration pour AutoCorrectionAvancee\n \"\"\"\n \n+import os\n+import shutil\n+import sqlite3\n+import sys\n+import tempfile\n import unittest\n-import tempfile\n-import os\n-import sys\n-import sqlite3\n-from unittest.mock import patch, MagicMock, mock_open\n from pathlib import Path\n-import shutil\n+from unittest.mock import MagicMock, mock_open, patch\n \n # Ajout du chemin du projet pour les imports\n sys.path.insert(0, str(Path(__file__).parent.parent))\n \n try:\n-    from athalia_core.advanced_modules.auto_correction_advanced import AutoCorrectionAvancee\n+    from athalia_core.advanced_modules.auto_correction_advanced import (\n+        AutoCorrectionAvancee,\n+    )\n     AUTO_CORRECTION_AVAILABLE = True\n except ImportError:\n     AUTO_CORRECTION_AVAILABLE = False\n--- /Volumes/T7/athalia-dev-setup/tests/test_cache_simple.py:before\t2025-07-29 19:56:14.730000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_cache_simple.py:after\t2025-07-29 20:12:22.217219\n@@ -4,17 +4,17 @@\n Tests simples pour le cache.\n \"\"\"\n \n-import unittest\n-import tempfile\n import os\n import shutil\n+import sys\n+import tempfile\n import time\n-import sys\n+import unittest\n from datetime import datetime, timedelta\n \n # Import direct du cache sans d\u00e9pendances\n sys.path.append('athalia_core')\n-from cache_manager import AnalysisCache, cached_analysis, get_cache_stats, clear_cache\n+from cache_manager import AnalysisCache, cached_analysis, clear_cache, get_cache_stats\n \n \n class TestCacheSimple(unittest.TestCase):\n--- /Volumes/T7/athalia-dev-setup/tests/test_imports_all.py:before\t2025-07-29 20:02:53.990000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_imports_all.py:after\t2025-07-29 20:12:22.219160\n@@ -2,10 +2,11 @@\n Test d'importation exhaustive de tous les modules\n V\u00e9rifie que tous les modules peuvent \u00eatre import\u00e9s sans erreur\n \"\"\"\n+import os\n+import sys\n+from pathlib import Path\n+\n import pytest\n-import sys\n-import os\n-from pathlib import Path\n \n \n class TestImportsAll:\n--- /Volumes/T7/athalia-dev-setup/tests/test_auto_correction_avancee.py:before\t2025-07-29 19:56:14.970000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_auto_correction_avancee.py:after\t2025-07-29 20:12:22.220725\n@@ -5,11 +5,11 @@\n Corrig\u00e9 apr\u00e8s r\u00e9organisation des modules\n \"\"\"\n \n+import os\n+import shutil\n+import sys\n+import tempfile\n import unittest\n-import tempfile\n-import os\n-import sys\n-import shutil\n from pathlib import Path\n \n # Ajouter le chemin du projet\n@@ -38,7 +38,9 @@\n     def test_import_dashboard_unified(self):\n         \"\"\"Test d'import du dashboard unifi\u00e9\"\"\"\n         try:\n-            from athalia_core.advanced_modules.dashboard_unified import DashboardUnifieSimple\n+            from athalia_core.advanced_modules.dashboard_unified import (\n+                DashboardUnifieSimple,\n+            )\n             self.assertTrue(True, \"Import r\u00e9ussi\")\n         except ImportError as e:\n             self.skipTest(f\"Module dashboard non disponible: {e}\")\n@@ -53,9 +55,11 @@\n     def test_advanced_modules_structure(self):\n         \"\"\"Test de la structure des modules avanc\u00e9s\"\"\"\n         try:\n-            from athalia_core.advanced_modules import auto_correction_advanced\n-            from athalia_core.advanced_modules import dashboard_unified\n-            from athalia_core.advanced_modules import user_profiles_advanced\n+            from athalia_core.advanced_modules import (\n+                auto_correction_advanced,\n+                dashboard_unified,\n+                user_profiles_advanced,\n+            )\n             \n             self.assertTrue(True, \"Structure des modules avanc\u00e9s correcte\")\n         except ImportError as e:\n--- /Volumes/T7/athalia-dev-setup/tests/test_cli_complete.py:before\t2025-07-29 20:02:53.980000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_cli_complete.py:after\t2025-07-29 20:12:22.225969\n@@ -6,19 +6,20 @@\n \"\"\"\n \n import os\n+import shutil\n import sys\n import tempfile\n-import shutil\n from pathlib import Path\n-from unittest.mock import Mock, patch, MagicMock, mock_open\n+from unittest.mock import MagicMock, Mock, mock_open, patch\n+\n+import click\n import pytest\n-import click\n import yaml\n \n # Ajouter le r\u00e9pertoire parent au path pour les imports\n sys.path.insert(0, str(Path(__file__).parent.parent))\n \n-from athalia_core.cli import cli, generate, audit, ai_status, test_ai  # noqa: E402\n+from athalia_core.cli import ai_status, audit, cli, generate, test_ai  # noqa: E402\n \n \n class TestCLIComplete:\n--- /Volumes/T7/athalia-dev-setup/tests/test_auto_documenter_unit.py:before\t2025-07-29 19:56:15.180000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_auto_documenter_unit.py:after\t2025-07-29 20:12:22.227209\n@@ -1,6 +1,8 @@\n+import os\n import unittest\n-import os\n+\n from athalia_core.auto_documenter import AutoDocumenter\n+\n \n class TestAutoDocumenter(unittest.TestCase):\n     def setUp(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_auto_tester_unit.py:before\t2025-07-29 19:56:15.280000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_auto_tester_unit.py:after\t2025-07-29 20:12:22.228290\n@@ -1,8 +1,10 @@\n+import os\n+import tempfile\n import unittest\n-import tempfile\n-import os\n from pathlib import Path\n+\n from athalia_core.auto_tester import AutoTester\n+\n \n class TestAutoTester(unittest.TestCase):\n     def setUp(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_autocomplete_server.py:before\t2025-07-29 19:56:15.390000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_autocomplete_server.py:after\t2025-07-29 20:12:22.230662\n@@ -1,10 +1,12 @@\n+from unittest.mock import patch\n+\n import pytest\n import requests\n-from unittest.mock import patch\n \n # V\u00e9rification de la disponibilit\u00e9 de FastAPI\n try:\n     from fastapi.testclient import TestClient\n+\n     from athalia_core.autocomplete_server import app\n     FASTAPI_AVAILABLE = True\n     client = TestClient(app)\n--- /Volumes/T7/athalia-dev-setup/tests/test_profils_utilisateur_avances.py:before\t2025-07-29 19:56:15.500000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_profils_utilisateur_avances.py:after\t2025-07-29 20:12:22.231994\n@@ -5,11 +5,11 @@\n Corrig\u00e9 apr\u00e8s r\u00e9organisation des modules\n \"\"\"\n \n+import os\n+import shutil\n+import sys\n+import tempfile\n import unittest\n-import tempfile\n-import os\n-import sys\n-import shutil\n from pathlib import Path\n \n # Ajouter le chemin du projet\n@@ -30,7 +30,9 @@\n     def test_import_user_profiles(self):\n         \"\"\"Test d'import des profils utilisateur\"\"\"\n         try:\n-            from athalia_core.advanced_modules.user_profiles_advanced import GestionnaireProfilsAvances\n+            from athalia_core.advanced_modules.user_profiles_advanced import (\n+                GestionnaireProfilsAvances,\n+            )\n             self.assertTrue(True, \"Import r\u00e9ussi\")\n         except ImportError as e:\n             self.skipTest(f\"Module profils utilisateur non disponible: {e}\")\n@@ -46,8 +48,10 @@\n     def test_profiles_functionality(self):\n         \"\"\"Test de la fonctionnalit\u00e9 des profils\"\"\"\n         try:\n-            from athalia_core.advanced_modules.user_profiles_advanced import GestionnaireProfilsAvances\n-            \n+            from athalia_core.advanced_modules.user_profiles_advanced import (\n+                GestionnaireProfilsAvances,\n+            )\n+\n             # Test de cr\u00e9ation du gestionnaire\n             manager = GestionnaireProfilsAvances(self.db_path)\n             self.assertIsNotNone(manager)\n--- /Volumes/T7/athalia-dev-setup/tests/test_benchmark_critical.py:before\t2025-07-29 19:56:15.610000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_benchmark_critical.py:after\t2025-07-29 20:12:22.233880\n@@ -1,7 +1,8 @@\n-import pytest\n import importlib\n import os\n import sys\n+\n+import pytest\n \n # V\u00e9rification de la disponibilit\u00e9 de pytest-benchmark\n try:\n--- /Volumes/T7/athalia-dev-setup/tests/test_correction_optimizer_complete.py:before\t2025-07-29 19:56:15.710000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_correction_optimizer_complete.py:after\t2025-07-29 20:12:22.238020\n@@ -4,12 +4,13 @@\n Tests : 35 tests unitaires et d'int\u00e9gration\n \"\"\"\n \n+import json\n+import os\n+import tempfile\n+from pathlib import Path\n+from unittest.mock import MagicMock, Mock, patch\n+\n import pytest\n-import tempfile\n-import os\n-from pathlib import Path\n-from unittest.mock import Mock, patch, MagicMock\n-import json\n import yaml\n \n # Import du module \u00e0 tester\n--- /Volumes/T7/athalia-dev-setup/tests/test_ci_robust.py:before\t2025-07-29 19:56:15.830000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_ci_robust.py:after\t2025-07-29 20:12:22.240231\n@@ -8,14 +8,15 @@\n Tests plus approfondis pour validation de qualit\u00e9\n \"\"\"\n \n+import json\n+import os\n+import subprocess\n+import sys\n+import time\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n-import sys\n-import os\n-import json\n import yaml\n-import subprocess\n-import time\n \n \n class TestCIRobust:\n--- /Volumes/T7/athalia-dev-setup/tests/test_no_polluting_files.py:before\t2025-07-29 20:02:53.990000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_no_polluting_files.py:after\t2025-07-29 20:12:22.241841\n@@ -4,9 +4,11 @@\n Tests pour d\u00e9tecter les fichiers polluants\n \"\"\"\n \n-import pytest\n import os\n from pathlib import Path\n+\n+import pytest\n+\n \n class TestNoPollutingFiles:\n     \"\"\"Tests pour d\u00e9tecter les fichiers polluants\"\"\"\n--- /Volumes/T7/athalia-dev-setup/tests/test_ci_ultra_fast.py:before\t2025-07-29 19:56:16.040000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_ci_ultra_fast.py:after\t2025-07-29 20:12:22.243205\n@@ -4,10 +4,11 @@\n Tests essentiels qui ne doivent jamais bloquer le CI\n \"\"\"\n \n-import pytest\n import os\n import sys\n from pathlib import Path\n+\n+import pytest\n \n \n class TestCIUltraFast:\n--- /Volumes/T7/athalia-dev-setup/tests/test_security_patterns.py:before\t2025-07-29 20:02:53.990000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_security_patterns.py:after\t2025-07-29 20:12:22.245209\n@@ -2,10 +2,11 @@\n Test de d\u00e9tection des patterns de s\u00e9curit\u00e9 dangereux\n V\u00e9rifie qu'il n'y a pas de code dangereux dans le projet\n \"\"\"\n-import pytest\n+import os\n import re\n from pathlib import Path\n-import os\n+\n+import pytest\n \n \n class TestSecurityPatterns:\n--- /Volumes/T7/athalia-dev-setup/tests/test_security_comprehensive.py:before\t2025-07-29 19:56:16.240000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_security_comprehensive.py:after\t2025-07-29 20:12:22.247610\n@@ -5,11 +5,12 @@\n Tests complets pour am\u00e9liorer la couverture du module security.py.\n \"\"\"\n \n+import os\n+import tempfile\n+from pathlib import Path\n+from unittest.mock import Mock, mock_open, patch\n+\n import pytest\n-import tempfile\n-import os\n-from pathlib import Path\n-from unittest.mock import Mock, patch, mock_open\n \n # Import du module \u00e0 tester\n from athalia_core.security import security_audit_project\n--- /Volumes/T7/athalia-dev-setup/tests/test_error_handling.py:before\t2025-07-29 19:56:16.580000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_error_handling.py:after\t2025-07-29 20:12:22.249852\n@@ -4,18 +4,26 @@\n Tests pour le syst\u00e8me de gestion d'erreurs d'Athalia\n \"\"\"\n \n+import os\n+import tempfile\n import unittest\n-import tempfile\n-import os\n from pathlib import Path\n \n from athalia_core.error_codes import (\n-    ErrorCode, ErrorSeverity, get_error_description, \n-    get_error_severity, format_error_message\n+    ErrorCode,\n+    ErrorSeverity,\n+    format_error_message,\n+    get_error_description,\n+    get_error_severity,\n )\n from athalia_core.error_handling import (\n-    AthaliaError, ErrorHandler, get_error_handler,\n-    handle_error, raise_athalia_error, error_handler, ErrorContext\n+    AthaliaError,\n+    ErrorContext,\n+    ErrorHandler,\n+    error_handler,\n+    get_error_handler,\n+    handle_error,\n+    raise_athalia_error,\n )\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/test_requirements_consistency.py:before\t2025-07-29 19:56:16.730000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_requirements_consistency.py:after\t2025-07-29 20:12:22.251976\n@@ -2,9 +2,10 @@\n Test de coh\u00e9rence des d\u00e9pendances\n V\u00e9rifie que les fichiers de d\u00e9pendances sont coh\u00e9rents\n \"\"\"\n+import os\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n-import os\n \n \n class TestRequirementsConsistency:\n--- /Volumes/T7/athalia-dev-setup/tests/test_code_genetics.py:before\t2025-07-29 19:56:16.840000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_code_genetics.py:after\t2025-07-29 20:12:22.253037\n@@ -1,5 +1,7 @@\n import unittest\n+\n from athalia_core.distillation.code_genetics import CodeGenetics\n+\n \n class TestCodeGenetics(unittest.TestCase):\n     def setUp(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_robotics_ci_complete.py:before\t2025-07-29 20:11:22.380000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_robotics_ci_complete.py:after\t2025-07-29 20:12:22.255534\n@@ -3,13 +3,16 @@\n Couverture : 100% des fonctionnalit\u00e9s de CI/CD robotics\n Tests : 20 tests unitaires et d'int\u00e9gration\n \"\"\"\n+import os\n+import subprocess\n+import tempfile\n+from pathlib import Path\n+from unittest.mock import MagicMock, Mock, patch\n+\n import pytest\n-import tempfile\n-import os\n-from pathlib import Path\n-from unittest.mock import Mock, patch, MagicMock\n-import subprocess\n+\n from athalia_core.robotics_ci import RoboticsCI, run_robotics_ci\n+\n \n class TestRoboticsCI:\n     def setup_method(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_ros2_validator_complete.py:before\t2025-07-29 20:12:12.350000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_ros2_validator_complete.py:after\t2025-07-29 20:12:22.258605\n@@ -3,14 +3,17 @@\n Couverture : 100% des fonctionnalit\u00e9s de validation ROS2\n Tests : 20 tests unitaires et d'int\u00e9gration\n \"\"\"\n+import os\n+import subprocess\n+import tempfile\n+import xml.etree.ElementTree as ET\n+from pathlib import Path\n+from unittest.mock import MagicMock, Mock, patch\n+\n import pytest\n-import tempfile\n-import os\n-from pathlib import Path\n-from unittest.mock import Mock, patch, MagicMock\n-import xml.etree.ElementTree as ET\n-import subprocess\n+\n from athalia_core.ros2_validator import ROS2Validator, validate_ros2_package\n+\n \n class TestROS2Validator:\n     def setup_method(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_security.py:before\t2025-07-29 19:56:17.160000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_security.py:after\t2025-07-29 20:12:22.259574\n@@ -1,9 +1,8 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-from athalia_core.security import security_audit_project\n import os\n \n-\n+from athalia_core.security import security_audit_project\n \n \n def test_security_audit_project(tmp_path):\n--- /Volumes/T7/athalia-dev-setup/tests/test_correction.py:before\t2025-07-29 19:56:17.260000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_correction.py:after\t2025-07-29 20:12:22.261398\n@@ -55,9 +55,9 @@\n     \"\"\"Test d'am\u00e9lioration du service de f\"\"\"\n     print(\"\\n\ud83d\ude80 Test d'am\u00e9lioration du service de g\u00e9n\u00e9ration...\")\n     try:\n+        from athalia_core.classification import classify_project\n         from athalia_core.generation import generate_project\n-        from athalia_core.classification import classify_project\n-        \n+\n         # Test de classification\n         idea = \"robot reachy mini wireless yeux qui bouge si f\"\n         project_type = classify_project(idea)\n--- /Volumes/T7/athalia-dev-setup/tests/test_plugins_validator_complete.py:before\t2025-07-29 20:09:42.290000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_plugins_validator_complete.py:after\t2025-07-29 20:12:22.266844\n@@ -3,14 +3,21 @@\n Couverture : 100% des fonctionnalit\u00e9s de validation de plugins\n Tests : 25 tests unitaires et d'int\u00e9gration\n \"\"\"\n+import json\n+import os\n+import tempfile\n+from pathlib import Path\n+from unittest.mock import MagicMock, Mock, patch\n+\n import pytest\n-import tempfile\n-import os\n-from pathlib import Path\n-from unittest.mock import Mock, patch, MagicMock\n-import json\n import yaml\n-from athalia_core.plugins_validator import PluginValidator, validate_plugin, validate_all_plugins\n+\n+from athalia_core.plugins_validator import (\n+    PluginValidator,\n+    validate_all_plugins,\n+    validate_plugin,\n+)\n+\n \n class TestPluginValidator:\n     def setup_method(self):\n--- /Volumes/T7/athalia-dev-setup/tests/test_encoding_utf8.py:before\t2025-07-29 19:56:17.360000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_encoding_utf8.py:after\t2025-07-29 20:12:22.268637\n@@ -2,8 +2,9 @@\n Test de v\u00e9rification de l'encodage UTF-8\n V\u00e9rifie que tous les fichiers sont correctement encod\u00e9s en UTF-8\n \"\"\"\n+from pathlib import Path\n+\n import pytest\n-from pathlib import Path\n \n \n class TestEncodingUTF8:\n@@ -149,7 +150,7 @@\n     def test_consistent_line_endings(self):\n         \"\"\"Test que tous les fichiers ont des fins de ligne coh\u00e9rentes\"\"\"\n         import os\n-        \n+\n         # Exclure les dossiers qui peuvent contenir des fichiers avec des fins de ligne mixtes\n         exclude_dirs = {'.git', '__pycache__', '.venv', 'venv', 'node_modules', 'build', 'dist'}\n         \n--- /Volumes/T7/athalia-dev-setup/tests/test_generation.py:before\t2025-07-29 19:56:17.570000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_generation.py:after\t2025-07-29 20:12:22.270789\n@@ -1,10 +1,11 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-from athalia_core import generation\n import os\n-import pytest\n import shutil\n \n+import pytest\n+\n+from athalia_core import generation\n \n \n def test_save_and_inject(tmp_path):\n--- /Volumes/T7/athalia-dev-setup/tests/test_performance_optimization.py:before\t2025-07-29 19:56:17.670000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_performance_optimization.py:after\t2025-07-29 20:12:22.273431\n@@ -6,13 +6,14 @@\n \"\"\"\n \n import os\n+import subprocess\n import sys\n+import tempfile\n import time\n+from pathlib import Path\n+from unittest.mock import MagicMock, patch\n+\n import pytest\n-import tempfile\n-import subprocess\n-from pathlib import Path\n-from unittest.mock import patch, MagicMock\n \n # Import conditionnel de psutil\n try:\n--- /Volumes/T7/athalia-dev-setup/tests/test_robotics_docker_complete.py:before\t2025-07-29 19:56:17.780000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_robotics_docker_complete.py:after\t2025-07-29 20:12:22.277773\n@@ -4,17 +4,18 @@\n Tests complets pour athalia_core.robotics.docker_robotics\n \"\"\"\n \n+import subprocess\n+import tempfile\n+from pathlib import Path\n+from unittest.mock import Mock, mock_open, patch\n+\n import pytest\n-import tempfile\n-import subprocess\n-from pathlib import Path\n-from unittest.mock import Mock, patch, mock_open\n import yaml\n \n from athalia_core.robotics.docker_robotics import (\n     DockerRoboticsManager,\n     DockerServiceConfig,\n-    DockerValidationResult\n+    DockerValidationResult,\n )\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/test_intelligent_memory.py:before\t2025-07-29 19:56:17.880000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_intelligent_memory.py:after\t2025-07-29 20:12:22.280423\n@@ -4,17 +4,20 @@\n Teste le syst\u00e8me de m\u00e9moire intelligente d'Athalia\n \"\"\"\n \n+import json\n+import os\n+import tempfile\n import unittest\n-import tempfile\n-import os\n-import json\n from datetime import datetime\n-from unittest.mock import patch, MagicMock\n from pathlib import Path\n+from unittest.mock import MagicMock, patch\n \n # Import du module \u00e0 tester\n from athalia_core.intelligent_memory import (\n-    IntelligentMemory, LearningEvent, Prediction, CorrectionSuggestion\n+    CorrectionSuggestion,\n+    IntelligentMemory,\n+    LearningEvent,\n+    Prediction,\n )\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/test_lint_flake8.py:before\t2025-07-29 19:56:17.990000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_lint_flake8.py:after\t2025-07-29 20:12:22.281117\n@@ -4,9 +4,11 @@\n Tests pour le linting flake8\n \"\"\"\n \n-import pytest\n import subprocess\n import sys\n+\n+import pytest\n+\n \n def test_flake8_clean():\n     \"\"\"Test que le code passe flake8 sans erreurs\"\"\"\n--- /Volumes/T7/athalia-dev-setup/tests/test_multi_file_editor.py:before\t2025-07-29 19:56:18.090000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_multi_file_editor.py:after\t2025-07-29 20:12:22.282061\n@@ -1,10 +1,13 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n import os\n+import shutil\n import tempfile\n-import shutil\n+\n import pytest\n+\n from athalia_core.multi_file_editor import MultiFileEditor\n+\n \n def test_apply_corrections_and_rollback():\n     # Pr\u00e9parer des fichiers temporaires\n--- /Volumes/T7/athalia-dev-setup/tests/test_multimodal_distiller.py:before\t2025-07-29 19:56:18.190000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_multimodal_distiller.py:after\t2025-07-29 20:12:22.283159\n@@ -1,6 +1,8 @@\n import unittest\n from unittest.mock import patch\n+\n from athalia_core.distillation.multimodal_distiller import MultimodalDistiller\n+\n \n class TestMultimodalDistiller(unittest.TestCase):\n     @patch.object(MultimodalDistiller, 'call_llava', return_value=\"Analyse image OK\")\n--- /Volumes/T7/athalia-dev-setup/tests/test_robotics_reachy_auditor_complete.py:before\t2025-07-29 19:56:18.300000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_robotics_reachy_auditor_complete.py:after\t2025-07-29 20:12:22.286469\n@@ -4,15 +4,15 @@\n Tests complets pour athalia_core.robotics.reachy_auditor\n \"\"\"\n \n+import shutil\n+import tempfile\n+from datetime import datetime\n+from pathlib import Path\n+from unittest.mock import Mock, mock_open, patch\n+\n import pytest\n-import tempfile\n-import shutil\n-from pathlib import Path\n-from unittest.mock import Mock, patch, mock_open\n-from datetime import datetime\n-from athalia_core.robotics.reachy_auditor import (\n-    ReachyAuditor, ReachyAuditResult\n-)\n+\n+from athalia_core.robotics.reachy_auditor import ReachyAuditor, ReachyAuditResult\n \n \n class TestReachyAuditorComplete:\n--- /Volumes/T7/athalia-dev-setup/tests/test_onboarding.py:before\t2025-07-29 19:56:18.410000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_onboarding.py:after\t2025-07-29 20:12:22.287491\n@@ -1,9 +1,12 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-from athalia_core.onboarding import generate_onboarding_md, generate_onboard_cli, generate_onboarding_html_advanced\n import os\n \n-\n+from athalia_core.onboarding import (\n+    generate_onboard_cli,\n+    generate_onboarding_html_advanced,\n+    generate_onboarding_md,\n+)\n \n \n def test_onboarding(tmp_path):\n--- /Volumes/T7/athalia-dev-setup/tests/test_user_profiles_advanced_complete.py:before\t2025-07-29 19:56:18.530000\n+++ /Volumes/T7/athalia-dev-setup/tests/test_user_profiles_advanced_complete.py:after\t2025-07-29 20:12:22.291002\n@@ -4,22 +4,23 @@\n Tests unitaires et d'int\u00e9gration pour ProfilUtilisateur et GestionnaireProfils\n \"\"\"\n \n+import json\n+import os\n+import sqlite3\n+import sys\n+import tempfile\n import unittest\n-import tempfile\n-import os\n-import sys\n-import sqlite3\n-import json\n-from unittest.mock import patch, MagicMock\n+from datetime import datetime, timedelta\n from pathlib import Path\n-from datetime import datetime, timedelta\n+from unittest.mock import MagicMock, patch\n \n # Ajout du chemin du projet pour les imports\n sys.path.insert(0, str(Path(__file__).parent.parent))\n \n try:\n     from athalia_core.advanced_modules.user_profiles_advanced import (\n-        ProfilUtilisateur, GestionnaireProfils\n+        GestionnaireProfils,\n+        ProfilUtilisateur,\n     )\n     USER_PROFILES_AVAILABLE = True\n except ImportError:\n--- /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_audit.py:before\t2025-07-29 19:56:09.290000\n+++ /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_audit.py:after\t2025-07-29 20:12:22.292840\n@@ -1,5 +1,6 @@\n+import os\n import subprocess\n-import os\n+\n import pytest\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_build.py:before\t2025-07-29 19:56:09.420000\n+++ /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_build.py:after\t2025-07-29 20:12:22.293807\n@@ -1,6 +1,6 @@\n-import subprocess\n import os\n import signal\n+import subprocess\n import time\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_coverage.py:before\t2025-07-29 19:56:09.530000\n+++ /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_coverage.py:after\t2025-07-29 20:12:22.294536\n@@ -1,7 +1,8 @@\n+import glob\n+import os\n import subprocess\n-import os\n-import glob\n import time\n+\n import pytest\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_test.py:before\t2025-07-29 19:56:09.640000\n+++ /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_test.py:after\t2025-07-29 20:12:22.295173\n@@ -1,5 +1,6 @@\n+import os\n import subprocess\n-import os\n+\n import pytest\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_lint.py:before\t2025-07-29 19:56:09.740000\n+++ /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_lint.py:after\t2025-07-29 20:12:22.295796\n@@ -1,5 +1,6 @@\n+import os\n import subprocess\n-import os\n+\n import pytest\n \n \n--- /Volumes/T7/athalia-dev-setup/tests/integration/test_cli_robustesse.py:before\t2025-07-29 20:02:53.970000\n+++ /Volumes/T7/athalia-dev-setup/tests/integration/test_cli_robustesse.py:after\t2025-07-29 20:12:22.301583\n@@ -5,14 +5,15 @@\n Tests professionnels pour la CI/CD.\n \"\"\"\n \n+import os\n+import subprocess\n import sys\n-import os\n+import tempfile\n import time\n-import subprocess\n+from pathlib import Path\n+from unittest.mock import MagicMock, patch\n+\n import pytest\n-import tempfile\n-from pathlib import Path\n-from unittest.mock import patch, MagicMock\n \n # Ajouter le r\u00e9pertoire parent au path\n sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n--- /Volumes/T7/athalia-dev-setup/tests/integration/test_end_to_end.py:before\t2025-07-29 20:02:53.970000\n+++ /Volumes/T7/athalia-dev-setup/tests/integration/test_end_to_end.py:after\t2025-07-29 20:12:22.304683\n@@ -5,14 +5,15 @@\n Tests professionnels pour la CI/CD.\n \"\"\"\n \n-import pytest\n import subprocess\n import sys\n-import yaml\n import tempfile\n import time\n from pathlib import Path\n-from unittest.mock import patch, MagicMock\n+from unittest.mock import MagicMock, patch\n+\n+import pytest\n+import yaml\n \n # Ajouter le r\u00e9pertoire parent au path\n sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n@@ -34,7 +35,10 @@\n     def test_generation_end_to_end_api(self):\n         \"\"\"Test de g\u00e9n\u00e9ration end-to-end pour un projet API.\"\"\"\n         try:\n-            from athalia_core.generation import generate_blueprint_mock, generate_project\n+            from athalia_core.generation import (\n+                generate_blueprint_mock,\n+                generate_project,\n+            )\n         except ImportError:\n             pytest.skip(\"Modules de g\u00e9n\u00e9ration non disponibles\")\n \n@@ -90,7 +94,10 @@\n     def test_generation_end_to_end_web(self):\n         \"\"\"Test de g\u00e9n\u00e9ration end-to-end pour un projet web.\"\"\"\n         try:\n-            from athalia_core.generation import generate_blueprint_mock, generate_project\n+            from athalia_core.generation import (\n+                generate_blueprint_mock,\n+                generate_project,\n+            )\n         except ImportError:\n             pytest.skip(\"Modules de g\u00e9n\u00e9ration non disponibles\")\n \n@@ -124,7 +131,10 @@\n     def test_generation_end_to_end_cli(self):\n         \"\"\"Test de g\u00e9n\u00e9ration end-to-end pour un projet CLI.\"\"\"\n         try:\n-            from athalia_core.generation import generate_blueprint_mock, generate_project\n+            from athalia_core.generation import (\n+                generate_blueprint_mock,\n+                generate_project,\n+            )\n         except ImportError:\n             pytest.skip(\"Modules de g\u00e9n\u00e9ration non disponibles\")\n \n@@ -211,7 +221,10 @@\n     def test_error_handling_end_to_end(self):\n         \"\"\"Test de gestion d'erreurs end-to-end.\"\"\"\n         try:\n-            from athalia_core.generation import generate_blueprint_mock, generate_project\n+            from athalia_core.generation import (\n+                generate_blueprint_mock,\n+                generate_project,\n+            )\n         except ImportError:\n             pytest.skip(\"Modules de g\u00e9n\u00e9ration non disponibles\")\n \n@@ -246,8 +259,8 @@\n         except ImportError:\n             pytest.skip(\"Modules de g\u00e9n\u00e9ration non disponibles\")\n \n+        import queue\n         import threading\n-        import queue\n \n         results = queue.Queue()\n         \n--- /Volumes/T7/athalia-dev-setup/tests/integration/test_yaml_validity.py:before\t2025-07-29 20:02:53.970000\n+++ /Volumes/T7/athalia-dev-setup/tests/integration/test_yaml_validity.py:after\t2025-07-29 20:12:22.307066\n@@ -5,12 +5,13 @@\n Tests professionnels pour la CI/CD.\n \"\"\"\n \n+import sys\n+import tempfile\n+from pathlib import Path\n+from unittest.mock import MagicMock, patch\n+\n import pytest\n import yaml\n-import tempfile\n-import sys\n-from pathlib import Path\n-from unittest.mock import patch, MagicMock\n \n # Ajouter le r\u00e9pertoire parent au path\n sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n@@ -290,7 +291,7 @@\n     def test_yaml_performance(self):\n         \"\"\"Test de performance YAML.\"\"\"\n         import time\n-        \n+\n         # Cr\u00e9er un YAML complexe\n         complex_data = {\n             'project': {\n--- /Volumes/T7/athalia-dev-setup/plugins/plugins_manager.py:before\t2025-07-29 20:02:53.950000\n+++ /Volumes/T7/athalia-dev-setup/plugins/plugins_manager.py:after\t2025-07-29 20:12:22.344463\n@@ -1,8 +1,8 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-import os\n import importlib.util\n import logging\n+import os\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/plugins/plugins_validator.py:before\t2025-07-29 20:02:53.950000\n+++ /Volumes/T7/athalia-dev-setup/plugins/plugins_validator.py:after\t2025-07-29 20:12:22.345433\n@@ -1,10 +1,10 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import importlib.util\n+import logging\n import os\n import sys\n-import importlib.util\n-import logging\n-from typing import Dict, Any\n+from typing import Any, Dict\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/tools/analysis/audit_complet_dossiers.py:before\t2025-07-29 19:56:19.430000\n+++ /Volumes/T7/athalia-dev-setup/tools/analysis/audit_complet_dossiers.py:after\t2025-07-29 20:12:22.353747\n@@ -6,10 +6,10 @@\n \"\"\"\n \n import ast\n+from dataclasses import dataclass, field\n+from datetime import datetime\n from pathlib import Path\n-from dataclasses import dataclass, field\n from typing import List, Optional\n-from datetime import datetime\n \n \n @dataclass\n--- /Volumes/T7/athalia-dev-setup/tools/analysis/verification_integration_simple.py:before\t2025-07-29 19:56:19.530000\n+++ /Volumes/T7/athalia-dev-setup/tools/analysis/verification_integration_simple.py:after\t2025-07-29 20:12:22.354726\n@@ -5,8 +5,9 @@\n Script simple pour v\u00e9rifier l'int\u00e9gration actuelle de l'orchestrateur unifi\u00e9.\n \"\"\"\n \n+import re\n from pathlib import Path\n-import re\n+\n \n def main():\n     \"\"\"Fonction principale\"\"\"\n--- /Volumes/T7/athalia-dev-setup/tools/monitoring/system_monitor.py:before\t2025-07-29 19:56:19.640000\n+++ /Volumes/T7/athalia-dev-setup/tools/monitoring/system_monitor.py:after\t2025-07-29 20:12:22.356471\n@@ -4,12 +4,13 @@\n Surveille les performances, l'espace disque, et l'\u00e9tat des processus\n \"\"\"\n \n-import os\n-import psutil\n import json\n import logging\n+import os\n from datetime import datetime\n from pathlib import Path\n+\n+import psutil\n \n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n--- /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_archives.py:before\t2025-07-29 19:56:19.740000\n+++ /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_archives.py:after\t2025-07-29 20:12:22.359128\n@@ -11,7 +11,7 @@\n import shutil\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, Any\n+from typing import Any, Dict\n \n # Configuration du logging\n logging.basicConfig(\n@@ -152,7 +152,7 @@\n     def _extract_date_from_filename(self, filename: str) -> str | None:\n         \"\"\"Extrait la date du nom de fichier\"\"\"\n         import re\n-        \n+\n         # Patterns de date courants\n         patterns = [\n             r'(\\d{8})',  # YYYYMMDD\n@@ -208,7 +208,7 @@\n def main():\n     \"\"\"Fonction principale\"\"\"\n     import sys\n-    \n+\n     # V\u00e9rifier les arguments\n     dry_run = \"--dry-run\" not in sys.argv\n     \n--- /Volumes/T7/athalia-dev-setup/tools/maintenance/validation_documentation.py:before\t2025-07-29 19:56:20.160000\n+++ /Volumes/T7/athalia-dev-setup/tools/maintenance/validation_documentation.py:after\t2025-07-29 20:12:22.364735\n@@ -5,14 +5,14 @@\n V\u00e9rifie la coh\u00e9rence entre la documentation et le code r\u00e9el\n \"\"\"\n \n+import argparse\n+import ast\n+import json\n+import logging\n+import re\n import sys\n-import re\n-import json\n-import argparse\n-import logging\n from pathlib import Path\n from typing import Dict, List\n-import ast\n \n # Configuration du logging\n logging.basicConfig(\n--- /Volumes/T7/athalia-dev-setup/athalia_core/__init__.py:before\t2025-07-29 20:02:53.590000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/__init__.py:after\t2025-07-29 20:12:22.489568\n@@ -5,36 +5,41 @@\n Version 2.0.0\n \"\"\"\n \n+# Analytics et performance\n+from .advanced_analytics import AdvancedAnalytics\n+\n+# IA et g\u00e9n\u00e9ration\n+from .ai_robust import RobustAI\n+\n # Imports principaux\n from .athalia_orchestrator import AthaliaOrchestrator\n-from .main import main\n+from .auto_cicd import AutoCICD\n+from .auto_cleaner import AutoCleaner\n+from .auto_documenter import AutoDocumenter\n+\n+# Modules automatiques\n+from .auto_tester import AutoTester\n from .cli import cli\n+from .code_linter import CodeLinter\n+\n+# Configuration et utilitaires\n+from .config_manager import ConfigManager\n+from .correction_optimizer import CorrectionOptimizer\n \n # Gestion d'erreurs\n from .error_codes import ErrorCode, ErrorSeverity\n-from .error_handling import AthaliaError, ErrorHandler, handle_error, raise_athalia_error\n-\n-# IA et g\u00e9n\u00e9ration\n-from .ai_robust import RobustAI\n-from .generation import generate_project, generate_blueprint_mock\n-\n-# Modules automatiques\n-from .auto_tester import AutoTester\n-from .auto_documenter import AutoDocumenter\n-from .auto_cleaner import AutoCleaner\n-from .auto_cicd import AutoCICD\n-\n-# Analytics et performance\n-from .advanced_analytics import AdvancedAnalytics\n+from .error_handling import (\n+    AthaliaError,\n+    ErrorHandler,\n+    handle_error,\n+    raise_athalia_error,\n+)\n+from .generation import generate_blueprint_mock, generate_project\n+from .main import main\n from .performance_analyzer import PerformanceAnalyzer\n \n # S\u00e9curit\u00e9 et qualit\u00e9\n from .security_auditor import SecurityAuditor\n-from .code_linter import CodeLinter\n-from .correction_optimizer import CorrectionOptimizer\n-\n-# Configuration et utilitaires\n-from .config_manager import ConfigManager\n \n # Version\n __version__ = \"2.0.0\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust.py:before\t2025-07-29 20:11:31.500000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust.py:after\t2025-07-29 20:12:22.492632\n@@ -9,6 +9,7 @@\n import subprocess\n from enum import Enum\n from typing import Dict, List, Optional\n+\n import requests\n \n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_analytics.py:before\t2025-07-29 20:02:53.590000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_analytics.py:after\t2025-07-29 20:12:22.495013\n@@ -1,11 +1,11 @@\n #!/usr/bin/env python3\n+import argparse\n+import ast\n+import json\n+import logging\n+from datetime import datetime\n from pathlib import Path\n-from typing import Dict, Any\n-import json\n-import argparse\n-from datetime import datetime\n-import ast\n-import logging\n+from typing import Any, Dict\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/analytics.py:before\t2025-07-29 20:02:53.590000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/analytics.py:after\t2025-07-29 20:12:22.496844\n@@ -6,9 +6,9 @@\n Fournit des analyses de base pour les tests.\n \"\"\"\n \n+from datetime import datetime\n from pathlib import Path\n-from typing import Dict, Any\n-from datetime import datetime\n+from typing import Any, Dict\n \n \n def analyze_project(project_path: str = \".\") -> Dict[str, Any]:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_tester.py:before\t2025-07-29 20:02:53.600000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_tester.py:after\t2025-07-29 20:12:22.500362\n@@ -1,12 +1,12 @@\n \n-import os\n-from pathlib import Path\n-from typing import Dict, List, Any\n-import re\n import argparse\n import ast\n+import logging\n+import os\n+import re\n import subprocess\n-import logging\n+from pathlib import Path\n+from typing import Any, Dict, List\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/architecture_analyzer.py:before\t2025-07-29 19:56:23.090000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/architecture_analyzer.py:after\t2025-07-29 20:12:22.502867\n@@ -9,10 +9,11 @@\n import json\n import logging\n import sqlite3\n+from dataclasses import dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any, Optional\n-from dataclasses import dataclass\n+from typing import Any, Dict, List, Optional\n+\n import yaml\n \n from .ast_analyzer import ASTAnalyzer, FileAnalysis\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ast_analyzer.py:before\t2025-07-29 19:56:23.200000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ast_analyzer.py:after\t2025-07-29 20:12:22.504343\n@@ -8,11 +8,11 @@\n \n import ast\n import logging\n+import re\n+from dataclasses import dataclass\n from datetime import datetime\n from pathlib import Path\n from typing import List, Optional\n-from dataclasses import dataclass\n-import re\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/generation.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/generation.py:after\t2025-07-29 20:12:22.507423\n@@ -5,9 +5,9 @@\n Version simplifi\u00e9e sans f-strings complexes\n \"\"\"\n \n+import json\n import os\n import re\n-import json\n from pathlib import Path\n from typing import Dict, Optional\n \n@@ -302,6 +302,7 @@\n def save_blueprint(blueprint: dict, outdir):\n     \"\"\"Sauvegarde un blueprint dans un fichier YAML.\"\"\"\n     from pathlib import Path\n+\n     import yaml\n \n     outdir = Path(outdir)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_cicd.py:before\t2025-07-29 20:02:53.600000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_cicd.py:after\t2025-07-29 20:12:22.509439\n@@ -1,9 +1,9 @@\n #!/usr/bin/env python3\n-from pathlib import Path\n-from typing import Dict, List, Any\n+import builtins\n import json\n import logging\n-import builtins\n+from pathlib import Path\n+from typing import Any, Dict, List\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_cleaner.py:before\t2025-07-29 20:02:53.600000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_cleaner.py:after\t2025-07-29 20:12:22.512348\n@@ -1,13 +1,13 @@\n #!/usr/bin/env python3\n+import argparse\n+import hashlib\n+import json\n+import logging\n+import os\n+import shutil\n+from datetime import datetime, timedelta\n from pathlib import Path\n-from typing import Dict, List, Any\n-import json\n-import os\n-import hashlib\n-import argparse\n-from datetime import datetime, timedelta\n-import shutil\n-import logging\n+from typing import Any, Dict, List\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py:before\t2025-07-29 20:09:10.960000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py:after\t2025-07-29 20:12:22.514683\n@@ -4,23 +4,24 @@\n Coordination centralis\u00e9e de tous les modules\n \"\"\"\n \n+import json\n+import logging\n+from datetime import datetime\n from pathlib import Path\n-from typing import Dict, Any, List, Optional\n-import logging\n-import json\n+from typing import Any, Dict, List, Optional\n+\n import yaml\n-from datetime import datetime\n \n # Imports des modules Athalia\n from .ai_robust import RobustAI\n+from .auto_cicd import AutoCICD\n+from .auto_cleaner import AutoCleaner\n+from .auto_documenter import AutoDocumenter\n+from .auto_tester import AutoTester\n+from .code_linter import CodeLinter\n+from .correction_optimizer import CorrectionOptimizer\n from .generation import generate_project\n from .security_auditor import SecurityAuditor\n-from .code_linter import CodeLinter\n-from .correction_optimizer import CorrectionOptimizer\n-from .auto_tester import AutoTester\n-from .auto_documenter import AutoDocumenter\n-from .auto_cleaner import AutoCleaner\n-from .auto_cicd import AutoCICD\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics_ci.py:before\t2025-07-29 20:08:50.400000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics_ci.py:after\t2025-07-29 20:12:22.517102\n@@ -4,13 +4,14 @@\n Int\u00e9gration continue pour ROS2, Rust et projets robotics\n \"\"\"\n \n+import json\n+import logging\n+import subprocess\n+import xml.etree.ElementTree as ET\n from pathlib import Path\n-from typing import Dict, Any, List, Optional\n-import subprocess\n-import json\n+from typing import Any, Dict, List, Optional\n+\n import yaml\n-import logging\n-import xml.etree.ElementTree as ET\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/auto_documenter.py:before\t2025-07-29 20:02:53.600000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/auto_documenter.py:after\t2025-07-29 20:12:22.521220\n@@ -1,14 +1,14 @@\n #!/usr/bin/env python3\n-from pathlib import Path\n-from typing import Dict, List, Any, Optional\n+import argparse\n+import ast\n+import importlib\n import json\n+import logging\n import os\n import re\n-import argparse\n from datetime import datetime\n-import ast\n-import importlib\n-import logging\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/plugins_validator.py:before\t2025-07-29 20:08:50.400000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/plugins_validator.py:after\t2025-07-29 20:12:22.524602\n@@ -4,13 +4,14 @@\n Validation et v\u00e9rification des plugins tiers\n \"\"\"\n \n+import ast\n+import importlib.util\n+import json\n+import logging\n from pathlib import Path\n-from typing import Dict, Any, List, Optional\n-import json\n+from typing import Any, Dict, List, Optional\n+\n import yaml\n-import logging\n-import importlib.util\n-import ast\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_engine.py:before\t2025-07-29 19:56:24.360000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_engine.py:after\t2025-07-29 20:12:22.525637\n@@ -1,5 +1,6 @@\n from abc import ABC, abstractmethod\n from typing import List\n+\n import requests\n \n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_server.py:before\t2025-07-29 19:56:24.470000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_server.py:after\t2025-07-29 20:12:22.527004\n@@ -1,8 +1,13 @@\n+import os\n+from typing import List\n+\n from fastapi import FastAPI, HTTPException\n from pydantic import BaseModel\n-from typing import List\n-import os\n-from athalia_core.autocomplete_engine import SimpleAutocompleteEngine, OllamaAutocompleteEngine\n+\n+from athalia_core.autocomplete_engine import (\n+    OllamaAutocompleteEngine,\n+    SimpleAutocompleteEngine,\n+)\n \n app = FastAPI(title=\"Athalia Autocomplete Server\")\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/cli.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/cli.py:after\t2025-07-29 20:12:22.528454\n@@ -3,16 +3,17 @@\n Interface CLI pour Athalia avec IA robuste.\n \"\"\"\n \n+import logging\n import os\n+import traceback\n from pathlib import Path\n-import traceback\n+\n+import click\n import yaml\n-import click\n-import logging\n \n-from .ai_robust import RobustAI, AIModel\n+from .ai_robust import AIModel, RobustAI\n+from .audit import audit_project_intelligent\n from .generation import generate_project\n-from .audit import audit_project_intelligent\n \n # TODO: Pr\u00e9parer linternationalisation (i18n) des messages CLI et prompts\n # utilisateur.\n--- /Volumes/T7/athalia-dev-setup/athalia_core/code_linter.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/code_linter.py:after\t2025-07-29 20:12:22.529887\n@@ -1,8 +1,8 @@\n #!/usr/bin/env python3\n+import logging\n+import subprocess\n from pathlib import Path\n-from typing import Dict, Any\n-import subprocess\n-import logging\n+from typing import Any, Dict\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.fonctionnels:before\t2025-07-27 18:51:36.760000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.fonctionnels:after\t2025-07-29 20:12:22.535986\n@@ -10,63 +10,65 @@\n - Optimisation automatique du code\n \"\"\"\n \n+import argparse\n+import json\n import logging\n-import json\n+import os\n import sqlite3\n-import os\n import sys\n+from dataclasses import asdict, dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any, Optional, Set, Tuple\n-from dataclasses import dataclass, asdict\n-import argparse\n+from typing import Any, Dict, List, Optional, Set, Tuple\n \n # Imports des modules Athalia\n from .advanced_analytics import AdvancedAnalytics\n+\n+# Imports robotiques (optionnels)\n+from .audit import Audit\n from .auto_cicd import AutoCICD\n from .auto_cleaner import AutoCleaner\n from .auto_documenter import AutoDocumenter\n from .auto_tester import AutoTester\n from .code_linter import CodeLinter\n-from .intelligent_auditor import IntelligentAuditor\n-from .project_importer import ProjectImporter\n-from .security_auditor import SecurityAuditor\n-from .intelligent_analyzer import IntelligentAnalyzer\n-\n-# Imports robotiques (optionnels)\n-from .audit import Audit\n from .config_manager import ConfigManager\n from .correction_optimizer import CorrectionOptimizer\n+from .intelligent_analyzer import IntelligentAnalyzer\n+from .intelligent_auditor import IntelligentAuditor\n+\n # from .generation import FlowerAnimation  # Classe non disponible\n from .intelligent_memory import IntelligentMemory\n from .logger_advanced import AthaliaLogger\n from .pattern_detector import PatternDetector\n from .performance_analyzer import PerformanceAnalyzer\n+from .project_importer import ProjectImporter\n+from .security_auditor import SecurityAuditor\n+\n try:\n+    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.reachy_auditor import ReachyAuditor\n+    from .robotics.robotics_ci import RoboticsCI\n     from .robotics.ros2_validator import ROS2Validator\n-    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.rust_analyzer import RustAnalyzer\n-    from .robotics.robotics_ci import RoboticsCI\n     ROBOTICS_AVAILABLE = True\n except ImportError:\n     ROBOTICS_AVAILABLE = False\n \n # Imports de distillation (optionnels)\n try:\n+    from .distillation.adaptive_distillation import AdaptiveDistiller\n+    from .distillation.audit_distiller import AuditDistiller\n+    from .distillation.code_genetics import CodeGenetics\n+    from .distillation.correction_distiller import CorrectionDistiller\n+    from .distillation.predictive_cache import PredictiveCache\n     from .distillation.response_distiller import ResponseDistiller\n-    from .distillation.audit_distiller import AuditDistiller\n-    from .distillation.correction_distiller import CorrectionDistiller\n-    from .distillation.adaptive_distillation import AdaptiveDistiller\n-    from .distillation.code_genetics import CodeGenetics\n-    from .distillation.predictive_cache import PredictiveCache\n     DISTILLATION_AVAILABLE = True\n except ImportError:\n     DISTILLATION_AVAILABLE = False\n \n # Imports IA robuste (optionnels)\n try:\n-    from .ai_robust import RobustAI, AIModel, PromptContext\n+    from .ai_robust import AIModel, PromptContext, RobustAI\n     AI_ROBUST_AVAILABLE = True\n except ImportError:\n     AI_ROBUST_AVAILABLE = False\n--- /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.phase2:before\t2025-07-27 18:51:36.760000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.phase2:after\t2025-07-29 20:12:22.542797\n@@ -10,63 +10,65 @@\n - Optimisation automatique du code\n \"\"\"\n \n+import argparse\n+import json\n import logging\n-import json\n+import os\n import sqlite3\n-import os\n import sys\n+from dataclasses import asdict, dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any, Optional, Set, Tuple\n-from dataclasses import dataclass, asdict\n-import argparse\n+from typing import Any, Dict, List, Optional, Set, Tuple\n \n # Imports des modules Athalia\n from .advanced_analytics import AdvancedAnalytics\n+\n+# Imports robotiques (optionnels)\n+from .audit import Audit\n from .auto_cicd import AutoCICD\n from .auto_cleaner import AutoCleaner\n from .auto_documenter import AutoDocumenter\n from .auto_tester import AutoTester\n from .code_linter import CodeLinter\n-from .intelligent_auditor import IntelligentAuditor\n-from .project_importer import ProjectImporter\n-from .security_auditor import SecurityAuditor\n-from .intelligent_analyzer import IntelligentAnalyzer\n-\n-# Imports robotiques (optionnels)\n-from .audit import Audit\n from .config_manager import ConfigManager\n from .correction_optimizer import CorrectionOptimizer\n+from .intelligent_analyzer import IntelligentAnalyzer\n+from .intelligent_auditor import IntelligentAuditor\n+\n # from .generation import FlowerAnimation  # Classe non disponible\n from .intelligent_memory import IntelligentMemory\n from .logger_advanced import AthaliaLogger\n from .pattern_detector import PatternDetector\n from .performance_analyzer import PerformanceAnalyzer\n+from .project_importer import ProjectImporter\n+from .security_auditor import SecurityAuditor\n+\n try:\n+    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.reachy_auditor import ReachyAuditor\n+    from .robotics.robotics_ci import RoboticsCI\n     from .robotics.ros2_validator import ROS2Validator\n-    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.rust_analyzer import RustAnalyzer\n-    from .robotics.robotics_ci import RoboticsCI\n     ROBOTICS_AVAILABLE = True\n except ImportError:\n     ROBOTICS_AVAILABLE = False\n \n # Imports de distillation (optionnels)\n try:\n+    from .distillation.adaptive_distillation import AdaptiveDistiller\n+    from .distillation.audit_distiller import AuditDistiller\n+    from .distillation.code_genetics import CodeGenetics\n+    from .distillation.correction_distiller import CorrectionDistiller\n+    from .distillation.predictive_cache import PredictiveCache\n     from .distillation.response_distiller import ResponseDistiller\n-    from .distillation.audit_distiller import AuditDistiller\n-    from .distillation.correction_distiller import CorrectionDistiller\n-    from .distillation.adaptive_distillation import AdaptiveDistiller\n-    from .distillation.code_genetics import CodeGenetics\n-    from .distillation.predictive_cache import PredictiveCache\n     DISTILLATION_AVAILABLE = True\n except ImportError:\n     DISTILLATION_AVAILABLE = False\n \n # Imports IA robuste (optionnels)\n try:\n-    from .ai_robust import RobustAI, AIModel, PromptContext\n+    from .ai_robust import AIModel, PromptContext, RobustAI\n     AI_ROBUST_AVAILABLE = True\n except ImportError:\n     AI_ROBUST_AVAILABLE = False\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ci.py:before\t2025-07-29 19:56:25.420000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ci.py:after\t2025-07-29 20:12:22.543908\n@@ -1,8 +1,7 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import logging\n import os\n-\n-import logging\n \n \"\"\"\n Module CI / CD, g\u00e9n\u00e9ration de workflows, badges, Taskfile.\n--- /Volumes/T7/athalia-dev-setup/athalia_core/cleanup.py:before\t2025-07-29 19:56:25.630000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/cleanup.py:after\t2025-07-29 20:12:22.544971\n@@ -1,9 +1,8 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-import os\n-\n import fnmatch\n import logging\n+import os\n import shutil\n from pathlib import Path\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_auditor.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_auditor.py:after\t2025-07-29 20:12:22.549278\n@@ -1,14 +1,14 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-from pathlib import Path\n-from typing import Dict, List, Any\n+import ast\n import json\n+import logging\n import os\n import re\n import sys\n from datetime import datetime\n-import ast\n-import logging\n+from pathlib import Path\n+from typing import Any, Dict, List\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/dashboard.py:before\t2025-07-29 19:56:26.230000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/dashboard.py:after\t2025-07-29 20:12:22.551725\n@@ -1,6 +1,7 @@\n+import os\n+\n+import pandas as pd\n import streamlit as st\n-import pandas as pd\n-import os\n \n \n def show_benchmarks():\n--- /Volumes/T7/athalia-dev-setup/athalia_core/main.py:before\t2025-07-29 20:02:53.620000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/main.py:after\t2025-07-29 20:12:22.554416\n@@ -1,16 +1,21 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-from athalia_core.ci import generate_github_ci_yaml, add_coverage_badge\n-from athalia_core.cleanup import clean_old_tests_and_caches\n-# from athalia_core.dashboard import generate_dashboard_html, generate_multi_project_mermaid\n-from athalia_core.onboarding import generate_onboard_cli, generate_onboarding_html_advanced\n-from athalia_core.security import security_audit_project\n+import logging\n import os\n-from datetime import datetime\n-import logging\n import shutil\n import signal\n import time\n+from datetime import datetime\n+\n+from athalia_core.ci import add_coverage_badge, generate_github_ci_yaml\n+from athalia_core.cleanup import clean_old_tests_and_caches\n+\n+# from athalia_core.dashboard import generate_dashboard_html, generate_multi_project_mermaid\n+from athalia_core.onboarding import (\n+    generate_onboard_cli,\n+    generate_onboarding_html_advanced,\n+)\n+from athalia_core.security import security_audit_project\n \n # Import du syst\u00e8me de logging avanc\u00e9\n try:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/config_manager.py:before\t2025-07-29 19:56:26.440000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/config_manager.py:after\t2025-07-29 20:12:22.557202\n@@ -1,11 +1,11 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import logging\n+import os\n+from dataclasses import dataclass\n from pathlib import Path\n-from typing import Dict, Any, Optional, List\n-import os\n-\n-from dataclasses import dataclass\n-import logging\n+from typing import Any, Dict, List, Optional\n+\n import yaml\n \n \"\"\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/ros2_validator.py:before\t2025-07-29 20:08:50.400000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ros2_validator.py:after\t2025-07-29 20:12:22.560187\n@@ -4,14 +4,15 @@\n Validation et v\u00e9rification des packages ROS2\n \"\"\"\n \n-from pathlib import Path\n-from typing import Dict, Any, List, Optional\n-import xml.etree.ElementTree as ET\n-import subprocess\n import json\n-import yaml\n import logging\n import re\n+import subprocess\n+import xml.etree.ElementTree as ET\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional\n+\n+import yaml\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.phase3:before\t2025-07-27 18:51:36.770000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.phase3:after\t2025-07-29 20:12:22.565808\n@@ -10,72 +10,83 @@\n - Optimisation automatique du code\n \"\"\"\n \n+import argparse\n+import json\n import logging\n-import json\n+import os\n import sqlite3\n-import os\n import sys\n+from dataclasses import asdict, dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any, Optional, Set, Tuple\n-from dataclasses import dataclass, asdict\n-import argparse\n+from typing import Any, Dict, List, Optional, Set, Tuple\n \n # Imports des modules Athalia\n from .advanced_analytics import AdvancedAnalytics\n+\n+# Imports robotiques (optionnels)\n+from .analytics import (\n+    analyze_project,\n+    generate_analytics_html,\n+    generate_heatmap_data,\n+    generate_technical_debt_analysis,\n+)\n+from .audit import Audit\n from .auto_cicd import AutoCICD\n from .auto_cleaner import AutoCleaner\n from .auto_documenter import AutoDocumenter\n from .auto_tester import AutoTester\n+from .cleanup import clean_macos_files, clean_old_tests_and_caches\n+from .cli import cli, generate\n from .code_linter import CodeLinter\n-from .intelligent_auditor import IntelligentAuditor\n-from .project_importer import ProjectImporter\n-from .security_auditor import SecurityAuditor\n-from .intelligent_analyzer import IntelligentAnalyzer\n-\n-# Imports robotiques (optionnels)\n-from .analytics import analyze_project, generate_heatmap_data, generate_technical_debt_analysis, generate_analytics_html\n-from .cleanup import clean_old_tests_and_caches, clean_macos_files\n-from .cli import cli, generate\n-from .main import main\n-from .security import security_audit_project\n-from .onboarding import generate_onboarding_md, generate_onboard_cli, generate_onboarding_html_advanced\n-from .plugins_manager import run_all_plugins\n-from .ready_check import open_patch, check_ready\n-from .dashboard import main\n-from .audit import Audit\n from .config_manager import ConfigManager\n from .correction_optimizer import CorrectionOptimizer\n+from .dashboard import main\n+from .intelligent_analyzer import IntelligentAnalyzer\n+from .intelligent_auditor import IntelligentAuditor\n+\n # from .generation import FlowerAnimation  # Classe non disponible\n from .intelligent_memory import IntelligentMemory\n from .logger_advanced import AthaliaLogger\n+from .main import main\n+from .onboarding import (\n+    generate_onboard_cli,\n+    generate_onboarding_html_advanced,\n+    generate_onboarding_md,\n+)\n from .pattern_detector import PatternDetector\n from .performance_analyzer import PerformanceAnalyzer\n+from .plugins_manager import run_all_plugins\n+from .project_importer import ProjectImporter\n+from .ready_check import check_ready, open_patch\n+from .security import security_audit_project\n+from .security_auditor import SecurityAuditor\n+\n try:\n+    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.reachy_auditor import ReachyAuditor\n+    from .robotics.robotics_ci import RoboticsCI\n     from .robotics.ros2_validator import ROS2Validator\n-    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.rust_analyzer import RustAnalyzer\n-    from .robotics.robotics_ci import RoboticsCI\n     ROBOTICS_AVAILABLE = True\n except ImportError:\n     ROBOTICS_AVAILABLE = False\n \n # Imports de distillation (optionnels)\n try:\n+    from .distillation.adaptive_distillation import AdaptiveDistiller\n+    from .distillation.audit_distiller import AuditDistiller\n+    from .distillation.code_genetics import CodeGenetics\n+    from .distillation.correction_distiller import CorrectionDistiller\n+    from .distillation.predictive_cache import PredictiveCache\n     from .distillation.response_distiller import ResponseDistiller\n-    from .distillation.audit_distiller import AuditDistiller\n-    from .distillation.correction_distiller import CorrectionDistiller\n-    from .distillation.adaptive_distillation import AdaptiveDistiller\n-    from .distillation.code_genetics import CodeGenetics\n-    from .distillation.predictive_cache import PredictiveCache\n     DISTILLATION_AVAILABLE = True\n except ImportError:\n     DISTILLATION_AVAILABLE = False\n \n # Imports IA robuste (optionnels)\n try:\n-    from .ai_robust import RobustAI, AIModel, PromptContext\n+    from .ai_robust import AIModel, PromptContext, RobustAI\n     AI_ROBUST_AVAILABLE = True\n except ImportError:\n     AI_ROBUST_AVAILABLE = False\n--- /Volumes/T7/athalia-dev-setup/athalia_core/correction_optimizer.py:before\t2025-07-29 19:56:27.390000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/correction_optimizer.py:after\t2025-07-29 20:12:22.569774\n@@ -8,9 +8,9 @@\n import ast\n import re\n import time\n-from typing import Dict, List, Any, Tuple, Optional\n+from collections import defaultdict\n from dataclasses import dataclass\n-from collections import defaultdict\n+from typing import Any, Dict, List, Optional, Tuple\n \n from .logger_advanced import log_correction, log_error\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_analyzer.py:before\t2025-07-29 19:56:27.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_analyzer.py:after\t2025-07-29 20:12:22.573407\n@@ -9,16 +9,16 @@\n - Performance Analyzer (analyse de performance)\n \"\"\"\n \n+import json\n import logging\n+from dataclasses import asdict, dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any, Optional\n-from dataclasses import dataclass, asdict\n-import json\n-\n+from typing import Any, Dict, List, Optional\n+\n+from .architecture_analyzer import ArchitectureAnalyzer\n from .ast_analyzer import ASTAnalyzer\n from .pattern_detector import PatternDetector\n-from .architecture_analyzer import ArchitectureAnalyzer\n from .performance_analyzer import PerformanceAnalyzer\n \n # Import de l'orchestrateur unifi\u00e9 (optionnel)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/project_importer.py:before\t2025-07-29 20:02:53.620000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/project_importer.py:after\t2025-07-29 20:12:22.575884\n@@ -1,13 +1,12 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-import sys\n-from typing import Dict, List, Any\n-import os\n-\n-import pprint\n-from datetime import datetime\n import ast\n import logging\n+import os\n+import pprint\n+import sys\n+from datetime import datetime\n+from typing import Any, Dict, List\n \n \"\"\"\n Module dimport intelligent pour projets existants.\n--- /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_memory.py:before\t2025-07-29 19:56:27.830000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_memory.py:after\t2025-07-29 20:12:22.578960\n@@ -10,16 +10,16 @@\n - Am\u00e9liore la qualit\u00e9 du code continuellement\n \"\"\"\n \n+import difflib\n+import hashlib\n import json\n import logging\n+import re\n import sqlite3\n-import hashlib\n-import re\n+from dataclasses import dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any, Optional\n-from dataclasses import dataclass\n-import difflib\n+from typing import Any, Dict, List, Optional\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/ready_check.py:before\t2025-07-29 19:56:27.940000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ready_check.py:after\t2025-07-29 20:12:22.579706\n@@ -1,8 +1,9 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import builtins\n+import logging\n import os\n-import logging\n-import builtins\n+\n _real_open = builtins.open\n \n logger = logging.getLogger(__name__)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/security_auditor.py:before\t2025-07-29 20:02:53.620000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/security_auditor.py:after\t2025-07-29 20:12:22.581768\n@@ -1,10 +1,10 @@\n #!/usr/bin/env python3\n+import json\n+import logging\n+import re\n+import subprocess\n from pathlib import Path\n-from typing import Dict, Any\n-import subprocess\n-import json\n-import re\n-import logging\n+from typing import Any, Dict\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/logger_advanced.py:before\t2025-07-29 19:56:29.330000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/logger_advanced.py:after\t2025-07-29 20:12:22.584644\n@@ -5,17 +5,17 @@\n Logging intelligent avec rotation, compression et analyse automatique\n \"\"\"\n \n+import gzip\n+import json\n import logging\n import logging.handlers\n-import json\n+import shutil\n+import threading\n import time\n+from collections import defaultdict, deque\n from datetime import datetime, timedelta\n from pathlib import Path\n-from typing import Dict, Any, Optional\n-import gzip\n-import shutil\n-import threading\n-from collections import defaultdict, deque\n+from typing import Any, Dict, Optional\n \n \n class AthaliaLogger:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/multi_file_editor.py:before\t2025-07-29 19:56:29.540000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/multi_file_editor.py:after\t2025-07-29 20:12:22.585581\n@@ -5,10 +5,10 @@\n Permet d'appliquer des corrections/refactoring sur plusieurs fichiers en une seule commande,\n avec logs et rollback.\n \"\"\"\n+import logging\n import os\n import shutil\n-from typing import List, Callable, Dict, Any, Tuple\n-import logging\n+from typing import Any, Callable, Dict, List, Tuple\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/pattern_detector.py:before\t2025-07-29 19:56:29.750000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/pattern_detector.py:after\t2025-07-29 20:12:22.587747\n@@ -6,14 +6,14 @@\n doublons et anti-patterns. Utilise l'analyseur AST de base.\n \"\"\"\n \n+import difflib\n import json\n import logging\n import sqlite3\n+from dataclasses import dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any\n-from dataclasses import dataclass\n-import difflib\n+from typing import Any, Dict, List\n \n from .ast_analyzer import ASTAnalyzer, FileAnalysis\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/performance_analyzer.py:before\t2025-07-29 19:56:29.960000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/performance_analyzer.py:after\t2025-07-29 20:12:22.591267\n@@ -6,16 +6,16 @@\n d\u00e9tection des goulots d'\u00e9tranglement et optimisation.\n \"\"\"\n \n+import cProfile\n+import io\n import logging\n+import pstats\n import sqlite3\n+import time\n+from dataclasses import dataclass\n from datetime import datetime\n from pathlib import Path\n-from typing import Dict, List, Any\n-from dataclasses import dataclass\n-import time\n-import cProfile\n-import pstats\n-import io\n+from typing import Any, Dict, List\n \n from .ast_analyzer import ASTAnalyzer, FileAnalysis\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust_broken.py:before\t2025-07-29 20:02:53.590000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust_broken.py:after\t2025-07-29 20:12:22.594757\n@@ -9,6 +9,7 @@\n import subprocess\n from enum import Enum\n from typing import Dict, List, Optional\n+\n import requests\n \n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/athalia_orchestrator.py:before\t2025-07-29 20:02:53.600000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/athalia_orchestrator.py:after\t2025-07-29 20:12:22.600080\n@@ -4,25 +4,26 @@\n Orchestrateur unifi\u00e9 pour Athalia - Industrialisation IA compl\u00e8te\n \"\"\"\n \n-from .intelligent_analyzer import IntelligentAnalyzer\n-from .security_auditor import SecurityAuditor\n-from .project_importer import ProjectImporter\n-from .intelligent_auditor import IntelligentAuditor\n-from .code_linter import CodeLinter\n-from .auto_tester import AutoTester\n-from .auto_documenter import AutoDocumenter\n-from .auto_cleaner import AutoCleaner\n-from .auto_cicd import AutoCICD\n-from .advanced_analytics import AdvancedAnalytics\n import argparse\n import json\n import logging\n import sys\n import time\n+from dataclasses import dataclass\n from datetime import datetime\n-from dataclasses import dataclass\n from pathlib import Path\n-from typing import Dict, List, Any, Optional\n+from typing import Any, Dict, List, Optional\n+\n+from .advanced_analytics import AdvancedAnalytics\n+from .auto_cicd import AutoCICD\n+from .auto_cleaner import AutoCleaner\n+from .auto_documenter import AutoDocumenter\n+from .auto_tester import AutoTester\n+from .code_linter import CodeLinter\n+from .intelligent_analyzer import IntelligentAnalyzer\n+from .intelligent_auditor import IntelligentAuditor\n+from .project_importer import ProjectImporter\n+from .security_auditor import SecurityAuditor\n \n # Configuration du logging\n logging.basicConfig(level=logging.INFO)\n@@ -35,11 +36,11 @@\n \n # Imports robotiques (optionnels)\n try:\n+    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.reachy_auditor import ReachyAuditor\n+    from .robotics.robotics_ci import RoboticsCI\n     from .robotics.ros2_validator import ROS2Validator\n-    from .robotics.docker_robotics import DockerRoboticsManager\n     from .robotics.rust_analyzer import RustAnalyzer\n-    from .robotics.robotics_ci import RoboticsCI\n \n     ROBOTICS_AVAILABLE = True\n except ImportError:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/cache_manager.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/cache_manager.py:after\t2025-07-29 20:12:22.602790\n@@ -7,12 +7,12 @@\n \n import hashlib\n import json\n+import logging\n import os\n import time\n from datetime import datetime, timedelta\n from functools import lru_cache, wraps\n-from typing import Any, Dict, Optional, Callable\n-import logging\n+from typing import Any, Callable, Dict, Optional\n \n \n class AnalysisCache:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/error_codes.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/error_codes.py:after\t2025-07-29 20:12:22.604342\n@@ -6,7 +6,7 @@\n \"\"\"\n \n from enum import Enum, auto\n-from typing import Dict, Any\n+from typing import Any, Dict\n \n \n class ErrorCode(Enum):\n--- /Volumes/T7/athalia-dev-setup/athalia_core/error_handling.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/error_handling.py:after\t2025-07-29 20:12:22.606318\n@@ -6,13 +6,18 @@\n \"\"\"\n \n import logging\n+import sys\n import traceback\n-import sys\n-from typing import Optional, Dict, Any, Callable\n+from datetime import datetime\n from pathlib import Path\n-from datetime import datetime\n-\n-from .error_codes import ErrorCode, ErrorSeverity, format_error_message, get_error_severity\n+from typing import Any, Callable, Dict, Optional\n+\n+from .error_codes import (\n+    ErrorCode,\n+    ErrorSeverity,\n+    format_error_message,\n+    get_error_severity,\n+)\n \n \n class AthaliaError(Exception):\n--- /Volumes/T7/athalia-dev-setup/athalia_core/generation_simple.py:before\t2025-07-29 20:02:53.610000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/generation_simple.py:after\t2025-07-29 20:12:22.609316\n@@ -237,6 +237,7 @@\n def save_blueprint(blueprint: dict, outdir):\n     \"\"\"Sauvegarde un blueprint dans un fichier YAML.\"\"\"\n     from pathlib import Path\n+\n     import yaml\n \n     outdir = Path(outdir)\n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/auto_correction_advanced.py:before\t2025-07-29 19:56:21.080000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/auto_correction_advanced.py:after\t2025-07-29 20:12:22.637135\n@@ -4,11 +4,11 @@\n Correction intelligente de code, suggestions d'am\u00e9lioration, refactoring automatique\n \"\"\"\n \n-from pathlib import Path\n-from typing import Dict, List, Tuple, Any\n-import re\n import ast\n import logging\n+import re\n+from pathlib import Path\n+from typing import Any, Dict, List, Tuple\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/dashboard_unified.py:before\t2025-07-29 19:56:21.180000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/dashboard_unified.py:after\t2025-07-29 20:12:22.639511\n@@ -1,15 +1,14 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import json\n+import logging\n+import os\n+import sqlite3\n import sys\n+import webbrowser\n+from datetime import datetime, timedelta\n from pathlib import Path\n-from typing import Dict, Any, Optional\n-import json\n-import os\n-\n-from datetime import datetime, timedelta\n-import logging\n-import sqlite3\n-import webbrowser\n+from typing import Any, Dict, Optional\n \n # !/usr/bin/env python3\n \"\"\"\n--- /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/user_profiles_advanced.py:before\t2025-07-29 19:56:21.380000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/user_profiles_advanced.py:after\t2025-07-29 20:12:22.642084\n@@ -4,11 +4,11 @@\n Gestion des pr\u00e9f\u00e9rences, historique, statistiques et personnalisation\n \"\"\"\n \n-import sqlite3\n import json\n import logging\n+import sqlite3\n from datetime import datetime\n-from typing import Dict, List, Any, Optional\n+from typing import Any, Dict, List, Optional\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/agents/context_prompt.py:before\t2025-07-29 19:56:22.310000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/agents/context_prompt.py:after\t2025-07-29 20:12:22.650397\n@@ -1,12 +1,13 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import logging\n import os\n import re\n+import subprocess\n import sys\n import tempfile\n from datetime import datetime\n-import logging\n-import subprocess\n+\n import yaml\n \n try:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_types.py:before\t2025-07-29 19:56:26.850000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_types.py:after\t2025-07-29 20:12:22.656909\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n-from typing import Dict, Any\n from enum import Enum\n+from typing import Any, Dict\n \n \"\"\"\n Types de projets et leurs configurations sp\u00e9cialis\u00e9es.\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/adaptive_distillation.py:before\t2025-07-29 19:56:28.480000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/adaptive_distillation.py:after\t2025-07-29 20:12:22.661080\n@@ -4,9 +4,9 @@\n - Pond\u00e9ration dynamique selon pr\u00e9f\u00e9rences et feedback utilisateur\n - Historique sauvegard\u00e9/charg\u00e9 en JSON\n \"\"\"\n-from typing import List, Dict, Any, Optional\n import json\n from pathlib import Path\n+from typing import Any, Dict, List, Optional\n \n \n class AdaptiveDistiller:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/audit_distiller.py:before\t2025-07-29 19:56:28.580000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/audit_distiller.py:after\t2025-07-29 20:12:22.661616\n@@ -3,7 +3,7 @@\n Module de distillation d'audits pour Athalia/Arkalia\n Fusionne et pond\u00e8re plusieurs audits (s\u00e9curit\u00e9, qualit\u00e9, performance...)\n \"\"\"\n-from typing import List, Dict, Any, Optional\n+from typing import Any, Dict, List, Optional\n \n \n class AuditDistiller:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/code_genetics.py:before\t2025-07-29 19:56:28.690000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/code_genetics.py:after\t2025-07-29 20:12:22.662197\n@@ -3,8 +3,8 @@\n Code Genetics pour Athalia/Arkalia\n - Croisement, mutation, s\u00e9lection, \u00e9volution de solutions IA\n \"\"\"\n-from typing import List, Callable\n import random\n+from typing import Callable, List\n \n \n class CodeGenetics:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/correction_distiller.py:before\t2025-07-29 19:56:28.790000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/correction_distiller.py:after\t2025-07-29 20:12:22.662707\n@@ -3,7 +3,7 @@\n Module de distillation de corrections IA pour Athalia/Arkalia\n Fusionne, score et s\u00e9lectionne la meilleure correction parmi plusieurs suggestions IA.\n \"\"\"\n-from typing import List, Dict, Any, Optional\n+from typing import Any, Dict, List, Optional\n \n \n class CorrectionDistiller:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/multimodal_distiller.py:before\t2025-07-29 19:56:28.900000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/multimodal_distiller.py:after\t2025-07-29 20:12:22.663482\n@@ -4,8 +4,9 @@\n - Fusionne r\u00e9ponses texte et image (LLaVA)\n - Appel r\u00e9el \u00e0 LLaVA via RobustAI (Ollama)\n \"\"\"\n-from typing import List, Dict, Any, Optional\n-from athalia_core.ai_robust import RobustAI, AIModel\n+from typing import Any, Dict, List, Optional\n+\n+from athalia_core.ai_robust import AIModel, RobustAI\n \n \n class MultimodalDistiller:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/predictive_cache.py:before\t2025-07-29 19:56:29.010000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/predictive_cache.py:after\t2025-07-29 20:12:22.664160\n@@ -3,8 +3,8 @@\n Caching pr\u00e9dictif pour Athalia/Arkalia\n - Anticipation contextuelle, pr\u00e9-g\u00e9n\u00e9ration, invalidation intelligente, stats\n \"\"\"\n-from typing import Dict, Any, Callable, Optional\n import time\n+from typing import Any, Callable, Dict, Optional\n \n \n class PredictiveCache:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/distillation/response_distiller.py:before\t2025-07-29 19:56:29.220000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/distillation/response_distiller.py:after\t2025-07-29 20:12:22.665487\n@@ -4,9 +4,9 @@\n Permet de fusionner plusieurs r\u00e9ponses IA en une solution optimale\n (voting, stacking, bagging, consensus scoring...)\n \"\"\"\n-from typing import List, Dict, Any, Optional\n+import random\n from collections import Counter\n-import random\n+from typing import Any, Dict, List, Optional\n \n \n class ResponseDistiller:\n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/__init__.py:before\t2025-07-29 19:56:31.360000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/__init__.py:after\t2025-07-29 20:12:22.670955\n@@ -17,11 +17,11 @@\n - robotics_ci: CI/CD robotique\n \"\"\"\n \n+from .docker_robotics import DockerRoboticsManager\n from .reachy_auditor import ReachyAuditor\n+from .robotics_ci import RoboticsCI\n from .ros2_validator import ROS2Validator\n-from .docker_robotics import DockerRoboticsManager\n from .rust_analyzer import RustAnalyzer\n-from .robotics_ci import RoboticsCI\n \n __all__ = [\n     'ReachyAuditor',\n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/docker_robotics.py:before\t2025-07-29 19:56:31.470000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/docker_robotics.py:after\t2025-07-29 20:12:22.672798\n@@ -9,13 +9,14 @@\n - Images sp\u00e9cialis\u00e9es\n \"\"\"\n \n+import logging\n import os\n-import yaml\n import subprocess\n+from dataclasses import dataclass\n from pathlib import Path\n from typing import Dict, List, Optional\n-from dataclasses import dataclass\n-import logging\n+\n+import yaml\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/reachy_auditor.py:before\t2025-07-29 20:02:53.620000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/reachy_auditor.py:after\t2025-07-29 20:12:22.674675\n@@ -10,13 +10,13 @@\n - Tests de connectivit\u00e9\n \"\"\"\n \n-import yaml\n+import logging\n+from dataclasses import dataclass\n+from datetime import datetime\n from pathlib import Path\n from typing import List, Optional, Tuple\n-from dataclasses import dataclass\n-from datetime import datetime\n-\n-import logging\n+\n+import yaml\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/robotics_ci.py:before\t2025-07-29 19:56:31.780000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/robotics_ci.py:after\t2025-07-29 20:12:22.677018\n@@ -10,12 +10,12 @@\n - D\u00e9ploiement automatis\u00e9\n \"\"\"\n \n+import logging\n import subprocess\n+import time\n+from dataclasses import dataclass\n from pathlib import Path\n from typing import Dict, List, Tuple\n-from dataclasses import dataclass\n-import logging\n-import time\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/robotics/ros2_validator.py:before\t2025-07-29 19:56:31.990000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/robotics/ros2_validator.py:after\t2025-07-29 20:12:22.679694\n@@ -10,13 +10,13 @@\n - Build system\n \"\"\"\n \n+import ast\n+import logging\n+import subprocess\n import xml.etree.ElementTree as ET\n+from dataclasses import dataclass\n from pathlib import Path\n from typing import Dict, List, Optional\n-from dataclasses import dataclass\n-import logging\n-import subprocess\n-import ast\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/templates/artistic_templates.py:before\t2025-07-29 19:56:32.950000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/templates/artistic_templates.py:after\t2025-07-29 20:12:22.686120\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import logging\n from typing import Dict\n-import logging\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/athalia_core/templates/base_templates.py:before\t2025-07-29 19:56:33.090000\n+++ /Volumes/T7/athalia-dev-setup/athalia_core/templates/base_templates.py:after\t2025-07-29 20:12:22.687507\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python3\n # -*- coding: utf-8 -*-\n+import logging\n from typing import Dict\n-import logging\n \n logger = logging.getLogger(__name__)\n \n--- /Volumes/T7/athalia-dev-setup/bin/ath-audit.py:before\t2025-07-29 19:49:47.610000\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-audit.py:after\t2025-07-29 20:12:22.691208\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python3\n+import argparse\n import subprocess\n import sys\n-import argparse\n \n \n def main():\n--- /Volumes/T7/athalia-dev-setup/bin/ath-lint-secure:before\t2025-07-29 20:12:11.640000\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-lint-secure:after\t2025-07-29 20:12:22.693045\n@@ -4,13 +4,13 @@\n Ex\u00e9cute tous les outils de qualit\u00e9 et s\u00e9curit\u00e9.\n \"\"\"\n \n+import json\n+import os\n import subprocess\n import sys\n-import os\n+import time\n from pathlib import Path\n-from typing import List, Dict, Any\n-import json\n-import time\n+from typing import Any, Dict, List\n \n # Configuration\n TOOLS = {\n--- /Volumes/T7/athalia-dev-setup/bin/ath-coverage.py:before\t2025-07-29 19:49:47.610000\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-coverage.py:after\t2025-07-29 20:12:22.695620\n@@ -1,8 +1,8 @@\n #!/usr/bin/env python3\n+import argparse\n+import os\n import subprocess\n import sys\n-import argparse\n-import os\n \n \n def main():\n--- /Volumes/T7/athalia-dev-setup/bin/ath-test.py:before\t2025-07-29 19:49:47.610000\n+++ /Volumes/T7/athalia-dev-setup/bin/ath-test.py:after\t2025-07-29 20:12:22.697109\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python3\n+import os\n import subprocess\n import sys\n-import os\n \n \n def main():\n--- /Volumes/T7/athalia-dev-setup/scripts/monitor_processes.py:before\t2025-07-29 19:49:47.620000\n+++ /Volumes/T7/athalia-dev-setup/scripts/monitor_processes.py:after\t2025-07-29 20:12:22.703912\n@@ -6,7 +6,7 @@\n \n import logging\n import time\n-from typing import Dict, Any, List\n+from typing import Any, Dict, List\n \n import psutil\n \n--- /Volumes/T7/athalia-dev-setup/scripts/validation_continue.py:before\t2025-07-29 19:49:47.620000\n+++ /Volumes/T7/athalia-dev-setup/scripts/validation_continue.py:after\t2025-07-29 20:12:22.707000\n@@ -5,13 +5,13 @@\n Surveillance en temps r\u00e9el de la qualit\u00e9 et d\u00e9tection de r\u00e9gressions\n \"\"\"\n \n+import json\n+import logging\n+import os\n+import subprocess\n+import threading\n import time\n-import json\n-import os\n-import threading\n-import logging\n from datetime import datetime\n-import subprocess\n \n \n class ValidationContinue:\n--- /Volumes/T7/athalia-dev-setup/scripts/quick_performance_test.py:before\t2025-07-29 19:54:37.960000\n+++ /Volumes/T7/athalia-dev-setup/scripts/quick_performance_test.py:after\t2025-07-29 20:12:22.708372\n@@ -5,11 +5,12 @@\n Version cibl\u00e9e et rapide\n \"\"\"\n \n+import json\n+import os\n import time\n+from datetime import datetime\n+\n import psutil\n-import os\n-import json\n-from datetime import datetime\n \n \n def quick_performance_test():\n--- /Volumes/T7/athalia-dev-setup/scripts/validation_dashboard_simple.py:before\t2025-07-29 19:49:47.620000\n+++ /Volumes/T7/athalia-dev-setup/scripts/validation_dashboard_simple.py:after\t2025-07-29 20:12:22.709853\n@@ -6,8 +6,8 @@\n \"\"\"\n \n import http.server\n+import json\n import socketserver\n-import json\n import subprocess\n from datetime import datetime\n \n--- /Volumes/T7/athalia-dev-setup/scripts/validation_objective.py:before\t2025-07-29 19:49:47.620000\n+++ /Volumes/T7/athalia-dev-setup/scripts/validation_objective.py:after\t2025-07-29 20:12:22.713272\n@@ -5,12 +5,12 @@\n Tests qui ne peuvent pas mentir - Mesures concr\u00e8tes et ind\u00e9pendantes\n \"\"\"\n \n+import json\n+import os\n import subprocess\n import time\n-import json\n-import os\n+from datetime import datetime\n from pathlib import Path\n-from datetime import datetime\n \n \n class ValidationObjective:\n--- /Volumes/T7/athalia-dev-setup/scripts/test_athalia_performance.py:before\t2025-07-29 19:54:51.270000\n+++ /Volumes/T7/athalia-dev-setup/scripts/test_athalia_performance.py:after\t2025-07-29 20:12:22.715004\n@@ -4,10 +4,10 @@\n Test de performance d'Athalia avec cache\n \"\"\"\n \n-import time\n-import subprocess\n import json\n import os\n+import subprocess\n+import time\n from datetime import datetime\n \n \nSkipped 3 files\n",
      "error": "ERROR: /Volumes/T7/athalia-dev-setup/athalia_unified.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/__init__.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_cleanup.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/audit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_i18n.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/audit_complet_dossiers.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/debug_correction.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/correction_finale.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_plugin_complet.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_plugins.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_adaptive_distillation.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/optimize_performance.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_advanced_analytics_unit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_ai_robust_integration.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_athalia_simple.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_agent_network.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_ai_robust.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_continue_models.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_ai_robust_unit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_analytics.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_security_auditor_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_analytics_unit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_predictive_cache.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_coverage_threshold.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_code_linter_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_audit_agent.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_plugins_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_audit_intelligent.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_project_importer.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_auto_cicd_unit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_hardcoded_paths.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_auto_cleaner_unit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_ready_check.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_auto_correction_advanced_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_cache_simple.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_imports_all.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_auto_correction_avancee.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_cli_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_auto_documenter_unit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_auto_tester_unit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_autocomplete_server.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_profils_utilisateur_avances.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_benchmark_critical.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_correction_optimizer_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_ci_robust.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_no_polluting_files.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_ci_ultra_fast.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_security_patterns.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_security_comprehensive.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_error_handling.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_requirements_consistency.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_code_genetics.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_robotics_ci_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_ros2_validator_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_security.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_correction.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_plugins_validator_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_encoding_utf8.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_generation.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_performance_optimization.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_robotics_docker_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_intelligent_memory.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_lint_flake8.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_multi_file_editor.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_multimodal_distiller.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_robotics_reachy_auditor_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_onboarding.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/test_user_profiles_advanced_complete.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_audit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_build.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_coverage.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_test.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/bin/test_ath_lint.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/integration/test_cli_robustesse.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/integration/test_end_to_end.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tests/integration/test_yaml_validity.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/plugins/plugins_manager.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/plugins/plugins_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tools/analysis/audit_complet_dossiers.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tools/analysis/verification_integration_simple.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tools/monitoring/system_monitor.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tools/maintenance/cleanup_archives.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/tools/maintenance/validation_documentation.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/__init__.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/advanced_analytics.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/analytics.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/auto_tester.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/architecture_analyzer.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/ast_analyzer.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/generation.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/auto_cicd.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/auto_cleaner.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/robotics_ci.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/auto_documenter.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/plugins_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_engine.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/autocomplete_server.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/cli.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/code_linter.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.fonctionnels Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.phase2 Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/ci.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/cleanup.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_auditor.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/dashboard.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/main.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/config_manager.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/ros2_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/unified_orchestrator.py.backup.phase3 Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/correction_optimizer.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_analyzer.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/project_importer.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/intelligent_memory.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/ready_check.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/security_auditor.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/logger_advanced.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/multi_file_editor.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/pattern_detector.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/performance_analyzer.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/ai_robust_broken.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/athalia_orchestrator.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/cache_manager.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/error_codes.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/error_handling.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/generation_simple.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/auto_correction_advanced.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/dashboard_unified.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/advanced_modules/user_profiles_advanced.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/agents/context_prompt.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/classification/project_types.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/distillation/adaptive_distillation.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/distillation/audit_distiller.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/distillation/code_genetics.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/distillation/correction_distiller.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/distillation/multimodal_distiller.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/distillation/predictive_cache.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/distillation/response_distiller.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/robotics/__init__.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/robotics/docker_robotics.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/robotics/reachy_auditor.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/robotics/robotics_ci.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/robotics/ros2_validator.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/templates/artistic_templates.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/athalia_core/templates/base_templates.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/bin/ath-audit.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/bin/ath-lint-secure Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/bin/ath-coverage.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/bin/ath-test.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/scripts/monitor_processes.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/scripts/validation_continue.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/scripts/quick_performance_test.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/scripts/validation_dashboard_simple.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/scripts/validation_objective.py Imports are incorrectly sorted and/or formatted.\nERROR: /Volumes/T7/athalia-dev-setup/scripts/test_athalia_performance.py Imports are incorrectly sorted and/or formatted.\n",
      "duration": 0.71014404296875,
      "critical": false
    },
    {
      "name": "flake8",
      "description": "Analyse statique du code",
      "success": false,
      "output": "",
      "error": "multiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/checker.py\", line 82, in _mp_run\n    ).run_checks()\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/checker.py\", line 526, in run_checks\n    self.run_ast_checks()\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/checker.py\", line 418, in run_ast_checks\n    ast = self.processor.build_ast()\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/processor.py\", line 240, in build_ast\n    return ast.parse(\"\".join(self.lines))\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/ast.py\", line 50, in parse\n    return compile(source, filename, mode, flags,\nValueError: source code string cannot contain null bytes\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/bin/flake8\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/main/cli.py\", line 23, in main\n    app.run(argv)\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/main/application.py\", line 198, in run\n    self._run(argv)\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/main/application.py\", line 187, in _run\n    self.run_checks()\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/main/application.py\", line 103, in run_checks\n    self.file_checker_manager.run()\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/checker.py\", line 235, in run\n    self.run_parallel()\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/site-packages/flake8/checker.py\", line 204, in run_parallel\n    self.results = list(pool.imap_unordered(_mp_run, self.filenames))\n  File \"/opt/homebrew/opt/pyenv/versions/3.10.14/lib/python3.10/multiprocessing/pool.py\", line 873, in next\n    raise value\nValueError: source code string cannot contain null bytes\n",
      "duration": 0.5012049674987793,
      "critical": true
    },
    {
      "name": "mypy",
      "description": "V\u00e9rification des types",
      "success": false,
      "output": "athalia_core/security.py: note: In function \"security_audit_project\":\nathalia_core/security.py:12:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def security_audit_project(project_path):\n    ^\nathalia_core/onboarding.py: note: In function \"generate_onboarding_md\":\nathalia_core/onboarding.py:9:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def generate_onboarding_md(blueprint, outdir):\n    ^\nathalia_core/onboarding.py: note: In function \"generate_onboard_cli\":\nathalia_core/onboarding.py:18:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def generate_onboard_cli(blueprint, outdir):\n    ^\nathalia_core/onboarding.py: note: In function \"generate_onboarding_html_advanced\":\nathalia_core/onboarding.py:27:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def generate_onboarding_html_advanced(blueprint, outdir):\n    ^\nathalia_core/error_codes.py: note: In function \"format_error_message\":\nathalia_core/error_codes.py:182:94: error: Incompatible default for argument\n\"context\" (default has type \"None\", argument has type \"dict[str, Any]\") \n[assignment]\n    ... ErrorCode, details: str = \"\", context: Dict[str, Any] = None) -> str:\n                                                                ^~~~\nathalia_core/error_codes.py:182:94: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/error_codes.py:182:94: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/dashboard.py:1:1: error: Cannot find implementation or library\nstub for module named \"streamlit\"  [import-not-found]\n    import streamlit as st\n    ^\nathalia_core/dashboard.py:2:1: error: Library stubs not installed for \"pandas\" \n[import-untyped]\n    import pandas as pd\n    ^\nathalia_core/dashboard.py:2:1: note: Hint: \"python3 -m pip install pandas-stubs\"\nathalia_core/dashboard.py:2:1: note: (or run \"mypy --install-types\" to install all missing stub packages)\nathalia_core/dashboard.py: note: In function \"show_benchmarks\":\nathalia_core/dashboard.py:6:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def show_benchmarks():\n    ^\nathalia_core/dashboard.py:6:1: note: Use \"-> None\" if function does not return a value\nathalia_core/dashboard.py: note: In function \"main\":\nathalia_core/dashboard.py:58:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/dashboard.py:58:1: note: Use \"-> None\" if function does not return a value\nathalia_core/distillation/predictive_cache.py: note: In member \"set\" of class \"PredictiveCache\":\nathalia_core/distillation/predictive_cache.py:32:5: error: Function is missing\na return type annotation  [no-untyped-def]\n        def set(self, key: str, value: Any):\n        ^\nathalia_core/distillation/predictive_cache.py: note: In member \"pre_generate\" of class \"PredictiveCache\":\nathalia_core/distillation/predictive_cache.py:39:5: error: Function is missing\na return type annotation  [no-untyped-def]\n        def pre_generate(self, context: Dict, generator: Callable[[Dict], ...\n        ^\nathalia_core/distillation/predictive_cache.py: note: In member \"invalidate\" of class \"PredictiveCache\":\nathalia_core/distillation/predictive_cache.py:50:5: error: Function is missing\na return type annotation  [no-untyped-def]\n        def invalidate(self, key: str):\n        ^\nathalia_core/analytics.py: note: In function \"generate_analytics_html\":\nathalia_core/analytics.py:126:19: error: Subclass of \"str\" and \"dict[Any, Any]\"\ncannot exist: would have incompatible method signatures  [unreachable]\n        if isinstance(project_path, dict) and \"path\" in project_path:\n                      ^~~~~~~~~~~~\nathalia_core/analytics.py:126:43: error: Right operand of \"and\" is never\nevaluated  [unreachable]\n        if isinstance(project_path, dict) and \"path\" in project_path:\n                                              ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/analytics.py:127:9: error: Statement is unreachable  [unreachable]\n            project_path = project_path[\"path\"]\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/analytics.py:128:21: error: Subclass of \"str\" and \"list[Any]\"\ncannot exist: would have incompatible method signatures  [unreachable]\n        elif isinstance(project_path, list) and project_path:\n                        ^~~~~~~~~~~~\nathalia_core/analytics.py:128:45: error: Right operand of \"and\" is never\nevaluated  [unreachable]\n        elif isinstance(project_path, list) and project_path:\n                                                ^~~~~~~~~~~~\nathalia_core/analytics.py:129:9: error: Statement is unreachable  [unreachable]\n            project_path = project_path[0]\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/analytics.py:131:9: error: Statement is unreachable  [unreachable]\n            project_path = \".\"\n            ^~~~~~~~~~~~~~~~~~\nathalia_core/distillation/adaptive_distillation.py: note: In member \"update_preferences\" of class \"AdaptiveDistiller\":\nathalia_core/distillation/adaptive_distillation.py:38:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def update_preferences(\n        ^\nathalia_core/distillation/adaptive_distillation.py: note: In function \"apply_learned_weights\":\nathalia_core/distillation/adaptive_distillation.py:67:9: error: Function is\nmissing a type annotation  [no-untyped-def]\n            def score(r):\n            ^\nathalia_core/distillation/adaptive_distillation.py: note: In member \"save_history\" of class \"AdaptiveDistiller\":\nathalia_core/distillation/adaptive_distillation.py:90:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def save_history(self):\n        ^\nathalia_core/distillation/adaptive_distillation.py:90:5: note: Use \"-> None\" if function does not return a value\nathalia_core/distillation/adaptive_distillation.py: note: In member \"load_history\" of class \"AdaptiveDistiller\":\nathalia_core/distillation/adaptive_distillation.py:103:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def load_history(self):\n        ^\nathalia_core/distillation/adaptive_distillation.py:103:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py: note: In member \"run\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:55:29: error: No overload variant of \"int\"\nmatches argument type \"object\"  [call-overload]\n                'global_score': int(self.report.get('score', 0)),\n                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:55:29: note: Possible overload variants:\nathalia_core/security_auditor.py:55:29: note:     def __new__(cls, str | Buffer | SupportsInt | SupportsIndex | SupportsTrunc = ..., /) -> int\nathalia_core/security_auditor.py:55:29: note:     def __new__(cls, str | bytes | bytearray, /, base: SupportsIndex) -> int\nathalia_core/security_auditor.py:56:24: error: No overload variant of \"list\"\nmatches argument type \"object\"  [call-overload]\n                'summary': list(self.report.get('warnings', [])),\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:56:24: note: Possible overload variants:\nathalia_core/security_auditor.py:56:24: note:     def [_T] __init__(self) -> list[_T]\nathalia_core/security_auditor.py:56:24: note:     def [_T] __init__(self, Iterable[_T], /) -> list[_T]\nathalia_core/security_auditor.py:57:24: error: No overload variant of \"list\"\nmatches argument type \"object\"  [call-overload]\n                'details': list(self.report.get('vulnerabilities', [])),\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:57:24: note: Possible overload variants:\nathalia_core/security_auditor.py:57:24: note:     def [_T] __init__(self) -> list[_T]\nathalia_core/security_auditor.py:57:24: note:     def [_T] __init__(self, Iterable[_T], /) -> list[_T]\nathalia_core/security_auditor.py:58:22: error: No overload variant of \"list\"\nmatches argument type \"object\"  [call-overload]\n                'files': list(self.report.get('recommendations', []))\n                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:58:22: note: Possible overload variants:\nathalia_core/security_auditor.py:58:22: note:     def [_T] __init__(self) -> list[_T]\nathalia_core/security_auditor.py:58:22: note:     def [_T] __init__(self, Iterable[_T], /) -> list[_T]\nathalia_core/security_auditor.py: note: In member \"_check_dependencies\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:61:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _check_dependencies(self):\n        ^\nathalia_core/security_auditor.py:61:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py:70:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    self.report[\"vulnerabilities\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:73:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    self.report[\"vulnerabilities\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:77:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.report[\"warnings\"].append(f\"Bandit non ex\u00e9cut\u00e9: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:88:21: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                        self.report[\"vulnerabilities\"].append(\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:94:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.report[\"warnings\"].append(f\"Safety non ex\u00e9cut\u00e9: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py: note: In member \"_check_code_vulnerabilities\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:96:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _check_code_vulnerabilities(self):\n        ^\nathalia_core/security_auditor.py:96:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py:117:25: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                            self.report[\"vulnerabilities\"].append(\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py: note: In member \"_check_secrets\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:124:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _check_secrets(self):\n        ^\nathalia_core/security_auditor.py:124:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py:142:25: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                            self.report[\"vulnerabilities\"].append(\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py: note: In member \"_check_permissions\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:149:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _check_permissions(self):\n        ^\nathalia_core/security_auditor.py:149:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py:156:25: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                            self.report[\"warnings\"].append(\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py: note: In member \"_check_encryption\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:162:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _check_encryption(self):\n        ^\nathalia_core/security_auditor.py:162:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py:185:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.report[\"recommendations\"].append(\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py: note: In member \"_calculate_score\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:189:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _calculate_score(self):\n        ^\nathalia_core/security_auditor.py:189:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py:193:27: error: Argument 1 to \"len\" has\nincompatible type \"object\"; expected \"Sized\"  [arg-type]\n            base_score -= len(self.report[\"vulnerabilities\"]) * 20\n                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:194:27: error: Argument 1 to \"len\" has\nincompatible type \"object\"; expected \"Sized\"  [arg-type]\n            base_score -= len(self.report[\"warnings\"]) * 5\n                              ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py: note: In member \"print_report\" of class \"SecurityAuditor\":\nathalia_core/security_auditor.py:197:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def print_report(self):\n        ^\nathalia_core/security_auditor.py:197:5: note: Use \"-> None\" if function does not return a value\nathalia_core/security_auditor.py:203:22: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for v in self.report[\"vulnerabilities\"]:\n                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:208:22: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for w in self.report[\"warnings\"]:\n                         ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/security_auditor.py:213:22: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for r in self.report[\"recommendations\"]:\n                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ready_check.py: note: In function \"open_patch\":\nathalia_core/ready_check.py:11:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def open_patch(file, mode='r', *args, **kwargs):\n    ^\nathalia_core/ready_check.py: note: In function \"check_ready\":\nathalia_core/ready_check.py:26:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                report[\"missing\"].append(file_handle)\n                ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ready_check.py:29:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                report[\"missing\"].append(dict_data + \"/\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ready_check.py:30:23: error: Argument 1 to \"len\" has incompatible\ntype \"object\"; expected \"Sized\"  [arg-type]\n        report[\"f\"] = len(report[\"missing\"]) == 0\n                          ^~~~~~~~~~~~~~~~~\nathalia_core/project_importer.py: note: In member \"__init__\" of class \"ProjectImporter\":\nathalia_core/project_importer.py:22:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/project_importer.py:22:5: note: Use \"-> None\" if function does not return a value\nathalia_core/project_importer.py: note: In member \"_detect_project_type\" of class \"ProjectImporter\":\nathalia_core/project_importer.py:132:36: error: Argument \"key\" to \"max\" has\nincompatible type overloaded function; expected\n\"Callable[[str], SupportsDunderLT[Any] | SupportsDunderGT[Any]]\"  [arg-type]\n                return max(scores, key=scores.get)\n                                       ^~~~~~~~~~\nathalia_core/project_importer.py: note: In member \"_analyze_code_quality\" of class \"ProjectImporter\":\nathalia_core/project_importer.py:159:13: error: Unsupported operand types for +\n(\"object\" and \"int\")  [operator]\n                analysis['python_files_count'] += len(\n                ^\nathalia_core/project_importer.py:161:13: error: Unsupported operand types for +\n(\"object\" and \"int\")  [operator]\n                analysis['test_files_count'] += len(\n                ^\nathalia_core/project_importer.py:166:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                analysis['issues'].append(\"Aucun test\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/project_importer.py:168:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                analysis['issues'].append(\"Documentation\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/project_importer.py:170:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                analysis['issues'].append(\"Fichier requirements.txt\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/project_importer.py:172:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                analysis['issues'].append(\"README.md\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_memory.py: note: In class \"LearningEvent\":\nathalia_core/intelligent_memory.py:38:31: error: Incompatible types in\nassignment (expression has type \"None\", variable has type \"dict[str, Any]\") \n[assignment]\n        context: Dict[str, Any] = None\n                                  ^~~~\nathalia_core/intelligent_memory.py: note: In member \"__init__\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:65:41: error: Incompatible default for\nargument \"root_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n        def __init__(self, root_path: str = None):\n                                            ^~~~\nathalia_core/intelligent_memory.py:65:41: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_memory.py:65:41: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_memory.py:77:9: error: Need type annotation for\n\"_pattern_cache\" (hint: \"_pattern_cache: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self._pattern_cache = {}\n            ^~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_memory.py:78:9: error: Need type annotation for\n\"_prediction_cache\" (hint: \"_prediction_cache: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self._prediction_cache = {}\n            ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_memory.py:79:9: error: Need type annotation for\n\"_correction_cache\" (hint: \"_correction_cache: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self._correction_cache = {}\n            ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_memory.py: note: In member \"_init_database\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:83:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _init_database(self):\n        ^\nathalia_core/intelligent_memory.py:83:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_memory.py: note: In member \"learn_from_error\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:167:35: error: Incompatible default for\nargument \"context\" (default has type \"None\", argument has type \"dict[str, Any]\")\n [assignment]\n            context: Dict[str, Any] = None\n                                      ^~~~\nathalia_core/intelligent_memory.py:167:35: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_memory.py:167:35: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_memory.py: note: In member \"learn_from_correction\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:194:35: error: Incompatible default for\nargument \"context\" (default has type \"None\", argument has type \"dict[str, Any]\")\n [assignment]\n            context: Dict[str, Any] = None\n                                      ^~~~\nathalia_core/intelligent_memory.py:194:35: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_memory.py:194:35: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_memory.py: note: In member \"learn_from_duplicate\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:226:60: error: Incompatible default for\nargument \"context\" (default has type \"None\", argument has type \"dict[str, Any]\")\n [assignment]\n            similarity_score: float, context: Dict[str, Any] = None\n                                                               ^~~~\nathalia_core/intelligent_memory.py:226:60: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_memory.py:226:60: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_memory.py: note: In member \"predict_issues\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:251:45: error: Incompatible default for\nargument \"context\" (default has type \"None\", argument has type \"dict[str, Any]\")\n [assignment]\n                                         Any] = None) -> List[Prediction]:\n                                                ^~~~\nathalia_core/intelligent_memory.py:251:45: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_memory.py:251:45: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_memory.py: note: In member \"_record_learning_event\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:368:49: error: Incompatible default for\nargument \"resolution\" (default has type \"None\", argument has type \"str\") \n[assignment]\n            success: bool = True, resolution: str = None,\n                                                    ^~~~\nathalia_core/intelligent_memory.py:368:49: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_memory.py:368:49: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_memory.py:369:35: error: Incompatible default for\nargument \"context\" (default has type \"None\", argument has type \"dict[str, Any]\")\n [assignment]\n            context: Dict[str, Any] = None\n                                      ^~~~\nathalia_core/intelligent_memory.py:369:35: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_memory.py:369:35: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_memory.py: note: In member \"_update_pattern_learning\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:424:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _update_pattern_learning(\n        ^\nathalia_core/intelligent_memory.py: note: In member \"_generate_predictions_from_error\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:472:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _generate_predictions_from_error(\n        ^\nathalia_core/intelligent_memory.py: note: In member \"_save_correction_suggestion\" of class \"IntelligentMemory\":\nathalia_core/intelligent_memory.py:617:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _save_correction_suggestion(\n        ^\nathalia_core/intelligent_memory.py: note: In function \"main\":\nathalia_core/intelligent_memory.py:644:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/intelligent_memory.py:644:1: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"__init__\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:24:44: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n        def __init__(self, project_path: str = None):\n                                               ^~~~\nathalia_core/intelligent_auditor.py:24:44: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_auditor.py:24:44: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_auditor.py:26:9: error: Need type annotation for\n\"audit_results\" (hint: \"audit_results: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self.audit_results = {}\n            ^~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py:27:9: error: Need type annotation for\n\"recommendations\" (hint: \"recommendations: list[<type>] = ...\")  [var-annotated]\n            self.recommendations = []\n            ^~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_project_info\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:70:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_project_info(self):\n        ^\nathalia_core/intelligent_auditor.py:70:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py:73:21: error: Item \"None\" of \"Path | None\"\nhas no attribute \"name\"  [union-attr]\n                \"name\": self.project_path.name,\n                        ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_detect_project_type\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:84:22: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            files = list(self.project_path.rglob(\"*\"))\n                         ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_calculate_project_size\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:105:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_detect_languages\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:133:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_detect_dependencies\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:158:20: error: Unsupported left operand\ntype for / (\"None\")  [operator]\n            req_file = self.project_path / \"requirements.txt\"\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py:158:20: note: Left operand is of type \"Path | None\"\nathalia_core/intelligent_auditor.py:170:24: error: Unsupported left operand\ntype for / (\"None\")  [operator]\n            package_file = self.project_path / \"package.json\"\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py:170:24: note: Left operand is of type \"Path | None\"\nathalia_core/intelligent_auditor.py: note: In member \"_get_last_modified\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:185:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py:189:30: error: Incompatible types in\nassignment (expression has type \"float | Any\", variable has type \"int\") \n[assignment]\n                        latest = mtime\n                                 ^~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_code_quality\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:192:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_code_quality(self):\n        ^\nathalia_core/intelligent_auditor.py:192:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_complexity\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:206:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_style\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:245:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_code_documentation\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:275:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_naming_conventions\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:303:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_security\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:324:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_security(self):\n        ^\nathalia_core/intelligent_auditor.py:324:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"_detect_security_vulnerabilities\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:347:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_detect_secrets\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:370:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_permissions\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:389:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*f\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_performance\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:403:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_performance(self):\n        ^\nathalia_core/intelligent_auditor.py:403:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_file_sizes\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:417:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_imports\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:435:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_estimate_memory_usage\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:459:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_documentation\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:470:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_documentation(self):\n        ^\nathalia_core/intelligent_auditor.py:470:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"_check_readme\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:482:29: error: Item \"None\" of \"Path | None\"\nhas no attribute \"glob\"  [union-attr]\n            readme_files = list(self.project_path.glob(\"README*\"))\n                                ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_check_api_documentation\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:507:9: error: Need type annotation for\n\"api_docs\" (hint: \"api_docs: list[<type>] = ...\")  [var-annotated]\n            api_docs = []\n            ^~~~~~~~\nathalia_core/intelligent_auditor.py:510:29: error: Item \"None\" of \"Path | None\"\nhas no attribute \"glob\"  [union-attr]\n                api_docs.extend(self.project_path.glob(pattern))\n                                ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_check_guides\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:519:9: error: Need type annotation for\n\"guides\" (hint: \"guides: list[<type>] = ...\")  [var-annotated]\n            guides = []\n            ^~~~~~\nathalia_core/intelligent_auditor.py:526:27: error: Item \"None\" of \"Path | None\"\nhas no attribute \"glob\"  [union-attr]\n                guides.extend(self.project_path.glob(pattern))\n                              ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_testing\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:533:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_testing(self):\n        ^\nathalia_core/intelligent_auditor.py:533:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_test_coverage\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:544:27: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            test_files = list(self.project_path.rglob(\"*test*.py\"))\n                              ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py:545:29: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            source_files = list(self.project_path.rglob(\"*.py\"))\n                                ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_find_test_files\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:564:9: error: Need type annotation for\n\"test_files\" (hint: \"test_files: list[<type>] = ...\")  [var-annotated]\n            test_files = []\n            ^~~~~~~~~~\nathalia_core/intelligent_auditor.py:567:31: error: Item \"None\" of \"Path | None\"\nhas no attribute \"glob\"  [union-attr]\n                test_files.extend(self.project_path.glob(pattern))\n                                  ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_test_quality\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:575:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*test*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_structure\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:593:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_structure(self):\n        ^\nathalia_core/intelligent_auditor.py:593:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_organization\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:605:40: error: Item \"None\" of \"Path | None\"\nhas no attribute \"iterdir\"  [union-attr]\n                dict_data for dict_data in self.project_path.iterdir() if ...\n                                           ^~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_structure_naming\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:623:21: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for item in self.project_path.rglob(\"*\"):\n                        ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_analyze_modularity\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:640:26: error: Item \"None\" of \"Path | None\"\nhas no attribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*.py\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_auditor.py: note: In member \"_calculate_score\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:650:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _calculate_score(self):\n        ^\nathalia_core/intelligent_auditor.py:650:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In member \"_generate_recommendations\" of class \"IntelligentAuditor\":\nathalia_core/intelligent_auditor.py:699:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _generate_recommendations(self):\n        ^\nathalia_core/intelligent_auditor.py:699:5: note: Use \"-> None\" if function does not return a value\nathalia_core/intelligent_auditor.py: note: In function \"main\":\nathalia_core/intelligent_auditor.py:762:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/intelligent_auditor.py:762:1: note: Use \"-> None\" if function does not return a value\nathalia_core/error_handling.py: note: In member \"__init__\" of class \"AthaliaError\":\nathalia_core/error_handling.py:22:44: error: Incompatible default for argument\n\"context\" (default has type \"None\", argument has type \"dict[str, Any]\") \n[assignment]\n                     context: Dict[str, Any] = None):\n                                               ^~~~\nathalia_core/error_handling.py:22:44: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/error_handling.py:22:44: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/error_handling.py: note: In member \"__init__\" of class \"ErrorHandler\":\nathalia_core/error_handling.py:57:9: error: Need type annotation for\n\"critical_errors\" (hint: \"critical_errors: list[<type>] = ...\")  [var-annotated]\n            self.critical_errors = []\n            ^~~~~~~~~~~~~~~~~~~~\nathalia_core/error_handling.py: note: In member \"_setup_logging\" of class \"ErrorHandler\":\nathalia_core/error_handling.py:63:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _setup_logging(self):\n        ^\nathalia_core/error_handling.py:63:5: note: Use \"-> None\" if function does not return a value\nathalia_core/error_handling.py:78:29: error: Argument 1 to \"append\" of \"list\"\nhas incompatible type \"FileHandler\"; expected \"StreamHandler[TextIO | Any]\" \n[arg-type]\n                handlers.append(file_handler)\n                                ^~~~~~~~~~~~\nathalia_core/error_handling.py: note: In member \"handle_error\" of class \"ErrorHandler\":\nathalia_core/error_handling.py:89:72: error: Incompatible default for argument\n\"context\" (default has type \"None\", argument has type \"dict[str, Any]\") \n[assignment]\n    ...elf, error: Exception, context: Dict[str, Any] = None) -> AthaliaError...\n                                                        ^~~~\nathalia_core/error_handling.py:89:72: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/error_handling.py:89:72: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/error_handling.py: note: In member \"_log_error\" of class \"ErrorHandler\":\nathalia_core/error_handling.py:141:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _log_error(self, error: AthaliaError):\n        ^\nathalia_core/error_handling.py: note: In member \"register_callback\" of class \"ErrorHandler\":\nathalia_core/error_handling.py:154:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def register_callback(self, error_code: ErrorCode, callback: Calla...\n        ^\nathalia_core/error_handling.py: note: In member \"clear_errors\" of class \"ErrorHandler\":\nathalia_core/error_handling.py:167:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def clear_errors(self):\n        ^\nathalia_core/error_handling.py:167:5: note: Use \"-> None\" if function does not return a value\nathalia_core/error_handling.py: note: In function \"handle_error\":\nathalia_core/error_handling.py:187:62: error: Incompatible default for argument\n\"context\" (default has type \"None\", argument has type \"dict[str, Any]\") \n[assignment]\n    ...rror(error: Exception, context: Dict[str, Any] = None) -> AthaliaError...\n                                                        ^~~~\nathalia_core/error_handling.py:187:62: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/error_handling.py:187:62: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/error_handling.py: note: In function \"raise_athalia_error\":\nathalia_core/error_handling.py:192:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def raise_athalia_error(error_code: ErrorCode, message: str = \"\", deta...\n    ^\nathalia_core/error_handling.py:193:50: error: Incompatible default for argument\n\"context\" (default has type \"None\", argument has type \"dict[str, Any]\") \n[assignment]\n                           context: Dict[str, Any] = None):\n                                                     ^~~~\nathalia_core/error_handling.py:193:50: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/error_handling.py:193:50: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/error_handling.py: note: In function \"error_handler\":\nathalia_core/error_handling.py:201:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def error_handler(error_code: ErrorCode = ErrorCode.UNKNOWN_ERROR):\n    ^\nathalia_core/error_handling.py:204:9: error: Function is missing a type\nannotation  [no-untyped-def]\n            def wrapper(*args, **kwargs):\n            ^\nathalia_core/error_handling.py: note: In member \"__init__\" of class \"ErrorContext\":\nathalia_core/error_handling.py:220:99: error: Incompatible default for argument\n\"context\" (default has type \"None\", argument has type \"dict[str, Any]\") \n[assignment]\n    ... ErrorCode = ErrorCode.UNKNOWN_ERROR, context: Dict[str, Any] = None):\n                                                                       ^~~~\nathalia_core/error_handling.py:220:99: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/error_handling.py:220:99: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/error_handling.py: note: In member \"__enter__\" of class \"ErrorContext\":\nathalia_core/error_handling.py:224:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def __enter__(self):\n        ^\nathalia_core/error_handling.py: note: In member \"__exit__\" of class \"ErrorContext\":\nathalia_core/error_handling.py:227:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def __exit__(self, exc_type, exc_val, exc_tb):\n        ^\nathalia_core/code_linter.py: note: In member \"_run_flake8\" of class \"CodeLinter\":\nathalia_core/code_linter.py:44:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_flake8(self):\n        ^\nathalia_core/code_linter.py:44:5: note: Use \"-> None\" if function does not return a value\nathalia_core/code_linter.py:54:25: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                            self.report[\"errors\"].append(f\"Flake8: {line}\"...\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:57:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                self.report[\"errors\"].append(f\"Flake8 non ex\u00e9cut\u00e9: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py: note: In member \"_run_black\" of class \"CodeLinter\":\nathalia_core/code_linter.py:59:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_black(self):\n        ^\nathalia_core/code_linter.py:59:5: note: Use \"-> None\" if function does not return a value\nathalia_core/code_linter.py:67:17: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                    self.report[\"warnings\"].append(\"Formatage Black \u00e0 corr...\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:70:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                self.report[\"warnings\"].append(f\"Black non ex\u00e9cut\u00e9: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py: note: In member \"_run_isort\" of class \"CodeLinter\":\nathalia_core/code_linter.py:72:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_isort(self):\n        ^\nathalia_core/code_linter.py:72:5: note: Use \"-> None\" if function does not return a value\nathalia_core/code_linter.py:80:17: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                    self.report[\"warnings\"].append(\"Tri des imports \u00e0 corr...\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:83:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                self.report[\"warnings\"].append(f\"isort non ex\u00e9cut\u00e9: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py: note: In member \"_run_mypy\" of class \"CodeLinter\":\nathalia_core/code_linter.py:85:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_mypy(self):\n        ^\nathalia_core/code_linter.py:85:5: note: Use \"-> None\" if function does not return a value\nathalia_core/code_linter.py:95:25: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                            self.report[\"warnings\"].append(f\"MyPy: {line}\"...\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:98:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                self.report[\"warnings\"].append(f\"Mypy non ex\u00e9cut\u00e9: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py: note: In member \"_run_bandit\" of class \"CodeLinter\":\nathalia_core/code_linter.py:100:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_bandit(self):\n        ^\nathalia_core/code_linter.py:100:5: note: Use \"-> None\" if function does not return a value\nathalia_core/code_linter.py:110:25: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                            self.report[\"warnings\"].append(f\"Bandit: {line...\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:113:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                self.report[\"warnings\"].append(f\"Bandit non ex\u00e9cut\u00e9: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py: note: In member \"_calculate_score\" of class \"CodeLinter\":\nathalia_core/code_linter.py:115:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _calculate_score(self):\n        ^\nathalia_core/code_linter.py:115:5: note: Use \"-> None\" if function does not return a value\nathalia_core/code_linter.py:118:27: error: Argument 1 to \"len\" has incompatible\ntype \"object\"; expected \"Sized\"  [arg-type]\n            base_score -= len(self.report[\"errors\"]) * 10\n                              ^~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:119:27: error: Argument 1 to \"len\" has incompatible\ntype \"object\"; expected \"Sized\"  [arg-type]\n            base_score -= len(self.report[\"warnings\"]) * 3\n                              ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:120:27: error: Argument 1 to \"len\" has incompatible\ntype \"object\"; expected \"Sized\"  [arg-type]\n            base_score -= len(self.report[\"fixes\"]) * 2\n                              ^~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py: note: In member \"print_report\" of class \"CodeLinter\":\nathalia_core/code_linter.py:123:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def print_report(self):\n        ^\nathalia_core/code_linter.py:123:5: note: Use \"-> None\" if function does not return a value\nathalia_core/code_linter.py:129:24: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for err in self.report[\"errors\"]:\n                           ^~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:134:25: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for warn in self.report[\"warnings\"]:\n                            ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/code_linter.py:139:24: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for fix in self.report[\"fixes\"]:\n                           ^~~~~~~~~~~~~~~~~~~~\nathalia_core/ci.py: note: In function \"generate_github_ci_yaml\":\nathalia_core/ci.py:12:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def generate_github_ci_yaml(outdir):\n    ^\nathalia_core/ci.py: note: In function \"add_coverage_badge\":\nathalia_core/ci.py:32:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def add_coverage_badge(outdir):\n    ^\nathalia_core/cache_manager.py: note: In member \"__init__\" of class \"AnalysisCache\":\nathalia_core/cache_manager.py:40:9: error: Need type annotation for\n\"_memory_cache\" (hint: \"_memory_cache: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self._memory_cache = {}\n            ^~~~~~~~~~~~~~~~~~\nathalia_core/cache_manager.py: note: In member \"_generate_cache_key\" of class \"AnalysisCache\":\nathalia_core/cache_manager.py:44:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def _generate_cache_key(self, project_path: str, analysis_type: st...\n        ^\nathalia_core/cache_manager.py: note: In member \"get\" of class \"AnalysisCache\":\nathalia_core/cache_manager.py:87:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def get(self, project_path: str, analysis_type: str, **kwargs) -> ...\n        ^\nathalia_core/cache_manager.py:105:13: error: Returning Any from function\ndeclared to return \"dict[str, Any] | None\"  [no-any-return]\n                return self._memory_cache[cache_key]\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/cache_manager.py:120:17: error: Returning Any from function\ndeclared to return \"dict[str, Any] | None\"  [no-any-return]\n                    return result\n                    ^~~~~~~~~~~~~\nathalia_core/cache_manager.py: note: In member \"set\" of class \"AnalysisCache\":\nathalia_core/cache_manager.py:129:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def set(self, project_path: str, analysis_type: str, result: Dict[...\n        ^\nathalia_core/cache_manager.py: note: In function \"cached_analysis\":\nathalia_core/cache_manager.py:265:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def wrapper(project_path: str, *args, **kwargs):\n        ^\nathalia_core/cache_manager.py:265:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def wrapper(project_path: str, *args, **kwargs):\n        ^\nathalia_core/cache_manager.py: note: In function \"cached_function\":\nathalia_core/cache_manager.py:302:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def cached_function(max_size: int = 1000):\n    ^\nathalia_core/auto_tester.py: note: In member \"__init__\" of class \"AutoTester\":\nathalia_core/auto_tester.py:20:44: error: Incompatible default for argument\n\"project_path\" (default has type \"None\", argument has type \"str\")  [assignment]\n        def __init__(self, project_path: str = None):\n                                               ^~~~\nathalia_core/auto_tester.py:20:44: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/auto_tester.py:20:44: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/auto_tester.py:23:9: error: Need type annotation for\n\"test_results\" (hint: \"test_results: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self.test_results = {}\n            ^~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:24:9: error: Need type annotation for\n\"generated_tests\" (hint: \"generated_tests: list[<type>] = ...\")  [var-annotated]\n            self.generated_tests = []\n            ^~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py: note: In member \"_analyze_modules\" of class \"AutoTester\":\nathalia_core/auto_tester.py:91:37: error: \"Sequence[str]\" has no attribute\n\"append\"  [attr-defined]\n                                        class_info['methods'].append(node....\n                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:92:29: error: \"Sequence[str]\" has no attribute\n\"append\"  [attr-defined]\n                                module_info['classes'].append(class_info)\n                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:97:29: error: \"Sequence[str]\" has no attribute\n\"append\"  [attr-defined]\n                                module_info['functions'].append(item.name)\n                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:101:37: error: \"Sequence[str]\" has no attribute\n\"append\"  [attr-defined]\n                                        module_info['imports'].append(alia...\n                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:103:33: error: \"Sequence[str]\" has no attribute\n\"append\"  [attr-defined]\n                                    module_info['imports'].append(\n                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py: note: In member \"_save_tests\" of class \"AutoTester\":\nathalia_core/auto_tester.py:352:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _save_tests(\n        ^\nathalia_core/auto_tester.py: note: In member \"_cleanup_generated_tests\" of class \"AutoTester\":\nathalia_core/auto_tester.py:442:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _cleanup_generated_tests(self):\n        ^\nathalia_core/auto_tester.py:442:5: note: Use \"-> None\" if function does not return a value\nathalia_core/auto_tester.py: note: In member \"_run_tests\" of class \"AutoTester\":\nathalia_core/auto_tester.py:499:21: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                        results[\"unit_tests\"][\"errors\"].append(result.stde...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:501:17: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                    results[\"unit_tests\"][\"errors\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:504:17: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                    results[\"unit_tests\"][\"errors\"].append(str(e))\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:523:21: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                        results[\"integration_tests\"][\"errors\"].append(\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:526:17: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                    results[\"integration_tests\"][\"errors\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:529:17: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                    results[\"integration_tests\"][\"errors\"].append(str(e))\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py:538:13: error: \"object\" has no attribute \"append\" \n[attr-defined]\n                results[\"unit_tests\"][\"errors\"].append(f\"Erreur g\u00e9n\u00e9rale: ...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_tester.py: note: In function \"main\":\nathalia_core/auto_tester.py:617:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/auto_tester.py:617:1: note: Use \"-> None\" if function does not return a value\nathalia_core/auto_documenter.py: note: In member \"_load_translations\" of class \"AutoDocumenter\":\nathalia_core/auto_documenter.py:36:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _load_translations(self, lang: str):\n        ^\nathalia_core/auto_documenter.py: note: In member \"_analyze_project\" of class \"AutoDocumenter\":\nathalia_core/auto_documenter.py:77:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _analyze_project(self):\n        ^\nathalia_core/auto_documenter.py:77:5: note: Use \"-> None\" if function does not return a value\nathalia_core/auto_documenter.py: note: In member \"_analyze_modules\" of class \"AutoDocumenter\":\nathalia_core/auto_documenter.py:258:29: error: \"Sequence[str]\" has no attribute\n\"append\"  [attr-defined]\n                                module_info[\"classes\"].append({\n                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_documenter.py:267:29: error: \"Sequence[str]\" has no attribute\n\"append\"  [attr-defined]\n                                module_info[\"functions\"].append({\n                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_documenter.py: note: In member \"_save_documents\" of class \"AutoDocumenter\":\nathalia_core/auto_documenter.py:712:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _save_documents(\n        ^\nathalia_core/auto_documenter.py: note: In function \"main\":\nathalia_core/auto_documenter.py:792:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/auto_documenter.py:792:1: note: Use \"-> None\" if function does not return a value\nathalia_core/auto_cicd.py: note: In member \"__init__\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:18:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/auto_cicd.py:18:5: note: Use \"-> None\" if function does not return a value\nathalia_core/auto_cicd.py: note: In member \"_analyze_project\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:45:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _analyze_project(self):\n        ^\nathalia_core/auto_cicd.py:45:5: note: Use \"-> None\" if function does not return a value\nathalia_core/auto_cicd.py:48:21: error: Item \"None\" of \"Any | None\" has no\nattribute \"name\"  [union-attr]\n                \"name\": self.project_path.name,\n                        ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py: note: In member \"_detect_project_type\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:59:13: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            if (self.project_path / \"package.json\").exists():\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:59:13: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py:61:15: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            elif (self.project_path / \"requirements.txt\").exists():\n                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:61:15: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py:63:15: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            elif (self.project_path / \"pom.xml\").exists():\n                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:63:15: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py:65:15: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            elif (self.project_path / \"Cargo.toml\").exists():\n                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:65:15: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py:67:15: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            elif (self.project_path / \"go.mod\").exists():\n                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:67:15: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py: note: In member \"_detect_languages\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:75:26: error: Item \"None\" of \"Any | None\" has no\nattribute \"rglob\"  [union-attr]\n            for file_path in self.project_path.rglob(\"*\"):\n                             ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py: note: In member \"_extract_dependencies\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:94:20: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            req_file = self.project_path / \"requirements.txt\"\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:94:20: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py:104:24: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            package_file = self.project_path / \"package.json\"\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:104:24: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py: note: In member \"_find_entry_points\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:121:25: error: Unsupported left operand type for /\n(\"None\")  [operator]\n                main_file = self.project_path / pattern\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:121:25: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py: note: In member \"_has_tests\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:130:21: error: Item \"None\" of \"Any | None\" has no\nattribute \"glob\"  [union-attr]\n                if list(self.project_path.glob(pattern)):\n                        ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py: note: In member \"_has_documentation\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:138:21: error: Item \"None\" of \"Any | None\" has no\nattribute \"glob\"  [union-attr]\n                if list(self.project_path.glob(pattern)):\n                        ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py: note: In member \"_save_cicd_configs\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:166:5: error: Function is missing a type annotation \n[no-untyped-def]\n        def _save_cicd_configs(\n        ^\nathalia_core/auto_cicd.py:172:23: error: Argument 1 to \"Path\" has incompatible\ntype \"Any | None\"; expected \"str | PathLike[str]\"  [arg-type]\n            ci_dir = Path(self.project_path) / '.f' / 'f'\n                          ^~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py: note: In member \"_get_created_files\" of class \"AutoCICD\":\nathalia_core/auto_cicd.py:178:19: error: Unsupported left operand type for /\n(\"None\")  [operator]\n            ci_file = self.project_path / '.f' / 'f' / 'ci.f(f'\n                      ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/auto_cicd.py:178:19: note: Left operand is of type \"Any | None\"\nathalia_core/auto_cicd.py: note: In function \"generate_github_ci_yaml\":\nathalia_core/auto_cicd.py:182:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def generate_github_ci_yaml(outdir):\n    ^\nathalia_core/auto_cicd.py: note: At top level:\nathalia_core/auto_cicd.py:193:1: error: Module has no attribute\n\"generate_github_ci_yaml\"  [attr-defined]\n    builtins.generate_github_ci_yaml = generate_github_ci_yaml\n    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ast_analyzer.py: note: In member \"__init__\" of class \"ASTAnalyzer\":\nathalia_core/ast_analyzer.py:48:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/ast_analyzer.py:48:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_analytics.py: note: In member \"_analyze_complexity\" of class \"AdvancedAnalytics\":\nathalia_core/advanced_analytics.py:51:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_complexity(self):\n        ^\nathalia_core/advanced_analytics.py:51:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_analytics.py:63:17: error: Unsupported target for indexed\nassignment (\"object\")  [index]\n                    complexity_data[\"complexity\"][\n                    ^\nathalia_core/advanced_analytics.py: note: In member \"_analyze_coverage\" of class \"AdvancedAnalytics\":\nathalia_core/advanced_analytics.py:91:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_coverage(self):\n        ^\nathalia_core/advanced_analytics.py:91:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_analytics.py: note: In member \"_analyze_performance\" of class \"AdvancedAnalytics\":\nathalia_core/advanced_analytics.py:137:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_performance(self):\n        ^\nathalia_core/advanced_analytics.py:137:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_analytics.py:148:17: error: Unsupported target for\nindexed assignment (\"object\")  [index]\n                    performance_data[\"file_sizes\"][str(\n                    ^\nathalia_core/advanced_analytics.py: note: In member \"_analyze_quality\" of class \"AdvancedAnalytics\":\nathalia_core/advanced_analytics.py:166:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_quality(self):\n        ^\nathalia_core/advanced_analytics.py:166:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_analytics.py: note: In member \"_analyze_evolution\" of class \"AdvancedAnalytics\":\nathalia_core/advanced_analytics.py:196:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _analyze_evolution(self):\n        ^\nathalia_core/advanced_analytics.py:196:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_analytics.py:205:13: error: Unsupported operand types for\n+ (\"None\" and \"int\")  [operator]\n                evolution_data[\"total_files\"] += 1\n                ^\nathalia_core/advanced_analytics.py:205:13: note: Left operand is of type \"int | None\"\nathalia_core/advanced_analytics.py:209:55: error: Incompatible types in\nassignment (expression has type \"float\", target has type \"int | None\") \n[assignment]\n                        evolution_data[\"last_modified\"] = mtime\n                                                          ^~~~~\nathalia_core/advanced_analytics.py: note: In member \"print_report\" of class \"AdvancedAnalytics\":\nathalia_core/advanced_analytics.py:329:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def print_report(self):\n        ^\nathalia_core/advanced_analytics.py:329:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_analytics.py: note: In function \"enrich_genesis_md\":\nathalia_core/advanced_analytics.py:334:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def enrich_genesis_md(outdir, infos, perf_log=None, test_log=None):\n    ^\nathalia_core/robotics/rust_analyzer.py: note: In member \"_generate_recommendations\" of class \"RustAnalyzer\":\nathalia_core/robotics/rust_analyzer.py:246:5: error: Function is missing a\nreturn type annotation  [no-untyped-def]\n        def _generate_recommendations(\n        ^\nathalia_core/robotics/rust_analyzer.py: note: In member \"validate_cargo_toml\" of class \"RustAnalyzer\":\nathalia_core/robotics/rust_analyzer.py:289:13: error: Need type annotation for\n\"cargo_data\" (hint: \"cargo_data: dict[<type>, <type>] = ...\")  [var-annotated]\n                cargo_data = {}\n                ^~~~~~~~~~\nathalia_core/robotics/ros2_validator.py: note: In member \"validate_workspace\" of class \"ROS2Validator\":\nathalia_core/robotics/ros2_validator.py:59:9: error: Need type annotation for\n\"recommendations\" (hint: \"recommendations: list[<type>] = ...\")  [var-annotated]\n            recommendations = []\n            ^~~~~~~~~~~~~~~\nathalia_core/robotics/ros2_validator.py: note: In member \"_analyze_package\" of class \"ROS2Validator\":\nathalia_core/robotics/ros2_validator.py:157:30: error: Argument \"dependencies\"\nto \"ROS2PackageInfo\" has incompatible type \"list[str | Any | None]\"; expected\n\"list[str]\"  [arg-type]\n                    dependencies=dependencies,\n                                 ^~~~~~~~~~~~\nathalia_core/robotics/robotics_ci.py: note: In member \"run_ci_pipeline\" of class \"RoboticsCI\":\nathalia_core/robotics/robotics_ci.py:196:9: error: Need type annotation for\n\"artifacts\" (hint: \"artifacts: list[<type>] = ...\")  [var-annotated]\n            artifacts = []\n            ^~~~~~~~~\nathalia_core/advanced_modules/user_profiles_advanced.py: note: In member \"__init__\" of class \"ProfilUtilisateur\":\nathalia_core/advanced_modules/user_profiles_advanced.py:19:71: error:\nIncompatible default for argument \"preferences\" (default has type \"None\",\nargument has type \"dict[Any, Any]\")  [assignment]\n    ...f __init__(self, nom: str, email: str = \"\", preferences: Dict = None):\n                                                                       ^~~~\nathalia_core/advanced_modules/user_profiles_advanced.py:19:71: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/advanced_modules/user_profiles_advanced.py:19:71: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/advanced_modules/user_profiles_advanced.py:25:9: error: Need type\nannotation for \"projets_consultes\" (hint:\n\"projets_consultes: list[<type>] = ...\")  [var-annotated]\n            self.projets_consultes = []\n            ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/user_profiles_advanced.py:26:9: error: Need type\nannotation for \"actions_frequentes\" (hint:\n\"actions_frequentes: dict[<type>, <type>] = ...\")  [var-annotated]\n            self.actions_frequentes = {}\n            ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/user_profiles_advanced.py: note: In member \"_init_database\" of class \"GestionnaireProfils\":\nathalia_core/advanced_modules/user_profiles_advanced.py:60:5: error: Function\nis missing a return type annotation  [no-untyped-def]\n        def _init_database(self):\n        ^\nathalia_core/advanced_modules/user_profiles_advanced.py:60:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_modules/user_profiles_advanced.py: note: In member \"creer_profil\" of class \"GestionnaireProfils\":\nathalia_core/advanced_modules/user_profiles_advanced.py:107:33: error:\nIncompatible default for argument \"preferences\" (default has type \"None\",\nargument has type \"dict[Any, Any]\")  [assignment]\n                preferences: Dict = None) -> ProfilUtilisateur:\n                                    ^~~~\nathalia_core/advanced_modules/user_profiles_advanced.py:107:33: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/advanced_modules/user_profiles_advanced.py:107:33: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/advanced_modules/user_profiles_advanced.py: note: In member \"mettre_a_jour_profil\" of class \"GestionnaireProfils\":\nathalia_core/advanced_modules/user_profiles_advanced.py:170:5: error: Function\nis missing a return type annotation  [no-untyped-def]\n        def mettre_a_jour_profil(self, profil: ProfilUtilisateur):\n        ^\nathalia_core/advanced_modules/user_profiles_advanced.py: note: In member \"enregistrer_action\" of class \"GestionnaireProfils\":\nathalia_core/advanced_modules/user_profiles_advanced.py:188:5: error: Function\nis missing a return type annotation  [no-untyped-def]\n        def enregistrer_action(\n        ^\nathalia_core/advanced_modules/user_profiles_advanced.py:192:29: error:\nIncompatible default for argument \"details\" (default has type \"None\", argument\nhas type \"dict[Any, Any]\")  [assignment]\n                details: Dict = None):\n                                ^~~~\nathalia_core/advanced_modules/user_profiles_advanced.py:192:29: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/advanced_modules/user_profiles_advanced.py:192:29: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/advanced_modules/user_profiles_advanced.py: note: In member \"enregistrer_consultation_projet\" of class \"GestionnaireProfils\":\nathalia_core/advanced_modules/user_profiles_advanced.py:212:5: error: Function\nis missing a return type annotation  [no-untyped-def]\n        def enregistrer_consultation_projet(\n        ^\nathalia_core/advanced_modules/user_profiles_advanced.py: note: In function \"main\":\nathalia_core/advanced_modules/user_profiles_advanced.py:383:1: error: Function\nis missing a return type annotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/advanced_modules/user_profiles_advanced.py:383:1: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_modules/user_profiles_advanced.py:415:18: error:\nIncompatible types in assignment (expression has type\n\"ProfilUtilisateur | None\", variable has type \"ProfilUtilisateur\")  [assignment]\n            profil = gestionnaire.obtenir_profil(args.nom)\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/user_profiles_advanced.py:424:9: error:\nIncompatible types in assignment (expression has type \"str\", variable has type\n\"ProfilUtilisateur\")  [assignment]\n            for profil in profils:\n            ^\nathalia_core/advanced_modules/dashboard_unified.py: note: In member \"_init_database\" of class \"DashboardUnifieSimple\":\nathalia_core/advanced_modules/dashboard_unified.py:31:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def _init_database(self):\n        ^\nathalia_core/advanced_modules/dashboard_unified.py:31:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_modules/dashboard_unified.py: note: In member \"enregistrer_metrique\" of class \"DashboardUnifieSimple\":\nathalia_core/advanced_modules/dashboard_unified.py:77:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def enregistrer_metrique(\n        ^\nathalia_core/advanced_modules/dashboard_unified.py: note: In member \"enregistrer_evenement\" of class \"DashboardUnifieSimple\":\nathalia_core/advanced_modules/dashboard_unified.py:96:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def enregistrer_evenement(\n        ^\nathalia_core/advanced_modules/dashboard_unified.py: note: In member \"enregistrer_rapport\" of class \"DashboardUnifieSimple\":\nathalia_core/advanced_modules/dashboard_unified.py:122:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def enregistrer_rapport(self, type_rapport: str, projet: str, cont...\n        ^\nathalia_core/advanced_modules/dashboard_unified.py: note: In member \"ajouter_section_distillation\" of class \"DashboardUnifieSimple\":\nathalia_core/advanced_modules/dashboard_unified.py:282:5: error: Function is\nmissing a type annotation  [no-untyped-def]\n        def ajouter_section_distillation(self, file_handle):\n        ^\nathalia_core/advanced_modules/dashboard_unified.py: note: In member \"generer_dashboard_html\" of class \"DashboardUnifieSimple\":\nathalia_core/advanced_modules/dashboard_unified.py:293:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def generer_dashboard_html(\n        ^\nathalia_core/advanced_modules/dashboard_unified.py: note: In member \"ouvrir_dashboard\" of class \"DashboardUnifieSimple\":\nathalia_core/advanced_modules/dashboard_unified.py:439:5: error: Function is\nmissing a return type annotation  [no-untyped-def]\n        def ouvrir_dashboard(self):\n        ^\nathalia_core/advanced_modules/dashboard_unified.py:439:5: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_modules/dashboard_unified.py: note: In function \"main\":\nathalia_core/advanced_modules/dashboard_unified.py:446:1: error: Function is\nmissing a return type annotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/advanced_modules/dashboard_unified.py:446:1: note: Use \"-> None\" if function does not return a value\nathalia_core/advanced_modules/auto_correction_advanced.py:135: error: Unused\n\"type: ignore\" comment  [unused-ignore]\n            return {}  # type: ignore\n            ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/auto_correction_advanced.py:155: error: Unused\n\"type: ignore\" comment  [unused-ignore]\n            return \"\"  # type: ignore\n            ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/auto_correction_advanced.py:173: error: Unused\n\"type: ignore\" comment  [unused-ignore]\n            return \"\"  # type: ignore\n            ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/auto_correction_advanced.py:188: error: Unused\n\"type: ignore\" comment  [unused-ignore]\n            return \"\"  # type: ignore\n            ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/auto_correction_advanced.py:202: error: Unused\n\"type: ignore\" comment  [unused-ignore]\n            return \"\"  # type: ignore\n            ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/advanced_modules/auto_correction_advanced.py: note: In function \"main\":\nathalia_core/advanced_modules/auto_correction_advanced.py:638:1: error:\nFunction is missing a return type annotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/advanced_modules/auto_correction_advanced.py:638:1: note: Use \"-> None\" if function does not return a value\nathalia_core/performance_analyzer.py: note: In member \"__init__\" of class \"PerformanceAnalyzer\":\nathalia_core/performance_analyzer.py:60:41: error: Incompatible default for\nargument \"root_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n        def __init__(self, root_path: str = None):\n                                            ^~~~\nathalia_core/performance_analyzer.py:60:41: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/performance_analyzer.py:60:41: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/performance_analyzer.py: note: In member \"_init_database\" of class \"PerformanceAnalyzer\":\nathalia_core/performance_analyzer.py:85:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _init_database(self):\n        ^\nathalia_core/performance_analyzer.py:85:5: note: Use \"-> None\" if function does not return a value\nathalia_core/performance_analyzer.py: note: In member \"analyze_project_performance\" of class \"PerformanceAnalyzer\":\nathalia_core/performance_analyzer.py:135:39: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n                self, project_path: str = None) -> PerformanceReport:\n                                          ^~~~\nathalia_core/performance_analyzer.py:135:39: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/performance_analyzer.py:135:39: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/performance_analyzer.py:137:24: error: Incompatible types in\nassignment (expression has type \"Path\", variable has type \"str\")  [assignment]\n            project_path = Path(project_path or self.root_path)\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/performance_analyzer.py:139:13: error: \"str\" has no attribute\n\"name\"  [attr-defined]\n                f\"\u26a1 Analyse des performances du projet: {project_path.name...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/performance_analyzer.py:142:36: error: \"str\" has no attribute\n\"rglob\"  [attr-defined]\n            python_files = [f for f in project_path.rglob(\n                                       ^~~~~~~~~~~~~~~~~~\nathalia_core/performance_analyzer.py: note: In member \"_calculate_overall_score\" of class \"PerformanceAnalyzer\":\nathalia_core/performance_analyzer.py:354:13: error: Incompatible types in\nassignment (expression has type \"float\", variable has type \"int\")  [assignment]\n                total_score += score * weight\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/performance_analyzer.py:355:13: error: Incompatible types in\nassignment (expression has type \"float\", variable has type \"int\")  [assignment]\n                total_weight += weight\n                ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/performance_analyzer.py: note: In member \"_generate_performance_recommendations\" of class \"PerformanceAnalyzer\":\nathalia_core/performance_analyzer.py:384:9: error: Need type annotation for\n\"issue_types\" (hint: \"issue_types: dict[<type>, <type>] = ...\")  [var-annotated]\n            issue_types = {}\n            ^~~~~~~~~~~\nathalia_core/performance_analyzer.py: note: In member \"_save_performance_report\" of class \"PerformanceAnalyzer\":\nathalia_core/performance_analyzer.py:448:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _save_performance_report(self, report: PerformanceReport):\n        ^\nathalia_core/performance_analyzer.py: note: In member \"profile_function\" of class \"PerformanceAnalyzer\":\nathalia_core/performance_analyzer.py:489:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def profile_function(self,\n        ^\nathalia_core/performance_analyzer.py:501:54: error: Argument 1 to\n\"module_from_spec\" has incompatible type \"ModuleSpec | None\"; expected\n\"ModuleSpec\"  [arg-type]\n                module = importlib.util.module_from_spec(spec)\n                                                         ^~~~\nathalia_core/performance_analyzer.py:502:13: error: Item \"None\" of\n\"ModuleSpec | None\" has no attribute \"loader\"  [union-attr]\n                spec.loader.exec_module(module)\n                ^~~~~~~~~~~\nathalia_core/performance_analyzer.py:502:13: error: Item \"None\" of\n\"Loader | Any | None\" has no attribute \"exec_module\"  [union-attr]\n                spec.loader.exec_module(module)\n                ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/performance_analyzer.py:530:20: error: Incompatible return value\ntype (got \"None\", expected \"dict[str, Any]\")  [return-value]\n                return None\n                       ^~~~\nathalia_core/pattern_detector.py: note: In class \"CodePattern\":\nathalia_core/pattern_detector.py:32:37: error: Incompatible types in assignment\n(expression has type \"None\", variable has type \"list[str]\")  [assignment]\n        correction_history: List[str] = None\n                                        ^~~~\nathalia_core/pattern_detector.py: note: In member \"__init__\" of class \"PatternDetector\":\nathalia_core/pattern_detector.py:61:41: error: Incompatible default for\nargument \"root_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n        def __init__(self, root_path: str = None):\n                                            ^~~~\nathalia_core/pattern_detector.py:61:41: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/pattern_detector.py:61:41: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/pattern_detector.py:75:9: error: Need type annotation for\n\"_pattern_cache\" (hint: \"_pattern_cache: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self._pattern_cache = {}\n            ^~~~~~~~~~~~~~~~~~~\nathalia_core/pattern_detector.py:76:9: error: Need type annotation for\n\"_duplicate_cache\" (hint: \"_duplicate_cache: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self._duplicate_cache = {}\n            ^~~~~~~~~~~~~~~~~~~~~\nathalia_core/pattern_detector.py:77:9: error: Need type annotation for\n\"_antipattern_cache\" (hint: \"_antipattern_cache: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self._antipattern_cache = {}\n            ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/pattern_detector.py: note: In member \"_init_database\" of class \"PatternDetector\":\nathalia_core/pattern_detector.py:84:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _init_database(self):\n        ^\nathalia_core/pattern_detector.py:84:5: note: Use \"-> None\" if function does not return a value\nathalia_core/pattern_detector.py: note: In member \"_load_patterns\" of class \"PatternDetector\":\nathalia_core/pattern_detector.py:138:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _load_patterns(self):\n        ^\nathalia_core/pattern_detector.py:138:5: note: Use \"-> None\" if function does not return a value\nathalia_core/pattern_detector.py: note: In member \"analyze_project_patterns\" of class \"PatternDetector\":\nathalia_core/pattern_detector.py:158:39: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n                self, project_path: str = None) -> Dict[str, Any]:\n                                          ^~~~\nathalia_core/pattern_detector.py:158:39: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/pattern_detector.py:158:39: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/pattern_detector.py:160:24: error: Incompatible types in\nassignment (expression has type \"Path\", variable has type \"str\")  [assignment]\n            project_path = Path(project_path or self.root_path)\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/pattern_detector.py:161:21: error: \"str\" has no attribute \"name\" \n[attr-defined]\n            logger.info(f\"\ud83d\udd0d Analyse des patterns du projet: {project_path....\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/pattern_detector.py:164:36: error: \"str\" has no attribute \"rglob\" \n[attr-defined]\n            python_files = [f for f in project_path.rglob(\n                                       ^~~~~~~~~~~~~~~~~~\nathalia_core/pattern_detector.py: note: In member \"_detect_duplicates\" of class \"PatternDetector\":\nathalia_core/pattern_detector.py:284:9: error: Need type annotation for\n\"patterns_by_type\" (hint: \"patterns_by_type: dict[<type>, <type>] = ...\") \n[var-annotated]\n            patterns_by_type = {}\n            ^~~~~~~~~~~~~~~~\nathalia_core/pattern_detector.py: note: In member \"_save_analysis_results\" of class \"PatternDetector\":\nathalia_core/pattern_detector.py:402:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _save_analysis_results(\n        ^\nathalia_core/multi_file_editor.py: note: In member \"backup_file\" of class \"MultiFileEditor\":\nathalia_core/multi_file_editor.py:23:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def backup_file(self, file_path: str):\n        ^\nathalia_core/multi_file_editor.py: note: In member \"rollback\" of class \"MultiFileEditor\":\nathalia_core/multi_file_editor.py:56:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def rollback(self):\n        ^\nathalia_core/multi_file_editor.py: note: In function \"dummy_correction\":\nathalia_core/multi_file_editor.py:70:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def dummy_correction(content):\n        ^\nathalia_core/logger_advanced.py: note: In member \"__init__\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:33:9: error: Need type annotation for \"loggers\"\n(hint: \"loggers: dict[<type>, <type>] = ...\")  [var-annotated]\n            self.loggers = {}\n            ^~~~~~~~~~~~\nathalia_core/logger_advanced.py:34:9: error: Need type annotation for \"metrics\"\n [var-annotated]\n            self.metrics = defaultdict(deque)\n            ^~~~~~~~~~~~\nathalia_core/logger_advanced.py:35:9: error: Need type annotation for\n\"performance_data\" (hint: \"performance_data: dict[<type>, <type>] = ...\") \n[var-annotated]\n            self.performance_data = {}\n            ^~~~~~~~~~~~~~~~~~~~~\nathalia_core/logger_advanced.py: note: In member \"_setup_loggers\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:45:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _setup_loggers(self):\n        ^\nathalia_core/logger_advanced.py:45:5: note: Use \"-> None\" if function does not return a value\nathalia_core/logger_advanced.py: note: In member \"log_main\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:122:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def log_main(self, message: str, level: str = 'INFO', **kwargs):\n        ^\nathalia_core/logger_advanced.py:122:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def log_main(self, message: str, level: str = 'INFO', **kwargs):\n        ^\nathalia_core/logger_advanced.py: note: In member \"log_validation\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:128:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def log_validation(self, test_name: str,\n        ^\nathalia_core/logger_advanced.py: note: In member \"log_correction\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:150:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def log_correction(\n        ^\nathalia_core/logger_advanced.py: note: In member \"log_performance\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:182:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def log_performance(\n        ^\nathalia_core/logger_advanced.py:182:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def log_performance(\n        ^\nathalia_core/logger_advanced.py: note: In member \"log_error\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:211:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def log_error(self, error: Exception, context: str = \"\", **kwargs)...\n        ^\nathalia_core/logger_advanced.py:211:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def log_error(self, error: Exception, context: str = \"\", **kwargs)...\n        ^\nathalia_core/logger_advanced.py: note: In member \"get_error_stats\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:345:9: error: Need type annotation for\n\"error_types\"  [var-annotated]\n            error_types = defaultdict(int)\n            ^~~~~~~~~~~\nathalia_core/logger_advanced.py: note: In member \"_cleanup_worker\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:355:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _cleanup_worker(self):\n        ^\nathalia_core/logger_advanced.py:355:5: note: Use \"-> None\" if function does not return a value\nathalia_core/logger_advanced.py:357:52: error: Cannot determine type of\n\"_cleanup_active\"  [has-type]\n    ...      while hasattr(self, '_cleanup_active') and self._cleanup_active:\n                                                        ^~~~~~~~~~~~~~~~~~~~\nathalia_core/logger_advanced.py: note: In member \"start_cleanup_worker\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:366:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def start_cleanup_worker(self):\n        ^\nathalia_core/logger_advanced.py:366:5: note: Use \"-> None\" if function does not return a value\nathalia_core/logger_advanced.py:368:56: error: Cannot determine type of\n\"_cleanup_active\"  [has-type]\n    ...  if not hasattr(self, '_cleanup_active') or not self._cleanup_active:\n                                                        ^~~~~~~~~~~~~~~~~~~~\nathalia_core/logger_advanced.py: note: In member \"stop_cleanup_worker\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:375:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def stop_cleanup_worker(self):\n        ^\nathalia_core/logger_advanced.py:375:5: note: Use \"-> None\" if function does not return a value\nathalia_core/logger_advanced.py: note: In member \"_cleanup_old_logs\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:381:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _cleanup_old_logs(self):\n        ^\nathalia_core/logger_advanced.py:381:5: note: Use \"-> None\" if function does not return a value\nathalia_core/logger_advanced.py: note: In member \"_compress_old_logs\" of class \"AthaliaLogger\":\nathalia_core/logger_advanced.py:394:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _compress_old_logs(self):\n        ^\nathalia_core/logger_advanced.py:394:5: note: Use \"-> None\" if function does not return a value\nathalia_core/logger_advanced.py: note: In function \"log_main\":\nathalia_core/logger_advanced.py:446:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def log_main(message: str, level: str = 'INFO', **kwargs):\n    ^\nathalia_core/logger_advanced.py:446:1: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n    def log_main(message: str, level: str = 'INFO', **kwargs):\n    ^\nathalia_core/logger_advanced.py: note: In function \"log_validation\":\nathalia_core/logger_advanced.py:451:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def log_validation(test_name: str, result: Dict[str, Any], duration: f...\n    ^\nathalia_core/logger_advanced.py: note: In function \"log_correction\":\nathalia_core/logger_advanced.py:456:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def log_correction(file_path: str, correction_type: str, success: bool...\n    ^\nathalia_core/logger_advanced.py: note: In function \"log_performance\":\nathalia_core/logger_advanced.py:463:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def log_performance(\n    ^\nathalia_core/logger_advanced.py:463:1: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n    def log_performance(\n    ^\nathalia_core/logger_advanced.py: note: In function \"log_error\":\nathalia_core/logger_advanced.py:478:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def log_error(error: Exception, context: str = \"\", **kwargs):\n    ^\nathalia_core/logger_advanced.py:478:1: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n    def log_error(error: Exception, context: str = \"\", **kwargs):\n    ^\nathalia_core/cleanup.py: note: In function \"clean_old_tests_and_caches\":\nathalia_core/cleanup.py:11:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def clean_old_tests_and_caches(outdir):\n    ^\nathalia_core/cleanup.py: note: In function \"clean_macos_files\":\nathalia_core/cleanup.py:65:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def clean_macos_files(directory: str):\n    ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_system_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:62:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_system_files(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_cache_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:104:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_cache_files(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_backup_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:137:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_backup_files(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_temp_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:156:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_temp_files(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_duplicate_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:171:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_duplicate_files(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_empty_directories\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:195:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_empty_directories(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_old_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:204:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_old_files(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_clean_large_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:219:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _clean_large_files(self, project_path: Path):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_safe_remove_file\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:234:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _safe_remove_file(self, file_path: Path, reason: str):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_safe_remove_dir\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:255:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _safe_remove_dir(self, dir_path: Path, reason: str):\n        ^\nathalia_core/auto_cleaner.py: note: In member \"_organize_files\" of class \"AutoCleaner\":\nathalia_core/auto_cleaner.py:394:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _organize_files(self, project_path: Path, optimizations: List[...\n        ^\nathalia_core/auto_cleaner.py: note: In function \"main\":\nathalia_core/auto_cleaner.py:424:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/auto_cleaner.py:424:1: note: Use \"-> None\" if function does not return a value\nathalia_core/audit.py: note: In member \"audit_project\" of class \"ProjectAuditor\":\nathalia_core/audit.py:21:5: error: Function is missing a return type annotation\n [no-untyped-def]\n        def audit_project(self):\n        ^\nathalia_core/audit.py: note: In function \"audit_project_intelligent\":\nathalia_core/audit.py:28:1: error: Function is missing a return type annotation\n [no-untyped-def]\n    def audit_project_intelligent(project_path: str):\n    ^\nathalia_core/audit.py: note: In function \"generate_audit_report\":\nathalia_core/audit.py:57:1: error: Function is missing a return type annotation\n [no-untyped-def]\n    def generate_audit_report(project_path: str):\n    ^\nathalia_core/audit.py: note: In member \"audit_project\" of class \"Audit\":\nathalia_core/audit.py:79:5: error: Function is missing a return type annotation\n [no-untyped-def]\n        def audit_project(self):\n        ^\nathalia_core/distillation/response_distiller.py: note: In function \"consensus_scoring\":\nathalia_core/distillation/response_distiller.py:74:9: error: Function is\nmissing a type annotation  [no-untyped-def]\n            def lcs(a, b):\n            ^\nathalia_core/ros2_validator.py: note: In member \"_check_package_structure\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:76:13: error: \"object\" has no attribute \"append\"\n [attr-defined]\n                self.validation_results[\"errors\"].append(\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py: note: In member \"_validate_package_xml\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:100:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    self.validation_results[\"errors\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:107:25: error: Item \"None\" of\n\"Element | Any | None\" has no attribute \"text\"  [union-attr]\n                    \"name\": root.find(\"name\").text,\n                            ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:108:28: error: Item \"None\" of\n\"Element | Any | None\" has no attribute \"text\"  [union-attr]\n                    \"version\": root.find(\"version\").text,\n                               ^~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:109:32: error: Item \"None\" of\n\"Element | Any | None\" has no attribute \"text\"  [union-attr]\n                    \"description\": root.find(\"description\").text,\n                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:110:31: error: Item \"None\" of\n\"Element | Any | None\" has no attribute \"text\"  [union-attr]\n                    \"maintainer\": root.find(\"maintainer\").text,\n                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:111:28: error: Item \"None\" of\n\"Element | Any | None\" has no attribute \"text\"  [union-attr]\n                    \"license\": root.find(\"license\").text\n                               ^~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:128:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"errors\"].append(f\"Erreur parsing ...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:131:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"errors\"].append(f\"Erreur validati...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py: note: In member \"_validate_setup_py\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:139:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"warnings\"].append(\"setup.py manqu...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:159:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    self.validation_results[\"warnings\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:166:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"errors\"].append(f\"Erreur validati...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py: note: In member \"_validate_cmakelists\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:174:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"warnings\"].append(\"CMakeLists.txt...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:194:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    self.validation_results[\"warnings\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:201:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"errors\"].append(f\"Erreur validati...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py: note: In member \"_check_launch_files\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:204:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _check_launch_files(self):\n        ^\nathalia_core/ros2_validator.py:204:5: note: Use \"-> None\" if function does not return a value\nathalia_core/ros2_validator.py:218:25: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                            self.validation_results[\"launch_files\"].append...\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:220:25: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                            self.validation_results[\"warnings\"].append(\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:225:21: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                        self.validation_results[\"warnings\"].append(\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py: note: In member \"_check_test_files\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:229:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _check_test_files(self):\n        ^\nathalia_core/ros2_validator.py:229:5: note: Use \"-> None\" if function does not return a value\nathalia_core/ros2_validator.py:243:25: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                            self.validation_results[\"test_files\"].append(s...\n                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:246:21: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                        self.validation_results[\"warnings\"].append(\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py: note: In member \"_check_dependencies\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:250:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _check_dependencies(self):\n        ^\nathalia_core/ros2_validator.py:250:5: note: Use \"-> None\" if function does not return a value\nathalia_core/ros2_validator.py:262:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    self.validation_results[\"warnings\"].append(\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:267:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"warnings\"].append(\"Timeout lors d...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:269:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                self.validation_results[\"warnings\"].append(f\"Erreur v\u00e9rifi...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py: note: In member \"generate_validation_report\" of class \"ROS2Validator\":\nathalia_core/ros2_validator.py:283:31: error: \"object\" has no attribute \"items\"\n [attr-defined]\n                for key, value in self.validation_results[\"metadata\"].item...\n                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:289:24: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for dep in self.validation_results[\"dependencies\"]:\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:295:32: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for launch_file in self.validation_results[\"launch_files\"]...\n                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:301:30: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for test_file in self.validation_results[\"test_files\"]:\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:307:26: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for error in self.validation_results[\"errors\"]:\n                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ros2_validator.py:313:28: error: \"object\" has no attribute\n\"__iter__\"; maybe \"__dir__\" or \"__str__\"? (not iterable)  [attr-defined]\n                for warning in self.validation_results[\"warnings\"]:\n                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"_check_project_structure\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:51:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _check_project_structure(self):\n        ^\nathalia_core/robotics_ci.py:51:5: note: Use \"-> None\" if function does not return a value\nathalia_core/robotics_ci.py:72:17: error: Statement is unreachable \n[unreachable]\n                    if not (self.project_path / file).is_dir():\n                    ^\nathalia_core/robotics_ci.py:76:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"errors\"].append(f\"Fichiers requis manquan...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"_run_build\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:79:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_build(self):\n        ^\nathalia_core/robotics_ci.py:79:5: note: Use \"-> None\" if function does not return a value\nathalia_core/robotics_ci.py:95:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"errors\"].append(f\"Build Rust \u00e9cho...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:110:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"errors\"].append(f\"Build ROS2 \u00e9cho...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:125:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"errors\"].append(f\"Build Python \u00e9c...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:129:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"errors\"].append(\"Build timeout\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:132:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"errors\"].append(f\"Erreur build: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"_run_tests\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:134:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_tests(self):\n        ^\nathalia_core/robotics_ci.py:134:5: note: Use \"-> None\" if function does not return a value\nathalia_core/robotics_ci.py:150:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"errors\"].append(f\"Tests Rust \u00e9cho...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:165:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"errors\"].append(f\"Tests ROS2 \u00e9cho...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:180:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"errors\"].append(f\"Tests Python \u00e9c...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:184:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"errors\"].append(\"Tests timeout\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:187:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"errors\"].append(f\"Erreur tests: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"_run_linting\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:189:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_linting(self):\n        ^\nathalia_core/robotics_ci.py:189:5: note: Use \"-> None\" if function does not return a value\nathalia_core/robotics_ci.py:205:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"warnings\"].append(f\"Lint Rust: {r...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:220:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"warnings\"].append(f\"Lint Python: ...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:224:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"warnings\"].append(\"Lint timeout\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:227:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"warnings\"].append(f\"Erreur lint: {e}\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"_run_security_scan\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:229:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_security_scan(self):\n        ^\nathalia_core/robotics_ci.py:229:5: note: Use \"-> None\" if function does not return a value\nathalia_core/robotics_ci.py:245:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"warnings\"].append(f\"Audit Rust: {...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:260:21: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                        self.ci_results[\"warnings\"].append(f\"Scan s\u00e9curit\u00e9...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:264:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"warnings\"].append(\"Scan s\u00e9curit\u00e9 timeout\"...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:267:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"warnings\"].append(f\"Erreur scan s\u00e9curit\u00e9:...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"_run_deployment_check\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:269:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _run_deployment_check(self):\n        ^\nathalia_core/robotics_ci.py:269:5: note: Use \"-> None\" if function does not return a value\nathalia_core/robotics_ci.py:282:17: error: Unsupported target for indexed\nassignment (\"Collection[str]\")  [index]\n                    self.ci_results[\"metrics\"][\"config_files\"] = found_con...\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:285:17: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                    self.ci_results[\"warnings\"].append(\"Aucun fichier de d...\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:289:13: error: \"Collection[str]\" has no attribute\n\"append\"  [attr-defined]\n                self.ci_results[\"errors\"].append(f\"Erreur v\u00e9rification d\u00e9p...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"_calculate_ci_score\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:291:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _calculate_ci_score(self):\n        ^\nathalia_core/robotics_ci.py:291:5: note: Use \"-> None\" if function does not return a value\nathalia_core/robotics_ci.py:306:22: error: No overload variant of \"get\" of\n\"dict\" matches argument types \"Collection[str]\", \"int\"  [call-overload]\n                score -= status_penalties.get(status, 0)\n                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:306:22: note: Possible overload variants:\nathalia_core/robotics_ci.py:306:22: note:     def get(self, str, /) -> int | None\nathalia_core/robotics_ci.py:306:22: note:     def get(self, str, int, /) -> int\nathalia_core/robotics_ci.py:306:22: note:     def [_T] get(self, str, _T, /) -> int | _T\nathalia_core/robotics_ci.py:312:9: error: Unsupported target for indexed\nassignment (\"Collection[str]\")  [index]\n            self.ci_results[\"metrics\"][\"ci_score\"] = max(0, score)\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py: note: In member \"generate_ci_report\" of class \"RoboticsCI\":\nathalia_core/robotics_ci.py:328:42: error: \"Collection[str]\" has no attribute\n\"get\"  [attr-defined]\n            report.append(f\"## Score CI/CD: {self.ci_results['metrics'].ge...\n                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/robotics_ci.py:345:31: error: \"Collection[str]\" has no attribute\n\"items\"  [attr-defined]\n                for key, value in self.ci_results[\"metrics\"].items():\n                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/plugins_validator.py: note: In member \"__init__\" of class \"PluginValidator\":\nathalia_core/plugins_validator.py:23:35: error: Need type annotation for\n\"validation_results\"  [var-annotated]\n            self.validation_results = {\n                                      ^\nathalia_core/plugins_validator.py: note: In member \"_check_metadata\" of class \"PluginValidator\":\nathalia_core/plugins_validator.py:94:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _check_metadata(self, plugin_path: Path, results: Dict[str, An...\n        ^\nathalia_core/plugins_validator.py: note: In member \"_check_dependencies\" of class \"PluginValidator\":\nathalia_core/plugins_validator.py:121:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _check_dependencies(self, plugin_path: Path, results: Dict[str...\n        ^\nathalia_core/main.py:21:22: error: Incompatible types in assignment (expression\nhas type \"None\", variable has type \"AthaliaLogger\")  [assignment]\n        athalia_logger = None\n                         ^~~~\nathalia_core/main.py: note: In function \"log_main\":\nathalia_core/main.py:23:5: error: Function is missing a type annotation \n[no-untyped-def]\n        def log_main(msg, level='INFO', **kwargs):\n        ^\nathalia_core/main.py:23:5: error: All conditional function variants must have\nidentical signatures  [misc]\n        def log_main(msg, level='INFO', **kwargs):\n        ^\nathalia_core/main.py:23:5: note: Original:\nathalia_core/main.py:23:5: note:     def log_main(message: str, level: str = ..., **kwargs: Any) -> Any\nathalia_core/main.py:23:5: note: Redefinition:\nathalia_core/main.py:23:5: note:     def log_main(msg: Any, level: Any = ..., **kwargs: Any) -> Any\nathalia_core/main.py: note: In function \"signal_handler\":\nathalia_core/main.py:37:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def signal_handler(signum, frame):\n    ^\nathalia_core/main.py: note: In function \"menu\":\nathalia_core/main.py:44:1: error: Function is missing a return type annotation \n[no-untyped-def]\n    def menu():\n    ^\nathalia_core/main.py: note: In function \"surveillance_mode\":\nathalia_core/main.py:76:1: error: Function is missing a return type annotation \n[no-untyped-def]\n    def surveillance_mode():\n    ^\nathalia_core/main.py:76:1: note: Use \"-> None\" if function does not return a value\nathalia_core/main.py: note: In function \"main\":\nathalia_core/main.py:87:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def main(test_mode=False):\n    ^\nathalia_core/generation_simple.py: note: In function \"generate_blueprint_mock\":\nathalia_core/generation_simple.py:13:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def generate_blueprint_mock(idea: str = \"\", *args, **kwargs):\n    ^\nathalia_core/generation_simple.py:13:1: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n    def generate_blueprint_mock(idea: str = \"\", *args, **kwargs):\n    ^\nathalia_core/generation_simple.py: note: In function \"generate_project\":\nathalia_core/generation_simple.py:56:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def generate_project(blueprint: dict, outdir, *args, **kwargs):\n    ^\nathalia_core/generation_simple.py:56:1: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n    def generate_project(blueprint: dict, outdir, *args, **kwargs):\n    ^\nathalia_core/generation_simple.py: note: In function \"generate_readme\":\nathalia_core/generation_simple.py:75:59: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"Path\") \n[assignment]\n    ...ef generate_readme(blueprint: dict, project_path: Path = None) -> str:\n                                                                ^~~~\nathalia_core/generation_simple.py:75:59: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/generation_simple.py:75:59: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/generation_simple.py: note: In function \"generate_main_code\":\nathalia_core/generation_simple.py:113:62: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"Path\") \n[assignment]\n    ...generate_main_code(blueprint: dict, project_path: Path = None) -> str:\n                                                                ^~~~\nathalia_core/generation_simple.py:113:62: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/generation_simple.py:113:62: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/generation_simple.py: note: In function \"generate_test_code\":\nathalia_core/generation_simple.py:182:62: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"Path\") \n[assignment]\n    ...generate_test_code(blueprint: dict, project_path: Path = None) -> str:\n                                                                ^~~~\nathalia_core/generation_simple.py:182:62: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/generation_simple.py:182:62: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/generation_simple.py: note: In function \"save_blueprint\":\nathalia_core/generation_simple.py:237:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def save_blueprint(blueprint: dict, outdir):\n    ^\nathalia_core/generation_simple.py:237:1: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n    def save_blueprint(blueprint: dict, outdir):\n    ^\nathalia_core/generation_simple.py: note: In function \"inject_booster_ia_elements\":\nathalia_core/generation_simple.py:252:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def inject_booster_ia_elements(outdir):\n    ^\nathalia_core/generation_simple.py: note: In function \"scan_existing_project\":\nathalia_core/generation_simple.py:265:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def scan_existing_project(outdir):\n    ^\nathalia_core/generation_simple.py: note: In function \"merge_or_suffix_file\":\nathalia_core/generation_simple.py:283:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def merge_or_suffix_file(\n    ^\nathalia_core/generation_simple.py: note: In function \"backup_file\":\nathalia_core/generation_simple.py:320:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def backup_file(file_path: str):\n    ^\nathalia_core/generation.py: note: In function \"generate_blueprint_mock\":\nathalia_core/generation.py:15:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def generate_blueprint_mock(idea: str = \"\", *args, **kwargs):\n    ^\nathalia_core/generation.py:15:1: error: Function is missing a type annotation\nfor one or more arguments  [no-untyped-def]\n    def generate_blueprint_mock(idea: str = \"\", *args, **kwargs):\n    ^\nathalia_core/generation.py: note: In function \"generate_project\":\nathalia_core/generation.py:58:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def generate_project(blueprint: dict, outdir, *args, **kwargs):\n    ^\nathalia_core/generation.py:58:1: error: Function is missing a type annotation\nfor one or more arguments  [no-untyped-def]\n    def generate_project(blueprint: dict, outdir, *args, **kwargs):\n    ^\nathalia_core/generation.py: note: In function \"save_blueprint\":\nathalia_core/generation.py:302:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def save_blueprint(blueprint: dict, outdir):\n    ^\nathalia_core/generation.py:302:1: error: Function is missing a type annotation\nfor one or more arguments  [no-untyped-def]\n    def save_blueprint(blueprint: dict, outdir):\n    ^\nathalia_core/generation.py: note: In function \"inject_booster_ia_elements\":\nathalia_core/generation.py:317:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def inject_booster_ia_elements(outdir):\n    ^\nathalia_core/generation.py: note: In function \"scan_existing_project\":\nathalia_core/generation.py:330:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def scan_existing_project(outdir):\n    ^\nathalia_core/generation.py: note: In function \"merge_or_suffix_file\":\nathalia_core/generation.py:348:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def merge_or_suffix_file(\n    ^\nathalia_core/generation.py: note: In function \"backup_file\":\nathalia_core/generation.py:384:1: error: Function is missing a return type\nannotation  [no-untyped-def]\n    def backup_file(file_path: str):\n    ^\nathalia_core/correction_optimizer.py: note: In member \"__init__\" of class \"CorrectionOptimizer\":\nathalia_core/correction_optimizer.py:32:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/correction_optimizer.py:32:5: note: Use \"-> None\" if function does not return a value\nathalia_core/correction_optimizer.py: note: In member \"_apply_ast_corrections\" of class \"CorrectionOptimizer\":\nathalia_core/correction_optimizer.py:261:9: error: Statement is unreachable \n[unreachable]\n            return content, corrections\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/correction_optimizer.py: note: In member \"_learn_from_correction\" of class \"CorrectionOptimizer\":\nathalia_core/correction_optimizer.py:587:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _learn_from_correction(\n        ^\nathalia_core/config_manager.py: note: In member \"_setup_logging\" of class \"ConfigManager\":\nathalia_core/config_manager.py:295:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def _setup_logging(self):\n        ^\nathalia_core/config_manager.py:295:5: note: Use \"-> None\" if function does not return a value\nathalia_core/config_manager.py: note: In member \"get_enabled_plugins\" of class \"ConfigManager\":\nathalia_core/config_manager.py:347:9: error: Returning Any from function\ndeclared to return \"list[str]\"  [no-any-return]\n            return self.config.plugins.get('enabled', [])\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/config_manager.py: note: In member \"get_available_templates\" of class \"ConfigManager\":\nathalia_core/config_manager.py:360:9: error: Returning Any from function\ndeclared to return \"list[str]\"  [no-any-return]\n            return self.config.templates.get(\n            ^\nathalia_core/config_manager.py: note: In member \"validate_config\" of class \"ConfigManager\":\nathalia_core/config_manager.py:391:17: error: Statement is unreachable \n[unreachable]\n                    return False\n                    ^~~~~~~~~~~~\nathalia_core/config_manager.py: note: In member \"resolve_environment_variables\" of class \"ConfigManager\":\nathalia_core/config_manager.py:442:33: error: Incompatible types in assignment\n(expression has type \"dict[str, Any]\", target has type \"str\")  [assignment]\n                    resolved[key] = self.resolve_environment_variables(val...\n                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/architecture_analyzer.py: note: In member \"__init__\" of class \"ArchitectureAnalyzer\":\nathalia_core/architecture_analyzer.py:63:41: error: Incompatible default for\nargument \"root_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n        def __init__(self, root_path: str = None):\n                                            ^~~~\nathalia_core/architecture_analyzer.py:63:41: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/architecture_analyzer.py:63:41: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/architecture_analyzer.py: note: In member \"_init_database\" of class \"ArchitectureAnalyzer\":\nathalia_core/architecture_analyzer.py:83:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _init_database(self):\n        ^\nathalia_core/architecture_analyzer.py:83:5: note: Use \"-> None\" if function does not return a value\nathalia_core/architecture_analyzer.py: note: In member \"_load_config\" of class \"ArchitectureAnalyzer\":\nathalia_core/architecture_analyzer.py:140:17: error: Returning Any from\nfunction declared to return \"dict[str, Any]\"  [no-any-return]\n                    return yaml.safe_load(f)\n                    ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/architecture_analyzer.py: note: In member \"_save_architecture_analysis\" of class \"ArchitectureAnalyzer\":\nathalia_core/architecture_analyzer.py:414:5: error: Function is missing a\nreturn type annotation  [no-untyped-def]\n        def _save_architecture_analysis(self, architecture: ArchitectureMa...\n        ^\nathalia_core/architecture_analyzer.py: note: In member \"generate_intelligent_coordination\" of class \"ArchitectureAnalyzer\":\nathalia_core/architecture_analyzer.py:502:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                coordination_plan[\"priority_tasks\"].append({\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/architecture_analyzer.py:508:13: error: Unsupported operand types\nfor + (\"object\" and \"int\")  [operator]\n                coordination_plan[\"estimated_time\"] += 4  # heures\n                ^\nathalia_core/architecture_analyzer.py:511:13: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                coordination_plan[\"priority_tasks\"].append({\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/architecture_analyzer.py:517:13: error: Unsupported operand types\nfor + (\"object\" and \"int\")  [operator]\n                coordination_plan[\"estimated_time\"] += 2  # heures\n                ^\nathalia_core/robotics/reachy_auditor.py: note: In member \"_audit_ros2\" of class \"ReachyAuditor\":\nathalia_core/robotics/reachy_auditor.py:99:9: error: Need type annotation for\n\"recommendations\" (hint: \"recommendations: list[<type>] = ...\")  [var-annotated]\n            recommendations = []\n            ^~~~~~~~~~~~~~~\nathalia_core/robotics/docker_robotics.py: note: In member \"validate_docker_setup\" of class \"DockerRoboticsManager\":\nathalia_core/robotics/docker_robotics.py:57:9: error: Need type annotation for\n\"issues\" (hint: \"issues: list[<type>] = ...\")  [var-annotated]\n            issues = []\n            ^~~~~~\nathalia_core/robotics/docker_robotics.py:58:9: error: Need type annotation for\n\"recommendations\" (hint: \"recommendations: list[<type>] = ...\")  [var-annotated]\n            recommendations = []\n            ^~~~~~~~~~~~~~~\nathalia_core/robotics/docker_robotics.py: note: In member \"_validate_reachy_service\" of class \"DockerRoboticsManager\":\nathalia_core/robotics/docker_robotics.py:136:5: error: Function is missing a\nreturn type annotation  [no-untyped-def]\n        def _validate_reachy_service(\n        ^\nathalia_core/robotics/docker_robotics.py:151:23: error: Subclass of\n\"dict[str, str]\" and \"list[Any]\" cannot exist: would have incompatible method\nsignatures  [unreachable]\n            if isinstance(service.environment, list):\n                          ^~~~~~~~~~~~~~~~~~~\nathalia_core/robotics/docker_robotics.py:152:13: error: Statement is\nunreachable  [unreachable]\n                env_vars = [str(v) for v in service.environment]\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/agents/context_prompt.py:13:1: error: Cannot find implementation\nor library stub for module named \"pyperclip\"  [import-not-found]\n        import pyperclip\n    ^\nathalia_core/agents/context_prompt.py:13:1: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports\nathalia_core/agents/context_prompt.py: note: In function \"score_prompt\":\nathalia_core/agents/context_prompt.py:94:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def score_prompt(prompt, filename, content):\n    ^\nathalia_core/agents/context_prompt.py: note: In function \"detect_prompts_scoring\":\nathalia_core/agents/context_prompt.py:108:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def detect_prompts_scoring(filepath):\n    ^\nathalia_core/agents/context_prompt.py: note: In function \"detect_prompt_semantic\":\nathalia_core/agents/context_prompt.py:124:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def detect_prompt_semantic(filepath):\n    ^\nathalia_core/agents/context_prompt.py:151:16: error: \"object\" has no attribute\n\"lower\"  [attr-defined]\n                if p['name'].lower() in answer.lower():\n                   ^~~~~~~~~~~~~~~\nathalia_core/agents/context_prompt.py: note: In function \"show_prompts\":\nathalia_core/agents/context_prompt.py:158:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def show_prompts(scored, semantic_prompt=None):\n    ^\nathalia_core/agents/context_prompt.py:181:9: error: Missing positional argument\n\"msg\" in call to \"info\"  [call-arg]\n            logging.info()\n            ^~~~~~~~~~~~~~\nathalia_core/agents/context_prompt.py:209:9: error: Missing positional argument\n\"msg\" in call to \"info\"  [call-arg]\n            logging.info()\n            ^~~~~~~~~~~~~~\nathalia_core/agents/context_prompt.py: note: In function \"main\":\nathalia_core/agents/context_prompt.py:213:1: error: Function is missing a\nreturn type annotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/agents/context_prompt.py:213:1: note: Use \"-> None\" if function does not return a value\nathalia_core/ai_robust.py: note: In member \"__init__\" of class \"RobustAI\":\nathalia_core/ai_robust.py:39:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/ai_robust.py:39:5: note: Use \"-> None\" if function does not return a value\nathalia_core/ai_robust.py: note: In member \"generate_blueprint\" of class \"RobustAI\":\nathalia_core/ai_robust.py:45:5: error: Function is missing a type annotation\nfor one or more arguments  [no-untyped-def]\n        def generate_blueprint(self, idea: str, **kwargs) -> dict:\n        ^\nathalia_core/ai_robust.py: note: In member \"get_dynamic_prompt\" of class \"RobustAI\":\nathalia_core/ai_robust.py:170:5: error: Function is missing a type annotation\nfor one or more arguments  [no-untyped-def]\n        def get_dynamic_prompt(self, context: str, **kwargs) -> str:\n        ^\nathalia_core/ai_robust.py: note: In member \"_get_dynamic_prompt\" of class \"RobustAI\":\nathalia_core/ai_robust.py:176:5: error: Function is missing a type annotation\nfor one or more arguments  [no-untyped-def]\n        def _get_dynamic_prompt(self, context, **kwargs) -> str:\n        ^\nathalia_core/ai_robust.py: note: In member \"generate_bluelogger\" of class \"RobustAI\":\nathalia_core/ai_robust.py:194:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def generate_bluelogger(self):\n        ^\nathalia_core/ai_robust.py: note: In function \"generate_bluelogger\":\nathalia_core/ai_robust.py:198:13: error: Function is missing a type annotation \n[no-untyped-def]\n                def __init__(self, parent):\n                ^\nathalia_core/ai_robust.py:201:13: error: Function is missing a type annotation \n[no-untyped-def]\n                def info(self, *args, **kwargs):\n                ^\nathalia_core/ai_robust.py: note: In member \"generate_response\" of class \"RobustAI\":\nathalia_core/ai_robust.py:277:5: error: Function is missing a type annotation\nfor one or more arguments  [no-untyped-def]\n        def generate_response(\n        ^\nathalia_core/ai_robust.py: note: In function \"query_qwen\":\nathalia_core/ai_robust.py:397:13: error: Returning Any from function declared\nto return \"str\"  [no-any-return]\n                return response.json().get(\"response\", \"\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ai_robust.py: note: In function \"query_mistral\":\nathalia_core/ai_robust.py:415:13: error: Returning Any from function declared\nto return \"str\"  [no-any-return]\n                return response.json().get(\"response\", \"\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"initialize_modules\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:52:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def initialize_modules(self):\n        ^\nathalia_core/unified_orchestrator.py:52:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:55:30: error: Incompatible types in\nassignment (expression has type \"RobustAI\", variable has type \"None\") \n[assignment]\n                self.robust_ai = RobustAI()\n                                 ^~~~~~~~~~\nathalia_core/unified_orchestrator.py:56:37: error: Incompatible types in\nassignment (expression has type \"SecurityAuditor\", variable has type \"None\") \n[assignment]\n                self.security_auditor = SecurityAuditor(str(self.project_p...\n                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/unified_orchestrator.py:57:32: error: Incompatible types in\nassignment (expression has type \"CodeLinter\", variable has type \"None\") \n[assignment]\n                self.code_linter = CodeLinter(str(self.project_path))\n                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:58:41: error: Incompatible types in\nassignment (expression has type \"CorrectionOptimizer\", variable has type \"None\")\n [assignment]\n                self.correction_optimizer = CorrectionOptimizer()\n                                            ^~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:59:32: error: Incompatible types in\nassignment (expression has type \"AutoTester\", variable has type \"None\") \n[assignment]\n                self.auto_tester = AutoTester(str(self.project_path))\n                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:60:36: error: Incompatible types in\nassignment (expression has type \"AutoDocumenter\", variable has type \"None\") \n[assignment]\n                self.auto_documenter = AutoDocumenter(str(self.project_pat...\n                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:61:33: error: Incompatible types in\nassignment (expression has type \"AutoCleaner\", variable has type \"None\") \n[assignment]\n                self.auto_cleaner = AutoCleaner(str(self.project_path))\n                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:62:30: error: Too many arguments for\n\"AutoCICD\"  [call-arg]\n                self.auto_cicd = AutoCICD(str(self.project_path))\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:62:30: error: Incompatible types in\nassignment (expression has type \"AutoCICD\", variable has type \"None\") \n[assignment]\n                self.auto_cicd = AutoCICD(str(self.project_path))\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:68:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"errors\"].append(f\"Erreur initialisa...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"run_full_workflow\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:106:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"errors\"].append(f\"Erreur workflow: ...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_generate_project\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:111:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_generate_project(self, blueprint: Dict[str, Any]):\n        ^\nathalia_core/unified_orchestrator.py:117:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"steps_completed\"].append(\"project_g...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:118:13: error: Unsupported target for\nindexed assignment (\"Collection[str]\")  [index]\n                self.workflow_results[\"artifacts\"][\"project_path\"] = proje...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:122:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"errors\"].append(f\"Erreur g\u00e9n\u00e9ration...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_security_audit\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:125:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_security_audit(self):\n        ^\nathalia_core/unified_orchestrator.py:125:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:131:17: error: Statement is unreachable \n[unreachable]\n                    security_results = self.security_auditor.run()\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:137:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"warnings\"].append(f\"Erreur audit s\u00e9...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_code_linting\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:139:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_code_linting(self):\n        ^\nathalia_core/unified_orchestrator.py:139:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:145:17: error: Statement is unreachable \n[unreachable]\n                    lint_results = self.code_linter.run()\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:151:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"warnings\"].append(f\"Erreur linting:...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_correction_optimization\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:153:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_correction_optimization(self):\n        ^\nathalia_core/unified_orchestrator.py:153:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:160:17: error: Statement is unreachable \n[unreachable]\n                    optimization_results = self.correction_optimizer.get_c...\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/unified_orchestrator.py:166:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"warnings\"].append(f\"Erreur optimisa...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_auto_testing\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:168:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_auto_testing(self):\n        ^\nathalia_core/unified_orchestrator.py:168:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:174:17: error: Statement is unreachable \n[unreachable]\n                    test_results = self.auto_tester.run_tests()\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:180:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"warnings\"].append(f\"Erreur tests au...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_auto_documentation\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:182:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_auto_documentation(self):\n        ^\nathalia_core/unified_orchestrator.py:182:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:188:17: error: Statement is unreachable \n[unreachable]\n                    doc_results = self.auto_documenter.generate_documentat...\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/unified_orchestrator.py:194:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"warnings\"].append(f\"Erreur document...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_auto_cleaning\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:196:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_auto_cleaning(self):\n        ^\nathalia_core/unified_orchestrator.py:196:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:202:17: error: Statement is unreachable \n[unreachable]\n                    clean_results = self.auto_cleaner.clean_project()\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:208:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"warnings\"].append(f\"Erreur nettoyag...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"_step_auto_cicd\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:210:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _step_auto_cicd(self):\n        ^\nathalia_core/unified_orchestrator.py:210:5: note: Use \"-> None\" if function does not return a value\nathalia_core/unified_orchestrator.py:216:17: error: Statement is unreachable \n[unreachable]\n                    cicd_results = self.auto_cicd.setup_cicd()\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:222:13: error: \"Collection[str]\" has no\nattribute \"append\"  [attr-defined]\n                self.workflow_results[\"warnings\"].append(f\"Erreur CI/CD: {...\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"generate_workflow_report\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:230:37: error: \"Collection[str]\" has no\nattribute \"upper\"  [attr-defined]\n            report.append(f\"## Statut: {self.workflow_results['status'].up...\n                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py:241:36: error: \"Collection[str]\" has no\nattribute \"items\"  [attr-defined]\n                for artifact, value in self.workflow_results[\"artifacts\"]....\n                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/unified_orchestrator.py:259:34: error: \"Collection[str]\" has no\nattribute \"items\"  [attr-defined]\n                for metric, value in self.workflow_results[\"metrics\"].item...\n                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/unified_orchestrator.py: note: In member \"save_workflow_results\" of class \"UnifiedOrchestrator\":\nathalia_core/unified_orchestrator.py:264:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def save_workflow_results(self, output_path: str = \"workflow_resul...\n        ^\nathalia_core/cli.py: note: In function \"cli\":\nathalia_core/cli.py:23:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def cli(verbose):\n    ^\nathalia_core/cli.py: note: In function \"generate\":\nathalia_core/cli.py:38:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def generate(idea, output, dry_run):\n    ^\nathalia_core/cli.py: note: In function \"audit\":\nathalia_core/cli.py:78:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def audit(project_path):\n    ^\nathalia_core/cli.py: note: In function \"ai_status\":\nathalia_core/cli.py:102:1: error: Function is missing a return type annotation \n[no-untyped-def]\n    def ai_status():\n    ^\nathalia_core/cli.py:102:1: note: Use \"-> None\" if function does not return a value\nathalia_core/cli.py: note: In function \"test_ai\":\nathalia_core/cli.py:136:1: error: Function is missing a type annotation \n[no-untyped-def]\n    def test_ai(idea):\n    ^\nathalia_core/agents/unified_agent.py: note: In member \"__init__\" of class \"UnifiedAgent\":\nathalia_core/agents/unified_agent.py:14:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def __init__(self, agent_type=\"general\"):\n        ^\nathalia_core/agents/unified_agent.py: note: In member \"act\" of class \"UnifiedAgent\":\nathalia_core/agents/unified_agent.py:19:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def act(self, prompt, responses=None):\n        ^\nathalia_core/agents/unified_agent.py: note: In member \"_process_prompt\" of class \"UnifiedAgent\":\nathalia_core/agents/unified_agent.py:26:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def _process_prompt(self, prompt):\n        ^\nathalia_core/agents/unified_agent.py: note: In member \"_synthesize_responses\" of class \"UnifiedAgent\":\nathalia_core/agents/unified_agent.py:33:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def _synthesize_responses(self, prompt, responses):\n        ^\nathalia_core/agents/unified_agent.py: note: In member \"__init__\" of class \"AuditAgent\":\nathalia_core/agents/unified_agent.py:43:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/agents/unified_agent.py:43:5: note: Use \"-> None\" if function does not return a value\nathalia_core/agents/unified_agent.py: note: In member \"__init__\" of class \"CorrectionAgent\":\nathalia_core/agents/unified_agent.py:50:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/agents/unified_agent.py:50:5: note: Use \"-> None\" if function does not return a value\nathalia_core/agents/unified_agent.py: note: In member \"__init__\" of class \"SynthesisAgent\":\nathalia_core/agents/unified_agent.py:57:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/agents/unified_agent.py:57:5: note: Use \"-> None\" if function does not return a value\nathalia_core/agents/unified_agent.py: note: In member \"__init__\" of class \"QwenAgent\":\nathalia_core/agents/unified_agent.py:64:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/agents/unified_agent.py:64:5: note: Use \"-> None\" if function does not return a value\nathalia_core/agents/audit_agent.py: note: In member \"__init__\" of class \"AuditAgent\":\nathalia_core/agents/audit_agent.py:7:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/agents/audit_agent.py:7:5: note: Use \"-> None\" if function does not return a value\nathalia_core/agents/audit_agent.py: note: In member \"act\" of class \"AuditAgent\":\nathalia_core/agents/audit_agent.py:10:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def act(self, prompt):\n        ^\nathalia_core/intelligent_analyzer.py: note: In member \"__init__\" of class \"IntelligentAnalyzer\":\nathalia_core/intelligent_analyzer.py:51:41: error: Incompatible default for\nargument \"root_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n        def __init__(self, root_path: str = None):\n                                            ^~~~\nathalia_core/intelligent_analyzer.py:51:41: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_analyzer.py:51:41: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_analyzer.py:56:49: error: Argument 1 to\n\"PatternDetector\" has incompatible type \"Path\"; expected \"str\"  [arg-type]\n            self.pattern_detector = PatternDetector(self.root_path)\n                                                    ^~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:57:59: error: Argument 1 to\n\"ArchitectureAnalyzer\" has incompatible type \"Path\"; expected \"str\"  [arg-type]\n    ...     self.architecture_analyzer = ArchitectureAnalyzer(self.root_path)\n                                                              ^~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:58:57: error: Argument 1 to\n\"PerformanceAnalyzer\" has incompatible type \"Path\"; expected \"str\"  [arg-type]\n    ...       self.performance_analyzer = PerformanceAnalyzer(self.root_path)\n                                                              ^~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py: note: In member \"analyze_project_comprehensive\" of class \"IntelligentAnalyzer\":\nathalia_core/intelligent_analyzer.py:63:39: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n                self, project_path: str = None) -> ComprehensiveAnalysis:\n                                          ^~~~\nathalia_core/intelligent_analyzer.py:63:39: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_analyzer.py:63:39: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_analyzer.py:65:24: error: Incompatible types in\nassignment (expression has type \"Path\", variable has type \"str\")  [assignment]\n            project_path = Path(project_path or self.root_path)\n                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:66:24: error: \"str\" has no attribute\n\"name\"  [attr-defined]\n            project_name = project_path.name\n                           ^~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:72:51: error: Argument 1 to\n\"_perform_ast_analysis\" of \"IntelligentAnalyzer\" has incompatible type \"str\";\nexpected \"Path\"  [arg-type]\n            ast_analysis = self._perform_ast_analysis(project_path)\n                                                      ^~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:108:35: error: Argument\n\"architecture_analysis\" to \"ComprehensiveAnalysis\" has incompatible type\n\"ArchitectureMapping\"; expected \"dict[str, Any]\"  [arg-type]\n                architecture_analysis=architecture_analysis,\n                                      ^~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:109:34: error: Argument\n\"performance_analysis\" to \"ComprehensiveAnalysis\" has incompatible type\n\"PerformanceReport\"; expected \"dict[str, Any]\"  [arg-type]\n                performance_analysis=performance_analysis,\n                                     ^~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py: note: In member \"_perform_ast_analysis\" of class \"IntelligentAnalyzer\":\nathalia_core/intelligent_analyzer.py:151:25: error: Generator has incompatible\nitem type \"object\"; expected \"bool\"  [misc]\n                        sum(f[\"complexity_score\"] for f in file_analyses) ...\n                            ^~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py: note: In member \"_create_optimization_plan\" of class \"IntelligentAnalyzer\":\nathalia_core/intelligent_analyzer.py:288:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    plan[\"priority_tasks\"].append({\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:295:17: error: Unsupported operand types\nfor + (\"object\" and \"int\")  [operator]\n                    plan[\"estimated_effort\"] += len(high_severity_duplicat...\n                    ^\nathalia_core/intelligent_analyzer.py:303:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    plan[\"priority_tasks\"].append({\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:311:17: error: Unsupported operand types\nfor + (\"object\" and \"int\")  [operator]\n                    plan[\"estimated_effort\"] += len(critical_perf_issues) ...\n                    ^\nathalia_core/intelligent_analyzer.py:320:17: error: \"object\" has no attribute\n\"append\"  [attr-defined]\n                    plan[\"medium_priority_tasks\"].append({\n                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:327:17: error: Unsupported operand types\nfor + (\"object\" and \"float\")  [operator]\n                    plan[\"estimated_effort\"] += len(\n                    ^\nathalia_core/intelligent_analyzer.py: note: In member \"_save_comprehensive_analysis\" of class \"IntelligentAnalyzer\":\nathalia_core/intelligent_analyzer.py:340:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _save_comprehensive_analysis(self, analysis: ComprehensiveAnal...\n        ^\nathalia_core/intelligent_analyzer.py:363:21: error: \"dict[str, Any]\" has no\nattribute \"modules\"  [attr-defined]\n                        analysis.architecture_analysis.modules),\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:365:21: error: \"dict[str, Any]\" has no\nattribute \"performance_issues\"  [attr-defined]\n                        analysis.architecture_analysis.performance_issues)...\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:367:34: error: \"dict[str, Any]\" has no\nattribute \"overall_score\"  [attr-defined]\n                    \"overall_score\": analysis.performance_analysis.overall...\n                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/intelligent_analyzer.py:369:21: error: \"dict[str, Any]\" has no\nattribute \"issues\"  [attr-defined]\n                        analysis.performance_analysis.issues)}}\n                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py: note: In member \"orchestrate_with_unified\" of class \"IntelligentAnalyzer\":\nathalia_core/intelligent_analyzer.py:404:35: error: Incompatible default for\nargument \"project_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n            self, project_path: str = None,\n                                      ^~~~\nathalia_core/intelligent_analyzer.py:404:35: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/intelligent_analyzer.py:404:35: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/intelligent_analyzer.py:412:20: error: Incompatible return value\ntype (got \"ComprehensiveAnalysis\", expected \"dict[str, Any]\")  [return-value]\n                return self.analyze_project_comprehensive(project_path)\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:415:52: error: Argument 1 to\n\"UnifiedOrchestrator\" has incompatible type \"Path\"; expected \"str\"  [arg-type]\n            unified_orchestrator = UnifiedOrchestrator(self.root_path)\n                                                       ^~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py:416:9: error: Returning Any from function\ndeclared to return \"dict[str, Any]\"  [no-any-return]\n            return unified_orchestrator.orchestrate_project_complete(\n            ^\nathalia_core/intelligent_analyzer.py:416:16: error: \"UnifiedOrchestrator\" has\nno attribute \"orchestrate_project_complete\"  [attr-defined]\n            return unified_orchestrator.orchestrate_project_complete(\n                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/intelligent_analyzer.py: note: In function \"main\":\nathalia_core/intelligent_analyzer.py:420:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def main():\n    ^\nathalia_core/intelligent_analyzer.py:420:1: note: Use \"-> None\" if function does not return a value\nathalia_core/athalia_orchestrator.py: note: In function \"get_backup_system\":\nathalia_core/athalia_orchestrator.py:112:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def get_backup_system():\n    ^\nathalia_core/athalia_orchestrator.py: note: In function \"standardize_cli_script\":\nathalia_core/athalia_orchestrator.py:117:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def standardize_cli_script():\n    ^\nathalia_core/athalia_orchestrator.py: note: In member \"__init__\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:166:41: error: Incompatible default for\nargument \"root_path\" (default has type \"None\", argument has type \"str\") \n[assignment]\n        def __init__(self, root_path: str = None):\n                                            ^~~~\nathalia_core/athalia_orchestrator.py:166:41: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/athalia_orchestrator.py:166:41: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/athalia_orchestrator.py:197:41: error: Incompatible types in\nassignment (expression has type \"None\", variable has type \"IntelligentAnalyzer\")\n [assignment]\n                self.intelligent_analyzer = None\n                                            ^~~~\nathalia_core/athalia_orchestrator.py: note: In member \"_init_database\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:203:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _init_database(self):\n        ^\nathalia_core/athalia_orchestrator.py:203:5: note: Use \"-> None\" if function does not return a value\nathalia_core/athalia_orchestrator.py: note: In member \"_init_components\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:266:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _init_components(self):\n        ^\nathalia_core/athalia_orchestrator.py:266:5: note: Use \"-> None\" if function does not return a value\nathalia_core/athalia_orchestrator.py: note: In member \"orchestrate_project_complete\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:311:24: error: Incompatible types in\nassignment (expression has type \"Path\", variable has type \"str\")  [assignment]\n            project_path = Path(project_path)\n                           ^~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:312:26: error: \"str\" has no attribute\n\"name\"  [attr-defined]\n            self.logger.info(f\"\ud83d\ude80 Orchestration compl\u00e8te pour: {project_pat...\n                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/athalia_orchestrator.py:316:29: error: \"str\" has no attribute\n\"name\"  [attr-defined]\n                \"project_name\": project_path.name,\n                                ^~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:330:17: error: Argument 1 to\n\"_run_industrialization\" of \"AthaliaOrchestrator\" has incompatible type \"str\";\nexpected \"Path\"  [arg-type]\n                    project_path\n                    ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:334:57: error: Argument 1 to \"_run_audit\"\nof \"AthaliaOrchestrator\" has incompatible type \"str\"; expected \"Path\" \n[arg-type]\n                results[\"steps\"][\"audit\"] = self._run_audit(project_path)\n                                                            ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:335:58: error: Argument 1 to\n\"_run_linting\" of \"AthaliaOrchestrator\" has incompatible type \"str\"; expected\n\"Path\"  [arg-type]\n                results[\"steps\"][\"lint\"] = self._run_linting(project_path)\n                                                             ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:336:69: error: Argument 1 to\n\"_run_security_audit\" of \"AthaliaOrchestrator\" has incompatible type \"str\";\nexpected \"Path\"  [arg-type]\n    ... results[\"steps\"][\"security\"] = self._run_security_audit(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:337:65: error: Argument 1 to\n\"_run_analytics\" of \"AthaliaOrchestrator\" has incompatible type \"str\"; expected\n\"Path\"  [arg-type]\n    ...     results[\"steps\"][\"analytics\"] = self._run_analytics(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:340:61: error: Argument 1 to\n\"_run_cleanup\" of \"AthaliaOrchestrator\" has incompatible type \"str\"; expected\n\"Path\"  [arg-type]\n    ...         results[\"steps\"][\"cleanup\"] = self._run_cleanup(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:341:64: error: Argument 1 to\n\"_run_documentation\" of \"AthaliaOrchestrator\" has incompatible type \"str\";\nexpected \"Path\"  [arg-type]\n    ...      results[\"steps\"][\"docs\"] = self._run_documentation(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:342:59: error: Argument 1 to\n\"_run_testing\" of \"AthaliaOrchestrator\" has incompatible type \"str\"; expected\n\"Path\"  [arg-type]\n    ...           results[\"steps\"][\"tests\"] = self._run_testing(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:343:55: error: Argument 1 to \"_run_cicd\"\nof \"AthaliaOrchestrator\" has incompatible type \"str\"; expected \"Path\" \n[arg-type]\n                results[\"steps\"][\"cicd\"] = self._run_cicd(project_path)\n                                                          ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:347:73: error: Argument 1 to\n\"_run_robotics_audit\" of \"AthaliaOrchestrator\" has incompatible type \"str\";\nexpected \"Path\"  [arg-type]\n    ... results[\"steps\"][\"robotics\"] = self._run_robotics_audit(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:350:61: error: Argument 1 to\n\"_run_plugins\" of \"AthaliaOrchestrator\" has incompatible type \"str\"; expected\n\"Path\"  [arg-type]\n    ...         results[\"steps\"][\"plugins\"] = self._run_plugins(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:351:65: error: Argument 1 to\n\"_run_templates\" of \"AthaliaOrchestrator\" has incompatible type \"str\"; expected\n\"Path\"  [arg-type]\n    ...     results[\"steps\"][\"templates\"] = self._run_templates(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:354:65: error: Argument 1 to\n\"_generate_predictions\" of \"AthaliaOrchestrator\" has incompatible type \"str\";\nexpected \"Path\"  [arg-type]\n    ...     results[\"predictions\"] = self._generate_predictions(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:355:69: error: Argument 1 to\n\"_generate_optimizations\" of \"AthaliaOrchestrator\" has incompatible type \"str\";\nexpected \"Path\"  [arg-type]\n    ... results[\"optimizations\"] = self._generate_optimizations(project_path)\n                                                                ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py: note: In member \"_run_robotics_audit\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:509:34: error: Missing positional argument\n\"project_path\" in call to \"ReachyAuditor\"  [call-arg]\n                    reachy_auditor = ReachyAuditor()\n                                     ^~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:510:33: error: Too many arguments for\n\"audit_complete\" of \"ReachyAuditor\"  [call-arg]\n                    reachy_result = reachy_auditor.audit_complete(str(proj...\n                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/athalia_orchestrator.py:513:37: error: Incompatible types in\nassignment (expression has type \"dict[str, object]\", target has type\n\"ReachyAuditResult\")  [assignment]\n                    results[\"reachy\"] = {\"error\": str(e), \"score\": 0}\n                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:517:34: error: Missing positional argument\n\"workspace_path\" in call to \"ROS2Validator\"  [call-arg]\n                    ros2_validator = ROS2Validator()\n                                     ^~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:518:31: error: Too many arguments for\n\"validate_workspace\" of \"ROS2Validator\"  [call-arg]\n                    ros2_result = ros2_validator.validate_workspace(str(pr...\n                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...\nathalia_core/athalia_orchestrator.py:519:35: error: Incompatible types in\nassignment (expression has type \"ROS2ValidationResult\", target has type\n\"ReachyAuditResult\")  [assignment]\n                    results[\"ros2\"] = ros2_result\n                                      ^~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:521:35: error: Incompatible types in\nassignment (expression has type \"dict[str, object]\", target has type\n\"ReachyAuditResult\")  [assignment]\n                    results[\"ros2\"] = {\"error\": str(e), \"valid\": False}\n                                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:525:34: error: Missing positional argument\n\"project_path\" in call to \"DockerRoboticsManager\"  [call-arg]\n                    docker_manager = DockerRoboticsManager()\n                                     ^~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:526:33: error: \"DockerRoboticsManager\" has\nno attribute \"manage_containers\"  [attr-defined]\n                    docker_result = docker_manager.manage_containers(str(p...\n                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:529:37: error: Incompatible types in\nassignment (expression has type \"dict[str, object]\", target has type\n\"ReachyAuditResult\")  [assignment]\n                    results[\"docker\"] = {\"error\": str(e), \"containers\": 0}\n                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:533:33: error: Missing positional argument\n\"project_path\" in call to \"RustAnalyzer\"  [call-arg]\n                    rust_analyzer = RustAnalyzer()\n                                    ^~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:534:31: error: \"RustAnalyzer\" has no\nattribute \"analyze_rust_code\"  [attr-defined]\n                    rust_result = rust_analyzer.analyze_rust_code(str(proj...\n                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:537:35: error: Incompatible types in\nassignment (expression has type \"dict[str, object]\", target has type\n\"ReachyAuditResult\")  [assignment]\n                    results[\"rust\"] = {\"error\": str(e), \"crates\": 0}\n                                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:541:31: error: Missing positional argument\n\"project_path\" in call to \"RoboticsCI\"  [call-arg]\n                    robotics_ci = RoboticsCI()\n                                  ^~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:542:29: error: \"RoboticsCI\" has no\nattribute \"setup_robotics_ci\"  [attr-defined]\n                    ci_result = robotics_ci.setup_robotics_ci(str(project_...\n                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:545:33: error: Incompatible types in\nassignment (expression has type \"dict[str, object]\", target has type\n\"ReachyAuditResult\")  [assignment]\n                    results[\"ci\"] = {\"error\": str(e), \"tests_passed\": 0}\n                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:550:20: error: Unsupported right operand\ntype for in (\"ReachyAuditResult\")  [operator]\n                    if \"score\" in result:\n                       ^~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:551:35: error: Value of type\n\"ReachyAuditResult\" is not indexable  [index]\n                        scores.append(result[\"score\"])\n                                      ^~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:552:22: error: Unsupported right operand\ntype for in (\"ReachyAuditResult\")  [operator]\n                    elif \"valid\" in result and result[\"valid\"]:\n                         ^~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:552:44: error: Value of type\n\"ReachyAuditResult\" is not indexable  [index]\n                    elif \"valid\" in result and result[\"valid\"]:\n                                               ^~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:554:22: error: Unsupported right operand\ntype for in (\"ReachyAuditResult\")  [operator]\n                    elif \"tests_passed\" in result and result[\"tests_passed...\n                         ^~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py:554:51: error: Value of type\n\"ReachyAuditResult\" is not indexable  [index]\n    ...               elif \"tests_passed\" in result and result[\"tests_passed\"...\n                                                        ^~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py: note: In member \"_save_unified_results\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:662:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def _save_unified_results(self, results: Dict[str, Any]):\n        ^\nathalia_core/athalia_orchestrator.py: note: In member \"validate_phase2_inputs\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:731:68: error: Incompatible default for\nargument \"required_fields\" (default has type \"None\", argument has type\n\"list[str]\")  [assignment]\n    ...       self, inputs: Dict[str, Any], required_fields: List[str] = None\n                                                                         ^~~~\nathalia_core/athalia_orchestrator.py:731:68: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\nathalia_core/athalia_orchestrator.py:731:68: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\nathalia_core/athalia_orchestrator.py:735:13: error: Statement is unreachable \n[unreachable]\n                required_fields = [\"project_path\"]\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/athalia_orchestrator.py: note: In member \"run_phase2_error_handling\" of class \"AthaliaOrchestrator\":\nathalia_core/athalia_orchestrator.py:762:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def run_phase2_error_handling(self, operation) -> Dict[str, Any]:\n        ^\nathalia_core/athalia_orchestrator.py: note: In function \"cli_entry\":\nathalia_core/athalia_orchestrator.py:812:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def cli_entry():\n    ^\nathalia_core/athalia_orchestrator.py:812:1: note: Use \"-> None\" if function does not return a value\nathalia_core/athalia_orchestrator.py: note: In function \"error_handler\":\nathalia_core/athalia_orchestrator.py:824:1: error: Function is missing a type\nannotation  [no-untyped-def]\n    def error_handler(func):\n    ^\nathalia_core/athalia_orchestrator.py:827:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def wrapper(*args, **kwargs):\n        ^\nathalia_core/athalia_orchestrator.py: note: In function \"orchestrator_auto_backup\":\nathalia_core/athalia_orchestrator.py:837:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def orchestrator_auto_backup():\n    ^\nathalia_core/athalia_orchestrator.py: note: In function \"orchestrator_main\":\nathalia_core/athalia_orchestrator.py:854:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def orchestrator_main():\n    ^\nathalia_core/athalia_orchestrator.py:854:1: note: Use \"-> None\" if function does not return a value\nathalia_core/athalia_orchestrator.py: note: In function \"main_orchestrator\":\nathalia_core/athalia_orchestrator.py:881:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def main_orchestrator():\n    ^\nathalia_core/athalia_orchestrator.py:881:1: note: Use \"-> None\" if function does not return a value\nathalia_core/ai_robust_broken.py: note: In member \"__init__\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:37:5: error: Function is missing a return type\nannotation  [no-untyped-def]\n        def __init__(self):\n        ^\nathalia_core/ai_robust_broken.py:37:5: note: Use \"-> None\" if function does not return a value\nathalia_core/ai_robust_broken.py: note: In member \"generate_blueprint\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:43:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def generate_blueprint(self, idea: str, **kwargs) -> dict:\n        ^\nathalia_core/ai_robust_broken.py: note: In member \"get_dynamic_prompt\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:200:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def get_dynamic_prompt(self, context: str, **kwargs) -> str:\n        ^\nathalia_core/ai_robust_broken.py: note: In member \"__init__\" of class \"_BlueprintProxy\":\nathalia_core/ai_robust_broken.py:206:9: error: Function is missing a type\nannotation  [no-untyped-def]\n            def __init__(self, parent):\n            ^\nathalia_core/ai_robust_broken.py: note: In member \"info\" of class \"_BlueprintProxy\":\nathalia_core/ai_robust_broken.py:209:9: error: Function is missing a type\nannotation  [no-untyped-def]\n            def info(self, *args, **kwargs):\n            ^\nathalia_core/ai_robust_broken.py: note: In member \"generate_bluelogger\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:214:5: error: Function is missing a return\ntype annotation  [no-untyped-def]\n        def generate_bluelogger(self):\n        ^\nathalia_core/ai_robust_broken.py: note: In member \"generate_blueprint_mock\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:218:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def generate_blueprint_mock(self, *args, **kwargs):\n        ^\nathalia_core/ai_robust_broken.py: note: In member \"save_blueprint\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:221:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def save_blueprint(self, *args, **kwargs):\n        ^\nathalia_core/ai_robust_broken.py: note: In member \"scan_existing_project\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:225:5: error: Function is missing a type\nannotation  [no-untyped-def]\n        def scan_existing_project(self, *args, **kwargs):\n        ^\nathalia_core/ai_robust_broken.py: note: In member \"generate_response\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:361:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def generate_response(\n        ^\nathalia_core/ai_robust_broken.py: note: In member \"_get_dynamic_prompt\" of class \"RobustAI\":\nathalia_core/ai_robust_broken.py:410:5: error: Function is missing a type\nannotation for one or more arguments  [no-untyped-def]\n        def _get_dynamic_prompt(self, context, **kwargs) -> str:\n        ^\nathalia_core/ai_robust_broken.py: note: In function \"query_qwen\":\nathalia_core/ai_robust_broken.py:515:13: error: Returning Any from function\ndeclared to return \"str\"  [no-any-return]\n                return response.json().get(\"response\", \"\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/ai_robust_broken.py: note: In function \"query_mistral\":\nathalia_core/ai_robust_broken.py:537:13: error: Returning Any from function\ndeclared to return \"str\"  [no-any-return]\n                return response.json().get(\"response\", \"\")\n                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nathalia_core/autocomplete_server.py: note: In function \"get_engine\":\nathalia_core/autocomplete_server.py:19:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def get_engine():\n    ^\nathalia_core/autocomplete_server.py: note: In function \"autocomplete\":\nathalia_core/autocomplete_server.py:27:1: error: Function is missing a return\ntype annotation  [no-untyped-def]\n    def autocomplete(request: AutocompleteRequest):\n    ^\nFound 628 errors in 56 files (checked 74 source files)\n",
      "error": "",
      "duration": 4.860282897949219,
      "critical": false
    },
    {
      "name": "bandit",
      "description": "Analyse de s\u00e9curit\u00e9",
      "success": false,
      "output": "Working... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% 0:00:00\nRun started:2025-07-29 18:12:28.832165\n\nTest results:\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/advanced_analytics.py:68:12\n67\t                file_count += 1\n68\t            except Exception:\n69\t                continue\n70\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/advanced_analytics.py:132:12\n131\t\n132\t            except Exception:\n133\t                continue\n134\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/advanced_analytics.py:150:12\n149\t                    py_file.relative_to(self.project_path))] = size\n150\t            except Exception:\n151\t                continue\n152\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/advanced_analytics.py:161:12\n160\t                    performance_data[\"dependencies\"] = len(deps)\n161\t            except Exception:\n162\t                pass\n163\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/advanced_analytics.py:191:12\n190\t\n191\t            except Exception:\n192\t                continue\n193\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/advanced_analytics.py:210:12\n209\t                    evolution_data[\"last_modified\"] = mtime\n210\t            except Exception:\n211\t                continue\n212\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/advanced_modules/auto_correction_advanced.py:132:12\n131\t                        \"description\": f\"Correction {type_correction} automatique\"}\n132\t            except Exception:\n133\t                continue\n134\t\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/agents/context_prompt.py:9:0\n8\timport logging\n9\timport subprocess\n10\timport yaml\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/agents/context_prompt.py:80:8\n79\t                PROMPTS.extend(custom_prompts)\n80\t        except Exception:\n81\t            pass\n82\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/agents/context_prompt.py:227:8\n226\t                all_content += f\"\\n# Fichier : {os.path.basename(filepath)}\\n\" + file_handle.read()\n227\t        except Exception:\n228\t            continue\n229\t    # On cr\u00e9e un fichier temporaire pour l'analyse globale\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/ai_robust.py:9:0\n8\timport logging\n9\timport subprocess\n10\tfrom enum import Enum\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/ai_robust_broken.py:9:0\n8\timport logging\n9\timport subprocess\n10\tfrom enum import Enum\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/analytics.py:80:8\n79\t                })\n80\t        except Exception:\n81\t            continue\n82\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/analytics.py:109:8\n108\t                    debt_indicators.append(f\"HACK dans {py_file.name}\")\n109\t        except Exception:\n110\t            continue\n111\t\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: athalia_core/athalia_orchestrator.py:875:27\n874\t            # En cas d'erreur (typique dans les tests), utiliser un chemin par d\u00e9faut\n875\t            project_path = \"/tmp/test_project\"\n876\t            orchestrator = AthaliaOrchestrator()\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_cicd.py:101:12\n100\t                    dependencies['python'] = deps\n101\t            except Exception:\n102\t                pass\n103\t        # Node.js\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_cicd.py:112:12\n111\t                    dependencies['nodejs'] = deps + dev_deps\n112\t            except Exception:\n113\t                pass\n114\t        return dependencies\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_cleaner.py:185:16\n184\t                        file_hashes[file_hash] = file_path\n185\t                except Exception:\n186\t                    pass\n187\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_cleaner.py:216:16\n215\t                            file_path, \"Fichier ancien (>1 an)\")\n216\t                except Exception:\n217\t                    pass\n218\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_cleaner.py:231:16\n230\t                            file_path, f\"Fichier volumineux ({size_mb:.1f}MB)\")\n231\t                except Exception:\n232\t                    pass\n233\t\n\n--------------------------------------------------\n>> Issue: [B324:hashlib] Use of weak MD5 hash for security. Consider usedforsecurity=False\n   Severity: High   Confidence: High\n   CWE: CWE-327 (https://cwe.mitre.org/data/definitions/327.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b324_hashlib.html\n   Location: athalia_core/auto_cleaner.py:301:19\n300\t        \"\"\"Calcule le hash dun fichier\"\"\"\n301\t        hash_md5 = hashlib.md5()\n302\t        try:\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:111:12\n110\t                        return lines[0]\n111\t            except Exception:\n112\t                pass\n113\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:124:12\n123\t                        return match.group(1)\n124\t            except Exception:\n125\t                pass\n126\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:141:12\n140\t                        return match.group(1)\n141\t            except Exception:\n142\t                pass\n143\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:154:12\n153\t                        return match.group(1)\n154\t            except Exception:\n155\t                pass\n156\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:171:12\n170\t                        return match.group(1)\n171\t            except Exception:\n172\t                pass\n173\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:196:12\n195\t                    dependencies['python'] = deps\n196\t            except Exception:\n197\t                pass\n198\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:208:12\n207\t                    dependencies['nodejs'] = deps + dev_deps\n208\t            except Exception:\n209\t                pass\n210\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_documenter.py:235:12\n234\t                            [ep.strip() for ep in match.split(',') if ep.strip()])\n235\t            except Exception:\n236\t                pass\n237\t\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/auto_tester.py:8:0\n7\timport ast\n8\timport subprocess\n9\timport logging\n\n--------------------------------------------------\n>> Issue: [B103:set_bad_file_permissions] Chmod setting a permissive mask 0o755 on file (run_tests_file).\n   Severity: Medium   Confidence: High\n   CWE: CWE-732 (https://cwe.mitre.org/data/definitions/732.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b103_set_bad_file_permissions.html\n   Location: athalia_core/auto_tester.py:440:8\n439\t        # Rendre le script ex\u00e9cutable\n440\t        os.chmod(run_tests_file, 0o755)\n441\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/auto_tester.py:542:12\n541\t                self._cleanup_generated_tests()\n542\t            except Exception:\n543\t                pass\n544\t\n\n--------------------------------------------------\n>> Issue: [B324:hashlib] Use of weak MD5 hash for security. Consider usedforsecurity=False\n   Severity: High   Confidence: High\n   CWE: CWE-327 (https://cwe.mitre.org/data/definitions/327.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b324_hashlib.html\n   Location: athalia_core/cache_manager.py:57:23\n56\t        # Hash du projet pour \u00e9viter les collisions\n57\t        project_hash = hashlib.md5(project_path.encode()).hexdigest()[:8]\n58\t\n\n--------------------------------------------------\n>> Issue: [B324:hashlib] Use of weak MD5 hash for security. Consider usedforsecurity=False\n   Severity: High   Confidence: High\n   CWE: CWE-327 (https://cwe.mitre.org/data/definitions/327.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b324_hashlib.html\n   Location: athalia_core/cache_manager.py:60:22\n59\t        # Param\u00e8tres suppl\u00e9mentaires\n60\t        params_hash = hashlib.md5(json.dumps(kwargs, sort_keys=True).encode()).hexdigest()[:8]\n61\t\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/code_linter.py:4:0\n3\tfrom typing import Dict, Any\n4\timport subprocess\n5\timport logging\n\n--------------------------------------------------\n>> Issue: [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n   Severity: Low   Confidence: High\n   CWE: CWE-330 (https://cwe.mitre.org/data/definitions/330.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_calls.html#b311-random\n   Location: athalia_core/distillation/code_genetics.py:23:21\n22\t        for i in range(length):\n23\t            parent = random.choice(fragments)\n24\t            child.append(parent[i])\n\n--------------------------------------------------\n>> Issue: [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n   Severity: Low   Confidence: High\n   CWE: CWE-330 (https://cwe.mitre.org/data/definitions/330.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_calls.html#b311-random\n   Location: athalia_core/distillation/code_genetics.py:36:15\n35\t        for i in range(len(words)):\n36\t            if random.random() < mutation_rate:\n37\t                words[i] = f\"MUT_{random.randint(0,999)}\"\n\n--------------------------------------------------\n>> Issue: [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n   Severity: Low   Confidence: High\n   CWE: CWE-330 (https://cwe.mitre.org/data/definitions/330.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_calls.html#b311-random\n   Location: athalia_core/distillation/code_genetics.py:37:34\n36\t            if random.random() < mutation_rate:\n37\t                words[i] = f\"MUT_{random.randint(0,999)}\"\n38\t        return ' '.join(words)\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/distillation/multimodal_distiller.py:50:8\n49\t        # Ollama LLaVA supporte --image <path> en CLI\n50\t        import subprocess\n51\t        try:\n\n--------------------------------------------------\n>> Issue: [B311:blacklist] Standard pseudo-random generators are not suitable for security/cryptographic purposes.\n   Severity: Low   Confidence: High\n   CWE: CWE-330 (https://cwe.mitre.org/data/definitions/330.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_calls.html#b311-random\n   Location: athalia_core/distillation/response_distiller.py:66:15\n65\t            count in counter.items() if count == top_count]\n66\t        return random.choice(top_responses)\n67\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:114:16\n113\t                            code_files += 1\n114\t                except Exception:\n115\t                    pass\n116\t        return {\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:167:12\n166\t                    dependencies['python'] = deps\n167\t            except Exception:\n168\t                pass\n169\t        # Node.js\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:178:12\n177\t                    dependencies['nodejs'] = deps + dev_deps\n178\t            except Exception:\n179\t                pass\n180\t        return dependencies\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:212:12\n211\t                    complexity_scores.append(complexity)\n212\t            except Exception:\n213\t                pass\n214\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:262:12\n261\t                                        f\"Indentation: {file_path.name}:{index}\")\n262\t            except Exception:\n263\t                pass\n264\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:284:12\n283\t                                documented_functions += 1\n284\t            except Exception:\n285\t                pass\n286\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:316:12\n315\t                                    f\"Classe: {node.name} dans {file_path.name}\")\n316\t            except Exception:\n317\t                pass\n318\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:355:12\n354\t                                f\"{description}: {file_path.name}\")\n355\t            except Exception:\n356\t                pass\n357\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:380:16\n379\t                                break\n380\t                except Exception:\n381\t                    pass\n382\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:395:16\n394\t                        sensitive_files.append(str(file_path))\n395\t                except Exception:\n396\t                    pass\n397\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:446:12\n445\t                                imports.append(node.module)\n446\t            except Exception:\n447\t                pass\n448\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:500:12\n499\t                    }\n500\t            except Exception:\n501\t                pass\n502\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: athalia_core/intelligent_auditor.py:585:12\n584\t                            f\"Pas de fonctions de test: {file_path.name}\")\n585\t            except Exception:\n586\t                pass\n587\t\n\n--------------------------------------------------\n>> Issue: [B324:hashlib] Use of weak MD5 hash for security. Consider usedforsecurity=False\n   Severity: High   Confidence: High\n   CWE: CWE-327 (https://cwe.mitre.org/data/definitions/327.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b324_hashlib.html\n   Location: athalia_core/intelligent_memory.py:405:15\n404\t        # Cr\u00e9er un hash\n405\t        return hashlib.md5(normalized.encode()).hexdigest()\n406\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/project_importer.py:112:12\n111\t                                imports.append(node.module)\n112\t            except Exception:\n113\t                continue\n114\t\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/robotics/docker_robotics.py:14:0\n13\timport yaml\n14\timport subprocess\n15\tfrom pathlib import Path\n\n--------------------------------------------------\n>> Issue: [B103:set_bad_file_permissions] Chmod setting a permissive mask 0o755 on file (start_script).\n   Severity: Medium   Confidence: High\n   CWE: CWE-732 (https://cwe.mitre.org/data/definitions/732.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b103_set_bad_file_permissions.html\n   Location: athalia_core/robotics/docker_robotics.py:305:16\n304\t                    f.write(self.create_start_script_template())\n305\t                os.chmod(start_script, 0o755)\n306\t                self.logger.info(\"\u2705 start.sh cr\u00e9\u00e9\")\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/robotics/robotics_ci.py:13:0\n12\t\n13\timport subprocess\n14\tfrom pathlib import Path\n\n--------------------------------------------------\n>> Issue: [B405:blacklist] Using xml.etree.ElementTree to parse untrusted XML data is known to be vulnerable to XML attacks. Replace xml.etree.ElementTree with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called.\n   Severity: Low   Confidence: High\n   CWE: CWE-20 (https://cwe.mitre.org/data/definitions/20.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b405-import-xml-etree\n   Location: athalia_core/robotics/ros2_validator.py:13:0\n12\t\n13\timport xml.etree.ElementTree as ET\n14\tfrom pathlib import Path\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/robotics/ros2_validator.py:18:0\n17\timport logging\n18\timport subprocess\n19\timport ast\n\n--------------------------------------------------\n>> Issue: [B314:blacklist] Using xml.etree.ElementTree.parse to parse untrusted XML data is known to be vulnerable to XML attacks. Replace xml.etree.ElementTree.parse with its defusedxml equivalent function or make sure defusedxml.defuse_stdlib() is called\n   Severity: Medium   Confidence: High\n   CWE: CWE-20 (https://cwe.mitre.org/data/definitions/20.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_calls.html#b313-b320-xml-bad-elementtree\n   Location: athalia_core/robotics/ros2_validator.py:124:19\n123\t        try:\n124\t            tree = ET.parse(package_xml)\n125\t            root = tree.getroot()\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/robotics/rust_analyzer.py:8:0\n7\timport logging\n8\timport subprocess\n9\tfrom dataclasses import dataclass\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/robotics_ci.py:9:0\n8\tfrom typing import Dict, Any, List, Optional\n9\timport subprocess\n10\timport json\n\n--------------------------------------------------\n>> Issue: [B405:blacklist] Using xml.etree.ElementTree to parse untrusted XML data is known to be vulnerable to XML attacks. Replace xml.etree.ElementTree with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called.\n   Severity: Low   Confidence: High\n   CWE: CWE-20 (https://cwe.mitre.org/data/definitions/20.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b405-import-xml-etree\n   Location: athalia_core/robotics_ci.py:13:0\n12\timport logging\n13\timport xml.etree.ElementTree as ET\n14\t\n\n--------------------------------------------------\n>> Issue: [B405:blacklist] Using xml.etree.ElementTree to parse untrusted XML data is known to be vulnerable to XML attacks. Replace xml.etree.ElementTree with the equivalent defusedxml package, or make sure defusedxml.defuse_stdlib() is called.\n   Severity: Low   Confidence: High\n   CWE: CWE-20 (https://cwe.mitre.org/data/definitions/20.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b405-import-xml-etree\n   Location: athalia_core/ros2_validator.py:9:0\n8\tfrom typing import Dict, Any, List, Optional\n9\timport xml.etree.ElementTree as ET\n10\timport subprocess\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/ros2_validator.py:10:0\n9\timport xml.etree.ElementTree as ET\n10\timport subprocess\n11\timport json\n\n--------------------------------------------------\n>> Issue: [B314:blacklist] Using xml.etree.ElementTree.parse to parse untrusted XML data is known to be vulnerable to XML attacks. Replace xml.etree.ElementTree.parse with its defusedxml equivalent function or make sure defusedxml.defuse_stdlib() is called\n   Severity: Medium   Confidence: High\n   CWE: CWE-20 (https://cwe.mitre.org/data/definitions/20.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_calls.html#b313-b320-xml-bad-elementtree\n   Location: athalia_core/ros2_validator.py:88:19\n87\t        try:\n88\t            tree = ET.parse(package_xml_path)\n89\t            root = tree.getroot()\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: athalia_core/security_auditor.py:4:0\n3\tfrom typing import Dict, Any\n4\timport subprocess\n5\timport json\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/security_auditor.py:121:12\n120\t\n121\t            except Exception:\n122\t                continue\n123\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/security_auditor.py:146:12\n145\t\n146\t            except Exception:\n147\t                continue\n148\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/security_auditor.py:159:16\n158\t                        )\n159\t                except Exception:\n160\t                    continue\n161\t\n\n--------------------------------------------------\n>> Issue: [B112:try_except_continue] Try, Except, Continue detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b112_try_except_continue.html\n   Location: athalia_core/security_auditor.py:181:12\n180\t\n181\t            except Exception:\n182\t                continue\n183\t\n\n--------------------------------------------------\n>> Issue: [B102:exec_used] Use of exec detected.\n   Severity: Medium   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b102_exec_used.html\n   Location: scripts/quick_performance_test.py:75:16\n74\t            try:\n75\t                exec(f\"import {module.replace('/', '.').replace('.py', '')}\")\n76\t                load_time = time.time() - start_time\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: scripts/test_athalia_performance.py:8:0\n7\timport time\n8\timport subprocess\n9\timport json\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: scripts/validation_continue.py:14:0\n13\tfrom datetime import datetime\n14\timport subprocess\n15\t\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_continue.py:77:21\n76\t            cmd = \"python scripts/athalia_unified.py --help\"\n77\t            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)\n78\t            return {'succes': result.returncode == 0}\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_continue.py:86:21\n85\t            cmd = \"python -c 'import athalia_core; print(\\\"OK\\\")'\"\n86\t            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)\n87\t            return {'succes': result.returncode == 0}\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_continue.py:95:26\n94\t            # Cr\u00e9e un projet test temporaire\n95\t            projet_test = f\"/tmp/test_continue_{int(time.time())}\"\n96\t            os.makedirs(projet_test, exist_ok=True)\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_continue.py:102:21\n101\t            cmd = f\"python scripts/athalia_unified.py {projet_test} --action audit\"\n102\t            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n103\t\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_continue.py:116:27\n115\t            # Cr\u00e9e un fichier avec une erreur simple\n116\t            fichier_test = \"/tmp/test_correction.py\"\n117\t            with open(fichier_test, 'w') as f:\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_continue.py:121:21\n120\t            cmd = f\"python scripts/athalia_unified.py {os.path.dirname(fichier_test)} --action fix\"\n121\t            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n122\t\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: scripts/validation_dashboard_simple.py:11:0\n10\timport json\n11\timport subprocess\n12\tfrom datetime import datetime\n\n--------------------------------------------------\n>> Issue: [B404:blacklist] Consider possible security implications associated with the subprocess module.\n   Severity: Low   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/blacklists/blacklist_imports.html#b404-import-subprocess\n   Location: scripts/validation_objective.py:8:0\n7\t\n8\timport subprocess\n9\timport time\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_objective.py:34:22\n33\t        # Cr\u00e9e un projet test avec Athalia\n34\t        projet_test = f\"/tmp/test_athalia_{int(time.time())}\"\n35\t        os.makedirs(projet_test, exist_ok=True)\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:50:21\n49\t        try:\n50\t            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n51\t            temps_generation = time.time() - start\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_objective.py:126:23\n125\t\n126\t        fichier_test = \"/tmp/code_avec_erreurs.py\"\n127\t        with open(fichier_test, 'w', encoding='utf-8') as f:\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:136:21\n135\t        try:\n136\t            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n137\t            temps_correction = time.time() - start\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:185:17\n184\t        cmd = \"python scripts/athalia_unified.py --audit /fichier/inexistant/qui/n/existe/pas\"\n185\t        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n186\t        tests_robustesse.append({\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_objective.py:193:23\n192\t        # Test avec fichier vide\n193\t        fichier_vide = \"/tmp/fichier_vide.py\"\n194\t        with open(fichier_vide, 'w') as f:\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:198:17\n197\t        cmd = f\"python scripts/athalia_unified.py --audit {fichier_vide}\"\n198\t        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n199\t        tests_robustesse.append({\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_objective.py:206:35\n205\t        # Test avec syntaxe invalide\n206\t        fichier_syntaxe_invalide = \"/tmp/syntaxe_invalide.py\"\n207\t        with open(fichier_syntaxe_invalide, 'w') as f:\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:211:17\n210\t        cmd = f\"python scripts/athalia_unified.py --audit {fichier_syntaxe_invalide}\"\n211\t        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n212\t        tests_robustesse.append({\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_objective.py:234:27\n233\t        start = time.time()\n234\t        projet_benchmark = \"/tmp/benchmark_test\"\n235\t        os.makedirs(projet_benchmark, exist_ok=True)\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:247:17\n246\t        cmd = f\"python scripts/athalia_unified.py {projet_benchmark} --action complete\"\n247\t        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n248\t        temps_athalia = time.time() - start\n\n--------------------------------------------------\n>> Issue: [B108:hardcoded_tmp_directory] Probable insecure usage of temp file/directory.\n   Severity: Medium   Confidence: Medium\n   CWE: CWE-377 (https://cwe.mitre.org/data/definitions/377.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b108_hardcoded_tmp_directory.html\n   Location: scripts/validation_objective.py:275:25\n274\t        # G\u00e9n\u00e8re un projet pour analyse\n275\t        projet_qualite = \"/tmp/projet_qualite\"\n276\t        os.makedirs(projet_qualite, exist_ok=True)\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:289:17\n288\t\n289\t        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n290\t\n\n--------------------------------------------------\n>> Issue: [B602:subprocess_popen_with_shell_equals_true] subprocess call with shell=True identified, security issue.\n   Severity: High   Confidence: High\n   CWE: CWE-78 (https://cwe.mitre.org/data/definitions/78.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b602_subprocess_popen_with_shell_equals_true.html\n   Location: scripts/validation_objective.py:300:28\n299\t            cmd_pylint = f\"python -m pylint {projet_qualite} --output-format=json\"\n300\t            result_pylint = subprocess.run(cmd_pylint, shell=True, capture_output=True, text=True)\n301\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: scripts/validation_objective.py:318:8\n317\t                    pass\n318\t        except Exception:\n319\t            pass\n320\t\n\n--------------------------------------------------\n>> Issue: [B110:try_except_pass] Try, Except, Pass detected.\n   Severity: Low   Confidence: High\n   CWE: CWE-703 (https://cwe.mitre.org/data/definitions/703.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b110_try_except_pass.html\n   Location: scripts/validation_objective.py:335:12\n334\t                    classes += contenu.count('class ')\n335\t            except Exception:\n336\t                pass\n337\t\n\n--------------------------------------------------\n>> Issue: [B324:hashlib] Use of weak MD5 hash for security. Consider usedforsecurity=False\n   Severity: High   Confidence: High\n   CWE: CWE-327 (https://cwe.mitre.org/data/definitions/327.html)\n   More Info: https://bandit.readthedocs.io/en/1.8.3/plugins/b324_hashlib.html\n   Location: tools/maintenance/cleanup_old_data.py:35:19\n34\t        \"\"\"Calcule le hash MD5 d'un fichier\"\"\"\n35\t        hash_md5 = hashlib.md5()\n36\t        with open(file_path, \"rb\") as f:\n\n--------------------------------------------------\n\nCode scanned:\n\tTotal lines of code: 17982\n\tTotal lines skipped (#nosec): 0\n\tTotal potential issues skipped due to specifically being disabled (e.g., #nosec BXXX): 0\n\nRun metrics:\n\tTotal issues (by severity):\n\t\tUndefined: 0\n\t\tLow: 69\n\t\tMedium: 14\n\t\tHigh: 17\n\tTotal issues (by confidence):\n\t\tUndefined: 0\n\t\tLow: 0\n\t\tMedium: 9\n\t\tHigh: 91\nFiles skipped (2):\n\tathalia_core/._ai_robust.py (syntax error while parsing AST from file)\n\tathalia_core/._unified_orchestrator.py (syntax error while parsing AST from file)\n",
      "error": "[main]\tINFO\tprofile include tests: None\n[main]\tINFO\tprofile exclude tests: B610,B604,B607,B101,B606,B601,B603,B609,B605,B608\n[main]\tINFO\tcli include tests: None\n[main]\tINFO\tcli exclude tests: None\n[main]\tINFO\tusing config: config/.bandit\n[main]\tINFO\trunning on Python 3.11.13\n",
      "duration": 0.7812490463256836,
      "critical": true
    },
    {
      "name": "safety",
      "description": "V\u00e9rification des vuln\u00e9rabilit\u00e9s",
      "success": false,
      "output": "\n\n+===========================================================================================================================================================================================+\n\n\nDEPRECATED: this command (`check`) has been DEPRECATED, and will be unsupported beyond 01 June 2024.\n\n\nWe highly encourage switching to the new `scan` command which is easier to use, more powerful, and can be set up to mimic the deprecated command if required.\n\n\n+===========================================================================================================================================================================================+\n\n\n",
      "error": "/opt/homebrew/lib/python3.11/site-packages/safety/safety.py:1585: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n  import pkg_resources\nUnhandled exception happened: post_dump() got an unexpected keyword argument 'pass_many'\n",
      "duration": 5.099197149276733,
      "critical": true
    },
    {
      "name": "pip-audit",
      "description": "Audit des d\u00e9pendances",
      "success": false,
      "output": "{\"dependencies\": [{\"name\": \"absl-py\", \"version\": \"2.2.1\", \"vulns\": []}, {\"name\": \"aiohappyeyeballs\", \"version\": \"2.6.1\", \"vulns\": []}, {\"name\": \"aiohttp\", \"version\": \"3.11.15\", \"vulns\": [{\"id\": \"GHSA-9548-qrrj-x5pj\", \"fix_versions\": [\"3.12.14\"], \"aliases\": [\"CVE-2025-53643\"], \"description\": \"### Summary The Python parser is vulnerable to a request smuggling vulnerability due to not parsing trailer sections of an HTTP request.  ### Impact If a pure Python version of aiohttp is installed (i.e. without the usual C extensions) or AIOHTTP_NO_EXTENSIONS is enabled, then an attacker may be able to execute a request smuggling attack to bypass certain firewalls or proxy protections.  ----  Patch: https://github.com/aio-libs/aiohttp/commit/e8d774f635dc6d1cd3174d0e38891da5de0e2b6a\"}]}, {\"name\": \"aiosignal\", \"version\": \"1.3.2\", \"vulns\": []}, {\"name\": \"alembic\", \"version\": \"1.15.1\", \"vulns\": []}, {\"name\": \"amqp\", \"version\": \"5.3.1\", \"vulns\": []}, {\"name\": \"annotated-types\", \"version\": \"0.7.0\", \"vulns\": []}, {\"name\": \"anyascii\", \"version\": \"0.3.2\", \"vulns\": []}, {\"name\": \"anyio\", \"version\": \"4.9.0\", \"vulns\": []}, {\"name\": \"astroid\", \"version\": \"3.3.9\", \"vulns\": []}, {\"name\": \"attrs\", \"version\": \"25.3.0\", \"vulns\": []}, {\"name\": \"audioread\", \"version\": \"3.0.1\", \"vulns\": []}, {\"name\": \"authlib\", \"version\": \"1.5.2\", \"vulns\": []}, {\"name\": \"babel\", \"version\": \"2.17.0\", \"vulns\": []}, {\"name\": \"bandit\", \"version\": \"1.8.3\", \"vulns\": []}, {\"name\": \"bangla\", \"version\": \"0.0.2\", \"vulns\": []}, {\"name\": \"bcrypt\", \"version\": \"4.3.0\", \"vulns\": []}, {\"name\": \"black\", \"version\": \"25.1.0\", \"vulns\": []}, {\"name\": \"blinker\", \"version\": \"1.9.0\", \"vulns\": []}, {\"name\": \"blis\", \"version\": \"1.2.1\", \"vulns\": []}, {\"name\": \"bnnumerizer\", \"version\": \"0.0.2\", \"vulns\": []}, {\"name\": \"bnunicodenormalizer\", \"version\": \"0.1.7\", \"vulns\": []}, {\"name\": \"boolean-py\", \"version\": \"5.0\", \"vulns\": []}, {\"name\": \"cachecontrol\", \"version\": \"0.14.2\", \"vulns\": []}, {\"name\": \"cachetools\", \"version\": \"5.5.2\", \"vulns\": []}, {\"name\": \"catalogue\", \"version\": \"2.0.10\", \"vulns\": []}, {\"name\": \"certifi\", \"version\": \"2025.1.31\", \"vulns\": []}, {\"name\": \"cffi\", \"version\": \"1.17.1\", \"vulns\": []}, {\"name\": \"charset-normalizer\", \"version\": \"3.4.1\", \"vulns\": []}, {\"name\": \"click\", \"version\": \"8.1.8\", \"vulns\": []}, {\"name\": \"cloudpathlib\", \"version\": \"0.21.0\", \"vulns\": []}, {\"name\": \"confection\", \"version\": \"0.1.5\", \"vulns\": []}, {\"name\": \"contourpy\", \"version\": \"1.3.1\", \"vulns\": []}, {\"name\": \"coqpit\", \"version\": \"0.0.17\", \"vulns\": []}, {\"name\": \"coverage\", \"version\": \"7.8.0\", \"vulns\": []}, {\"name\": \"croniter\", \"version\": \"6.0.0\", \"vulns\": []}, {\"name\": \"cryptography\", \"version\": \"44.0.2\", \"vulns\": []}, {\"name\": \"cycler\", \"version\": \"0.12.1\", \"vulns\": []}, {\"name\": \"cyclonedx-python-lib\", \"version\": \"9.1.0\", \"vulns\": []}, {\"name\": \"cymem\", \"version\": \"2.0.11\", \"vulns\": []}, {\"name\": \"cython\", \"version\": \"3.0.12\", \"vulns\": []}, {\"name\": \"dateparser\", \"version\": \"1.1.8\", \"vulns\": []}, {\"name\": \"debtcollector\", \"version\": \"3.0.0\", \"vulns\": []}, {\"name\": \"decorator\", \"version\": \"5.2.1\", \"vulns\": []}, {\"name\": \"defusedxml\", \"version\": \"0.7.1\", \"vulns\": []}, {\"name\": \"dill\", \"version\": \"0.4.0\", \"vulns\": []}, {\"name\": \"distro\", \"version\": \"1.9.0\", \"vulns\": []}, {\"name\": \"dnspython\", \"version\": \"2.7.0\", \"vulns\": []}, {\"name\": \"docopt\", \"version\": \"0.6.2\", \"vulns\": []}, {\"name\": \"dodgy\", \"version\": \"0.2.1\", \"vulns\": []}, {\"name\": \"dogpile-cache\", \"version\": \"1.3.4\", \"vulns\": []}, {\"name\": \"dparse\", \"version\": \"0.6.4\", \"vulns\": []}, {\"name\": \"ecdsa\", \"version\": \"0.19.1\", \"vulns\": []}, {\"name\": \"edge-tts\", \"version\": \"7.0.0\", \"vulns\": []}, {\"name\": \"einops\", \"version\": \"0.8.1\", \"vulns\": []}, {\"name\": \"encodec\", \"version\": \"0.1.1\", \"vulns\": []}, {\"name\": \"eventlet\", \"version\": \"0.39.1\", \"vulns\": []}, {\"name\": \"fastapi\", \"version\": \"0.115.12\", \"vulns\": []}, {\"name\": \"fasteners\", \"version\": \"0.19\", \"vulns\": []}, {\"name\": \"filelock\", \"version\": \"3.16.1\", \"vulns\": []}, {\"name\": \"flake8\", \"version\": \"7.2.0\", \"vulns\": []}, {\"name\": \"flake8-polyfill\", \"version\": \"1.0.2\", \"vulns\": []}, {\"name\": \"flask\", \"version\": \"3.1.0\", \"vulns\": [{\"id\": \"GHSA-4grg-w6v8-c28g\", \"fix_versions\": [\"3.1.1\"], \"aliases\": [\"CVE-2025-47278\"], \"description\": \"In Flask 3.1.0, the way fallback key configuration was handled resulted in the last fallback key being used for signing, rather than the current signing key.  Signing is provided by the `itsdangerous` library. A list of keys can be passed, and it expects the last (top) key in the list to be the most recent key, and uses that for signing. Flask was incorrectly constructing that list in reverse, passing the signing key first.  Sites that have opted-in to use key rotation by setting `SECRET_KEY_FALLBACKS` are likely to unexpectedly be signing their sessions with stale keys, and their transition to fresher keys will be impeded. Sessions are still signed, so this would not cause any sort of data integrity loss.\"}]}, {\"name\": \"fonttools\", \"version\": \"4.56.0\", \"vulns\": []}, {\"name\": \"frozenlist\", \"version\": \"1.5.0\", \"vulns\": []}, {\"name\": \"fsspec\", \"version\": \"2025.3.2\", \"vulns\": []}, {\"name\": \"futurist\", \"version\": \"3.1.0\", \"vulns\": []}, {\"name\": \"g2pkk\", \"version\": \"0.1.2\", \"vulns\": []}, {\"name\": \"gitdb\", \"version\": \"4.0.12\", \"vulns\": []}, {\"name\": \"gitpython\", \"version\": \"3.1.44\", \"vulns\": []}, {\"name\": \"greenlet\", \"version\": \"3.1.1\", \"vulns\": []}, {\"name\": \"grpcio\", \"version\": \"1.71.0\", \"vulns\": []}, {\"name\": \"gruut\", \"version\": \"2.2.3\", \"vulns\": []}, {\"name\": \"gruut-ipa\", \"version\": \"0.13.0\", \"vulns\": []}, {\"name\": \"gruut-lang-de\", \"version\": \"2.0.1\", \"vulns\": []}, {\"name\": \"gruut-lang-en\", \"version\": \"2.0.1\", \"vulns\": []}, {\"name\": \"gruut-lang-es\", \"version\": \"2.0.1\", \"vulns\": []}, {\"name\": \"gruut-lang-fr\", \"version\": \"2.0.2\", \"vulns\": []}, {\"name\": \"h11\", \"version\": \"0.14.0\", \"vulns\": [{\"id\": \"GHSA-vqfr-h8mv-ghfj\", \"fix_versions\": [\"0.16.0\"], \"aliases\": [\"CVE-2025-43859\"], \"description\": \"### Impact  A leniency in h11's parsing of line terminators in chunked-coding message bodies can lead to request smuggling vulnerabilities under certain conditions.  ### Details  HTTP/1.1 Chunked-Encoding bodies are formatted as a sequence of \\\"chunks\\\", each of which consists of:  - chunk length - `\\\\r\\\\n` - `length` bytes of content - `\\\\r\\\\n`  In versions of h11 up to 0.14.0, h11 instead parsed them as:  - chunk length - `\\\\r\\\\n` - `length` bytes of content - any two bytes  i.e. it did not validate that the trailing `\\\\r\\\\n` bytes were correct, and if you put 2 bytes of garbage there it would be accepted, instead of correctly rejecting the body as malformed.  By itself this is harmless. However, suppose you have a proxy or reverse-proxy that tries to analyze HTTP requests, and your proxy has a _different_ bug in parsing Chunked-Encoding, acting as if the format is:  - chunk length - `\\\\r\\\\n` - `length` bytes of content - more bytes of content, as many as it takes until you find a `\\\\r\\\\n`  For example, [pound](https://github.com/graygnuorg/pound/pull/43) had this bug -- it can happen if an implementer uses a generic \\\"read until end of line\\\" helper to consumes the trailing `\\\\r\\\\n`.  In this case, h11 and your proxy may both accept the same stream of bytes, but interpret them differently. For example, consider the following HTTP request(s) (assume all line breaks are `\\\\r\\\\n`):  ``` GET /one HTTP/1.1 Host: localhost Transfer-Encoding: chunked  5 AAAAAXX2 45 0  GET /two HTTP/1.1 Host: localhost Transfer-Encoding: chunked  0 ```  Here h11 will interpret it as two requests, one with body `AAAAA45` and one with an empty body, while our hypothetical buggy proxy will interpret it as a single request, with body `AAAAXX20\\\\r\\\\n\\\\r\\\\nGET /two ...`. And any time two HTTP processors both accept the same string of bytes but interpret them differently, you have the conditions for a \\\"request smuggling\\\" attack. For example, if `/two` is a dangerous endpoint and the job of the reverse proxy is to stop requests from getting there, then an attacker could use a bytestream like the above to circumvent this protection.  Even worse, if our buggy reverse proxy receives two requests from different users:  ``` GET /one HTTP/1.1 Host: localhost Transfer-Encoding: chunked  5 AAAAAXX999 0 ```  ``` GET /two HTTP/1.1 Host: localhost Cookie: SESSION_KEY=abcdef... ```  ...it will consider the first request to be complete and valid, and send both on to the h11-based web server over the same socket. The server will then see the two concatenated requests, and interpret them as _one_ request to `/one` whose body includes `/two`'s session key, potentially allowing one user to steal another's credentials.  ### Patches  Fixed in h11 0.15.0.  ### Workarounds  Since exploitation requires the combination of buggy h11 with a buggy (reverse) proxy, fixing either component is sufficient to mitigate this issue.  ### Credits  Reported by Jeppe Bonde Weikop on 2025-01-09.\"}]}, {\"name\": \"hangul-romanize\", \"version\": \"0.1.0\", \"vulns\": []}, {\"name\": \"httpcore\", \"version\": \"1.0.7\", \"vulns\": []}, {\"name\": \"httpx\", \"version\": \"0.28.1\", \"vulns\": []}, {\"name\": \"huggingface-hub\", \"version\": \"0.30.1\", \"vulns\": []}, {\"name\": \"idna\", \"version\": \"3.10\", \"vulns\": []}, {\"name\": \"importlib-metadata\", \"version\": \"8.6.1\", \"vulns\": []}, {\"name\": \"inflect\", \"version\": \"7.5.0\", \"vulns\": []}, {\"name\": \"iniconfig\", \"version\": \"2.1.0\", \"vulns\": []}, {\"name\": \"iso8601\", \"version\": \"2.1.0\", \"vulns\": []}, {\"name\": \"isort\", \"version\": \"6.0.1\", \"vulns\": []}, {\"name\": \"itsdangerous\", \"version\": \"2.2.0\", \"vulns\": []}, {\"name\": \"jamo\", \"version\": \"0.4.1\", \"vulns\": []}, {\"name\": \"jieba\", \"version\": \"0.42.1\", \"vulns\": []}, {\"name\": \"jinja2\", \"version\": \"3.1.6\", \"vulns\": []}, {\"name\": \"jiter\", \"version\": \"0.9.0\", \"vulns\": []}, {\"name\": \"joblib\", \"version\": \"1.4.2\", \"vulns\": []}, {\"name\": \"jose\", \"version\": \"1.0.0\", \"vulns\": []}, {\"name\": \"jsonlines\", \"version\": \"1.2.0\", \"vulns\": []}, {\"name\": \"jsonschema\", \"version\": \"4.23.0\", \"vulns\": []}, {\"name\": \"jsonschema-specifications\", \"version\": \"2024.10.1\", \"vulns\": []}, {\"name\": \"keystoneauth1\", \"version\": \"5.10.0\", \"vulns\": []}, {\"name\": \"keystonemiddleware\", \"version\": \"10.9.0\", \"vulns\": []}, {\"name\": \"kiwisolver\", \"version\": \"1.4.8\", \"vulns\": []}, {\"name\": \"kombu\", \"version\": \"5.5.0\", \"vulns\": []}, {\"name\": \"langcodes\", \"version\": \"3.5.0\", \"vulns\": []}, {\"name\": \"language-data\", \"version\": \"1.3.0\", \"vulns\": []}, {\"name\": \"lazy-loader\", \"version\": \"0.4\", \"vulns\": []}, {\"name\": \"librosa\", \"version\": \"0.11.0\", \"vulns\": []}, {\"name\": \"license-expression\", \"version\": \"30.4.1\", \"vulns\": []}, {\"name\": \"llvmlite\", \"version\": \"0.44.0\", \"vulns\": []}, {\"name\": \"mako\", \"version\": \"1.3.9\", \"vulns\": []}, {\"name\": \"marisa-trie\", \"version\": \"1.2.1\", \"vulns\": []}, {\"name\": \"markdown\", \"version\": \"3.7\", \"vulns\": []}, {\"name\": \"markdown-it-py\", \"version\": \"3.0.0\", \"vulns\": []}, {\"name\": \"markupsafe\", \"version\": \"3.0.2\", \"vulns\": []}, {\"name\": \"marshmallow\", \"version\": \"4.0.0\", \"vulns\": []}, {\"name\": \"matplotlib\", \"version\": \"3.10.1\", \"vulns\": []}, {\"name\": \"mccabe\", \"version\": \"0.7.0\", \"vulns\": []}, {\"name\": \"mdurl\", \"version\": \"0.1.2\", \"vulns\": []}, {\"name\": \"mistral\", \"version\": \"19.0.0\", \"vulns\": []}, {\"name\": \"mistral-lib\", \"version\": \"3.3.1\", \"vulns\": []}, {\"name\": \"more-itertools\", \"version\": \"10.6.0\", \"vulns\": []}, {\"name\": \"mpmath\", \"version\": \"1.3.0\", \"vulns\": []}, {\"name\": \"msgpack\", \"version\": \"1.1.0\", \"vulns\": []}, {\"name\": \"multidict\", \"version\": \"6.3.0\", \"vulns\": []}, {\"name\": \"murmurhash\", \"version\": \"1.0.12\", \"vulns\": []}, {\"name\": \"mutagen\", \"version\": \"1.47.0\", \"vulns\": []}, {\"name\": \"mypy\", \"version\": \"1.15.0\", \"vulns\": []}, {\"name\": \"mypy-extensions\", \"version\": \"1.0.0\", \"vulns\": []}, {\"name\": \"netaddr\", \"version\": \"1.3.0\", \"vulns\": []}, {\"name\": \"networkx\", \"version\": \"2.8.8\", \"vulns\": []}, {\"name\": \"nltk\", \"version\": \"3.9.1\", \"vulns\": []}, {\"name\": \"num2words\", \"version\": \"0.5.14\", \"vulns\": []}, {\"name\": \"numba\", \"version\": \"0.61.0\", \"vulns\": []}, {\"name\": \"numpy\", \"version\": \"1.26.4\", \"vulns\": []}, {\"name\": \"openai\", \"version\": \"1.70.0\", \"vulns\": []}, {\"name\": \"os-service-types\", \"version\": \"1.7.0\", \"vulns\": []}, {\"name\": \"oslo-cache\", \"version\": \"3.10.1\", \"vulns\": []}, {\"name\": \"oslo-concurrency\", \"version\": \"7.1.0\", \"vulns\": []}, {\"name\": \"oslo-config\", \"version\": \"9.7.1\", \"vulns\": []}, {\"name\": \"oslo-context\", \"version\": \"5.7.1\", \"vulns\": []}, {\"name\": \"oslo-db\", \"version\": \"17.2.1\", \"vulns\": []}, {\"name\": \"oslo-i18n\", \"version\": \"6.5.1\", \"vulns\": []}, {\"name\": \"oslo-log\", \"version\": \"7.1.0\", \"vulns\": []}, {\"name\": \"oslo-messaging\", \"version\": \"16.1.0\", \"vulns\": []}, {\"name\": \"oslo-metrics\", \"version\": \"0.11.0\", \"vulns\": []}, {\"name\": \"oslo-middleware\", \"version\": \"6.3.1\", \"vulns\": []}, {\"name\": \"oslo-policy\", \"version\": \"4.5.1\", \"vulns\": []}, {\"name\": \"oslo-serialization\", \"version\": \"5.7.0\", \"vulns\": []}, {\"name\": \"oslo-service\", \"version\": \"4.1.1\", \"vulns\": []}, {\"name\": \"oslo-utils\", \"version\": \"8.2.0\", \"vulns\": []}, {\"name\": \"osprofiler\", \"version\": \"4.2.0\", \"vulns\": []}, {\"name\": \"packageurl-python\", \"version\": \"0.16.0\", \"vulns\": []}, {\"name\": \"packaging\", \"version\": \"24.2\", \"vulns\": []}, {\"name\": \"pandas\", \"version\": \"1.5.3\", \"vulns\": []}, {\"name\": \"paramiko\", \"version\": \"3.5.1\", \"vulns\": []}, {\"name\": \"paste\", \"version\": \"3.10.1\", \"vulns\": []}, {\"name\": \"pastedeploy\", \"version\": \"3.1.0\", \"vulns\": []}, {\"name\": \"pathspec\", \"version\": \"0.12.1\", \"vulns\": []}, {\"name\": \"pbr\", \"version\": \"6.1.1\", \"vulns\": []}, {\"name\": \"pecan\", \"version\": \"1.6.0\", \"vulns\": []}, {\"name\": \"pep8-naming\", \"version\": \"0.10.0\", \"vulns\": []}, {\"name\": \"pillow\", \"skip_reason\": \"Dependency not found on PyPI and could not be audited: pillow (11.2.0)\"}, {\"name\": \"pip\", \"version\": \"25.1.1\", \"vulns\": []}, {\"name\": \"pip-api\", \"version\": \"0.0.34\", \"vulns\": []}, {\"name\": \"pip-audit\", \"version\": \"2.9.0\", \"vulns\": []}, {\"name\": \"pip-requirements-parser\", \"version\": \"32.0.1\", \"vulns\": []}, {\"name\": \"platformdirs\", \"version\": \"4.3.7\", \"vulns\": []}, {\"name\": \"pluggy\", \"version\": \"1.5.0\", \"vulns\": []}, {\"name\": \"ply\", \"version\": \"3.11\", \"vulns\": []}, {\"name\": \"pooch\", \"version\": \"1.8.2\", \"vulns\": []}, {\"name\": \"preshed\", \"version\": \"3.0.9\", \"vulns\": []}, {\"name\": \"prettytable\", \"version\": \"3.15.1\", \"vulns\": []}, {\"name\": \"prometheus-client\", \"version\": \"0.21.1\", \"vulns\": []}, {\"name\": \"propcache\", \"version\": \"0.3.1\", \"vulns\": []}, {\"name\": \"prospector\", \"version\": \"1.16.1\", \"vulns\": []}, {\"name\": \"protobuf\", \"version\": \"6.30.2\", \"vulns\": [{\"id\": \"GHSA-8qvm-5x2c-j2w7\", \"fix_versions\": [\"4.25.8\", \"5.29.5\", \"6.31.1\"], \"aliases\": [\"CVE-2025-4565\"], \"description\": \"### Summary Any project that uses Protobuf pure-Python backend to parse untrusted Protocol Buffers data containing an arbitrary number of **recursive groups**, **recursive messages** or **a series of [`SGROUP`](https://protobuf.dev/programming-guides/encoding/#groups) tags** can be corrupted by exceeding the Python recursion limit.  Reporter: Alexis Challande, Trail of Bits Ecosystem Security Team [ecosystem@trailofbits.com](mailto:ecosystem@trailofbits.com)  Affected versions: This issue only affects the [pure-Python implementation](https://github.com/protocolbuffers/protobuf/tree/main/python#implementation-backends) of protobuf-python backend. This is the implementation when `PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python` environment variable is set or the default when protobuf is used from Bazel or pure-Python PyPi wheels. CPython PyPi wheels do not use pure-Python by default.  This is a Python variant of a [previous issue affecting protobuf-java](https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-735f-pc8j-v9w8).  ### Severity This is a potential Denial of Service. Parsing nested protobuf data creates unbounded recursions that can be abused by an attacker.  ### Proof of Concept For reproduction details, please refer to the unit tests [decoder_test.py](https://github.com/protocolbuffers/protobuf/blob/main/python/google/protobuf/internal/decoder_test.py#L87-L98) and [message_test](https://github.com/protocolbuffers/protobuf/blob/main/python/google/protobuf/internal/message_test.py#L1436-L1478)  ### Remediation and Mitigation A mitigation is available now. Please update to the latest available versions of the following packages: * protobuf-python(4.25.8, 5.29.5, 6.31.1)\"}]}, {\"name\": \"psutil\", \"version\": \"6.1.1\", \"vulns\": []}, {\"name\": \"py-serializable\", \"version\": \"2.0.0\", \"vulns\": []}, {\"name\": \"pyasn1\", \"version\": \"0.4.8\", \"vulns\": []}, {\"name\": \"pycadf\", \"version\": \"4.0.1\", \"vulns\": []}, {\"name\": \"pycodestyle\", \"version\": \"2.13.0\", \"vulns\": []}, {\"name\": \"pycparser\", \"version\": \"2.22\", \"vulns\": []}, {\"name\": \"pydantic\", \"version\": \"2.9.2\", \"vulns\": []}, {\"name\": \"pydantic-core\", \"version\": \"2.23.4\", \"vulns\": []}, {\"name\": \"pydocstyle\", \"version\": \"6.3.0\", \"vulns\": []}, {\"name\": \"pyflakes\", \"version\": \"3.3.2\", \"vulns\": []}, {\"name\": \"pygments\", \"version\": \"2.19.1\", \"vulns\": []}, {\"name\": \"pyjwt\", \"version\": \"2.10.1\", \"vulns\": []}, {\"name\": \"pylint\", \"version\": \"3.3.6\", \"vulns\": []}, {\"name\": \"pylint-celery\", \"version\": \"0.3\", \"vulns\": []}, {\"name\": \"pylint-django\", \"version\": \"2.6.1\", \"vulns\": []}, {\"name\": \"pylint-plugin-utils\", \"version\": \"0.8.2\", \"vulns\": []}, {\"name\": \"pynacl\", \"version\": \"1.5.0\", \"vulns\": []}, {\"name\": \"pynndescent\", \"version\": \"0.5.13\", \"vulns\": []}, {\"name\": \"pyparsing\", \"version\": \"3.2.1\", \"vulns\": []}, {\"name\": \"pypinyin\", \"version\": \"0.54.0\", \"vulns\": []}, {\"name\": \"pysbd\", \"version\": \"0.3.4\", \"vulns\": []}, {\"name\": \"pytest\", \"version\": \"8.3.5\", \"vulns\": []}, {\"name\": \"python-crfsuite\", \"version\": \"0.9.11\", \"vulns\": []}, {\"name\": \"python-dateutil\", \"version\": \"2.9.0.post0\", \"vulns\": []}, {\"name\": \"python-dotenv\", \"version\": \"1.1.0\", \"vulns\": []}, {\"name\": \"python-gnupg\", \"version\": \"0.5.4\", \"vulns\": []}, {\"name\": \"python-jose\", \"version\": \"3.4.0\", \"vulns\": []}, {\"name\": \"python-keystoneclient\", \"version\": \"5.6.0\", \"vulns\": []}, {\"name\": \"pytz\", \"version\": \"2025.1\", \"vulns\": []}, {\"name\": \"pyyaml\", \"version\": \"6.0.2\", \"vulns\": []}, {\"name\": \"redis\", \"version\": \"5.2.1\", \"vulns\": []}, {\"name\": \"referencing\", \"version\": \"0.36.2\", \"vulns\": []}, {\"name\": \"regex\", \"version\": \"2024.11.6\", \"vulns\": []}, {\"name\": \"repoze-lru\", \"version\": \"0.7\", \"vulns\": []}, {\"name\": \"requests\", \"version\": \"2.32.3\", \"vulns\": [{\"id\": \"GHSA-9hjg-9r4m-mvj7\", \"fix_versions\": [\"2.32.4\"], \"aliases\": [\"CVE-2024-47081\"], \"description\": \"### Impact  Due to a URL parsing issue, Requests releases prior to 2.32.4 may leak .netrc credentials to third parties for specific maliciously-crafted URLs.  ### Workarounds For older versions of Requests, use of the .netrc file can be disabled with `trust_env=False` on your Requests Session ([docs](https://requests.readthedocs.io/en/latest/api/#requests.Session.trust_env)).  ### References https://github.com/psf/requests/pull/6965 https://seclists.org/fulldisclosure/2025/Jun/2\"}]}, {\"name\": \"requirements-detector\", \"version\": \"1.3.2\", \"vulns\": []}, {\"name\": \"rfc3986\", \"version\": \"2.0.0\", \"vulns\": []}, {\"name\": \"rich\", \"version\": \"14.0.0\", \"vulns\": []}, {\"name\": \"routes\", \"version\": \"2.5.1\", \"vulns\": []}, {\"name\": \"rpds-py\", \"version\": \"0.23.1\", \"vulns\": []}, {\"name\": \"rsa\", \"version\": \"4.9\", \"vulns\": []}, {\"name\": \"ruamel-yaml\", \"version\": \"0.18.10\", \"vulns\": []}, {\"name\": \"ruamel-yaml-clib\", \"version\": \"0.2.12\", \"vulns\": []}, {\"name\": \"safetensors\", \"version\": \"0.5.3\", \"vulns\": []}, {\"name\": \"safety\", \"version\": \"3.3.1\", \"vulns\": []}, {\"name\": \"safety-schemas\", \"version\": \"0.0.11\", \"vulns\": []}, {\"name\": \"scikit-learn\", \"version\": \"1.6.1\", \"vulns\": []}, {\"name\": \"scipy\", \"version\": \"1.15.2\", \"vulns\": []}, {\"name\": \"semver\", \"version\": \"3.0.4\", \"vulns\": []}, {\"name\": \"setoptconf-tmp\", \"version\": \"0.3.1\", \"vulns\": []}, {\"name\": \"setuptools\", \"version\": \"80.9.0\", \"vulns\": []}, {\"name\": \"shellingham\", \"version\": \"1.5.4\", \"vulns\": []}, {\"name\": \"simplegeneric\", \"version\": \"0.8.1\", \"vulns\": []}, {\"name\": \"six\", \"version\": \"1.17.0\", \"vulns\": []}, {\"name\": \"smart-open\", \"version\": \"7.1.0\", \"vulns\": []}, {\"name\": \"smmap\", \"version\": \"5.0.2\", \"vulns\": []}, {\"name\": \"sniffio\", \"version\": \"1.3.1\", \"vulns\": []}, {\"name\": \"snowballstemmer\", \"version\": \"2.2.0\", \"vulns\": []}, {\"name\": \"sortedcontainers\", \"version\": \"2.4.0\", \"vulns\": []}, {\"name\": \"sounddevice\", \"version\": \"0.5.1\", \"vulns\": []}, {\"name\": \"soundfile\", \"version\": \"0.13.1\", \"vulns\": []}, {\"name\": \"soxr\", \"version\": \"0.5.0.post1\", \"vulns\": []}, {\"name\": \"spacy\", \"version\": \"3.8.5\", \"vulns\": []}, {\"name\": \"spacy-legacy\", \"version\": \"3.0.12\", \"vulns\": []}, {\"name\": \"spacy-loggers\", \"version\": \"1.0.5\", \"vulns\": []}, {\"name\": \"spotipy\", \"version\": \"2.25.1\", \"vulns\": []}, {\"name\": \"sqlalchemy\", \"version\": \"2.0.39\", \"vulns\": []}, {\"name\": \"srsly\", \"version\": \"2.5.1\", \"vulns\": []}, {\"name\": \"srt\", \"version\": \"3.5.3\", \"vulns\": []}, {\"name\": \"starlette\", \"version\": \"0.46.1\", \"vulns\": [{\"id\": \"GHSA-2c2j-9gv5-cj73\", \"fix_versions\": [\"0.47.2\"], \"aliases\": [\"CVE-2025-54121\"], \"description\": \"### Summary When parsing a multi-part form with large files (greater than the [default max spool size](https://github.com/encode/starlette/blob/fa5355442753f794965ae1af0f87f9fec1b9a3de/starlette/formparsers.py#L126)) `starlette` will block the main thread to roll the file over to disk. This blocks the event thread which means we can't accept new connections.  ### Details Please see this discussion for details: https://github.com/encode/starlette/discussions/2927#discussioncomment-13721403. In summary the following UploadFile code (copied from [here](https://github.com/encode/starlette/blob/fa5355442753f794965ae1af0f87f9fec1b9a3de/starlette/datastructures.py#L436C5-L447C14)) has a minor bug. Instead of just checking for `self._in_memory` we should also check if the additional bytes will cause a rollover.  ```python      @property     def _in_memory(self) -> bool:         # check for SpooledTemporaryFile._rolled         rolled_to_disk = getattr(self.file, \\\"_rolled\\\", True)         return not rolled_to_disk      async def write(self, data: bytes) -> None:         if self.size is not None:             self.size += len(data)          if self._in_memory:             self.file.write(data)         else:             await run_in_threadpool(self.file.write, data) ```  I have already created a PR which fixes the problem: https://github.com/encode/starlette/pull/2962   ### PoC See the discussion [here](https://github.com/encode/starlette/discussions/2927#discussioncomment-13721403) for steps on how to reproduce.  ### Impact To be honest, very low and not many users will be impacted. Parsing large forms is already CPU intensive so the additional IO block doesn't slow down `starlette` that much on systems with modern HDDs/SSDs. If someone is running on tape they might see a greater impact.\"}]}, {\"name\": \"statsd\", \"version\": \"4.0.1\", \"vulns\": []}, {\"name\": \"stevedore\", \"version\": \"5.4.1\", \"vulns\": []}, {\"name\": \"sudachidict-core\", \"version\": \"20250129\", \"vulns\": []}, {\"name\": \"sudachipy\", \"version\": \"0.6.10\", \"vulns\": []}, {\"name\": \"sympy\", \"version\": \"1.13.1\", \"vulns\": []}, {\"name\": \"tabulate\", \"version\": \"0.9.0\", \"vulns\": []}, {\"name\": \"tenacity\", \"version\": \"9.0.0\", \"vulns\": []}, {\"name\": \"tensorboard\", \"version\": \"2.19.0\", \"vulns\": []}, {\"name\": \"tensorboard-data-server\", \"version\": \"0.7.2\", \"vulns\": []}, {\"name\": \"termcolor\", \"version\": \"3.0.1\", \"vulns\": []}, {\"name\": \"testresources\", \"version\": \"2.0.1\", \"vulns\": []}, {\"name\": \"testscenarios\", \"version\": \"0.5.0\", \"vulns\": []}, {\"name\": \"testtools\", \"version\": \"2.7.2\", \"vulns\": []}, {\"name\": \"textblob\", \"version\": \"0.19.0\", \"vulns\": []}, {\"name\": \"thinc\", \"version\": \"8.3.4\", \"vulns\": []}, {\"name\": \"threadpoolctl\", \"version\": \"3.6.0\", \"vulns\": []}, {\"name\": \"tokenizers\", \"version\": \"0.21.1\", \"vulns\": []}, {\"name\": \"toml\", \"version\": \"0.10.2\", \"vulns\": []}, {\"name\": \"tomlkit\", \"version\": \"0.13.2\", \"vulns\": []}, {\"name\": \"tooz\", \"version\": \"6.3.0\", \"vulns\": []}, {\"name\": \"torch\", \"version\": \"2.6.0\", \"vulns\": [{\"id\": \"GHSA-3749-ghw9-m3mg\", \"fix_versions\": [\"2.7.1rc1\"], \"aliases\": [\"CVE-2025-2953\"], \"description\": \"A vulnerability, which was classified as problematic, has been found in PyTorch 2.6.0+cu124. Affected by this issue is the function torch.mkldnn_max_pool2d. The manipulation leads to denial of service. An attack has to be approached locally. The exploit has been disclosed to the public and may be used.\"}, {\"id\": \"GHSA-887c-mr87-cxwp\", \"fix_versions\": [], \"aliases\": [\"CVE-2025-3730\"], \"description\": \"A vulnerability, which was classified as problematic, was found in PyTorch 2.6.0. Affected is the function torch.nn.functional.ctc_loss of the file aten/src/ATen/native/LossCTC.cpp. The manipulation leads to denial of service. An attack has to be approached locally. The exploit has been disclosed to the public and may be used. The name of the patch is 46fc5d8e360127361211cb237d5f9eef0223e567. It is recommended to apply a patch to fix this issue.\"}]}, {\"name\": \"torchaudio\", \"version\": \"2.6.0\", \"vulns\": []}, {\"name\": \"tqdm\", \"version\": \"4.67.1\", \"vulns\": []}, {\"name\": \"trainer\", \"version\": \"0.0.36\", \"vulns\": []}, {\"name\": \"transformers\", \"version\": \"4.50.3\", \"vulns\": [{\"id\": \"GHSA-489j-g2vx-39wf\", \"fix_versions\": [\"4.51.0\"], \"aliases\": [\"CVE-2025-3262\"], \"description\": \"A Regular Expression Denial of Service (ReDoS) vulnerability was discovered in the huggingface/transformers repository, specifically in version 4.49.0. The vulnerability is due to inefficient regular expression complexity in the `SETTING_RE` variable within the `transformers/commands/chat.py` file. The regex contains repetition groups and non-optimized quantifiers, leading to exponential backtracking when processing 'almost matching' payloads. This can degrade application performance and potentially result in a denial-of-service (DoS) when handling specially crafted input strings. The issue is fixed in version 4.51.0.\"}, {\"id\": \"GHSA-q2wp-rjmx-x6x9\", \"fix_versions\": [\"4.51.0\"], \"aliases\": [\"CVE-2025-3263\"], \"description\": \"A Regular Expression Denial of Service (ReDoS) vulnerability was discovered in the Hugging Face Transformers library, specifically in the `get_configuration_file()` function within the `transformers.configuration_utils` module. The affected version is 4.49.0, and the issue is resolved in version 4.51.0. The vulnerability arises from the use of a regular expression pattern `config\\\\.(.*)\\\\.json` that can be exploited to cause excessive CPU consumption through crafted input strings, leading to catastrophic backtracking. This can result in model serving disruption, resource exhaustion, and increased latency in applications using the library.\"}, {\"id\": \"GHSA-jjph-296x-mrcr\", \"fix_versions\": [\"4.51.0\"], \"aliases\": [\"CVE-2025-3264\"], \"description\": \"A Regular Expression Denial of Service (ReDoS) vulnerability was discovered in the Hugging Face Transformers library, specifically in the `get_imports()` function within `dynamic_module_utils.py`. This vulnerability affects versions 4.49.0 and is fixed in version 4.51.0. The issue arises from a regular expression pattern `\\\\s*try\\\\s*:.*?except.*?:` used to filter out try/except blocks from Python code, which can be exploited to cause excessive CPU consumption through crafted input strings due to catastrophic backtracking. This vulnerability can lead to remote code loading disruption, resource exhaustion in model serving, supply chain attack vectors, and development pipeline disruption.\"}, {\"id\": \"GHSA-phhr-52qp-3mj4\", \"fix_versions\": [\"4.52.1\"], \"aliases\": [\"CVE-2025-3777\"], \"description\": \"Hugging Face Transformers versions up to 4.49.0 are affected by an improper input validation vulnerability in the `image_utils.py` file. The vulnerability arises from insecure URL validation using the `startswith()` method, which can be bypassed through URL username injection. This allows attackers to craft URLs that appear to be from YouTube but resolve to malicious domains, potentially leading to phishing attacks, malware distribution, or data exfiltration. The issue is fixed in version 4.52.1.\"}, {\"id\": \"GHSA-37mw-44qp-f5jm\", \"fix_versions\": [\"4.52.1\"], \"aliases\": [\"CVE-2025-3933\"], \"description\": \"A Regular Expression Denial of Service (ReDoS) vulnerability was discovered in the Hugging Face Transformers library, specifically within the DonutProcessor class's `token2json()` method. This vulnerability affects versions 4.51.3 and earlier, and is fixed in version 4.52.1. The issue arises from the regex pattern `<s_(.*?)>` which can be exploited to cause excessive CPU consumption through crafted input strings due to catastrophic backtracking. This vulnerability can lead to service disruption, resource exhaustion, and potential API service vulnerabilities, impacting document processing tasks using the Donut model.\"}]}, {\"name\": \"tts\", \"version\": \"0.22.0\", \"vulns\": []}, {\"name\": \"typeguard\", \"version\": \"4.4.2\", \"vulns\": []}, {\"name\": \"typer\", \"version\": \"0.15.2\", \"vulns\": []}, {\"name\": \"types-colorama\", \"version\": \"0.4.15.20240311\", \"vulns\": []}, {\"name\": \"types-defusedxml\", \"version\": \"0.7.0.20250516\", \"vulns\": []}, {\"name\": \"types-docutils\", \"version\": \"0.21.0.20250604\", \"vulns\": []}, {\"name\": \"types-pexpect\", \"version\": \"4.9.0.20250516\", \"vulns\": []}, {\"name\": \"types-psutil\", \"version\": \"7.0.0.20250601\", \"vulns\": []}, {\"name\": \"types-pygments\", \"version\": \"2.19.0.20250516\", \"vulns\": []}, {\"name\": \"types-pyyaml\", \"version\": \"6.0.12.20250516\", \"vulns\": []}, {\"name\": \"types-requests\", \"version\": \"2.32.4.20250611\", \"vulns\": []}, {\"name\": \"types-toml\", \"version\": \"0.10.8.20240310\", \"vulns\": []}, {\"name\": \"types-ujson\", \"version\": \"5.10.0.20250326\", \"vulns\": []}, {\"name\": \"typing-extensions\", \"version\": \"4.12.2\", \"vulns\": []}, {\"name\": \"typing-inspection\", \"version\": \"0.4.0\", \"vulns\": []}, {\"name\": \"tzdata\", \"version\": \"2025.1\", \"vulns\": []}, {\"name\": \"tzlocal\", \"version\": \"5.3.1\", \"vulns\": []}, {\"name\": \"umap-learn\", \"version\": \"0.5.7\", \"vulns\": []}, {\"name\": \"unidecode\", \"version\": \"1.3.8\", \"vulns\": []}, {\"name\": \"urllib3\", \"version\": \"2.4.0\", \"vulns\": [{\"id\": \"GHSA-48p4-8xcf-vxj5\", \"fix_versions\": [\"2.5.0\"], \"aliases\": [\"CVE-2025-50182\"], \"description\": \"urllib3 [supports](https://urllib3.readthedocs.io/en/2.4.0/reference/contrib/emscripten.html) being used in a Pyodide runtime utilizing the [JavaScript Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) or falling back on [XMLHttpRequest](https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest). This means you can use Python libraries to make HTTP requests from your browser or Node.js. Additionally, urllib3 provides [a mechanism](https://urllib3.readthedocs.io/en/2.4.0/user-guide.html#retrying-requests) to control redirects.  However, the `retries` and `redirect` parameters are ignored with Pyodide; the runtime itself determines redirect behavior.   ## Affected usages  Any code which relies on urllib3 to control the number of redirects for an HTTP request in a Pyodide runtime.   ## Impact  Redirects are often used to exploit SSRF vulnerabilities. An application attempting to mitigate SSRF or open redirect vulnerabilities by disabling redirects may remain vulnerable if a Pyodide runtime redirect mechanism is unsuitable.   ## Remediation  If you use urllib3 in Node.js, upgrade to a patched version of urllib3.  Unfortunately, browsers provide no suitable way which urllib3 can use: `XMLHttpRequest` provides no control over redirects, the Fetch API returns `opaqueredirect` responses lacking data when redirects are controlled manually. Expect default browser behavior for redirects.\"}, {\"id\": \"GHSA-pq67-6m6q-mj2v\", \"fix_versions\": [\"2.5.0\"], \"aliases\": [\"CVE-2025-50181\"], \"description\": \"urllib3 handles redirects and retries using the same mechanism, which is controlled by the `Retry` object. The most common way to disable redirects is at the request level, as follows:  ```python resp = urllib3.request(\\\"GET\\\", \\\"https://httpbin.org/redirect/1\\\", redirect=False) print(resp.status) # 302 ```  However, it is also possible to disable redirects, for all requests, by instantiating a `PoolManager` and specifying `retries` in a way that disable redirects:  ```python import urllib3  http = urllib3.PoolManager(retries=0)  # should raise MaxRetryError on redirect http = urllib3.PoolManager(retries=urllib3.Retry(redirect=0))  # equivalent to the above http = urllib3.PoolManager(retries=False)  # should return the first response  resp = http.request(\\\"GET\\\", \\\"https://httpbin.org/redirect/1\\\") ```  However, the `retries` parameter is currently ignored, which means all the above examples don't disable redirects.  ## Affected usages  Passing `retries` on `PoolManager` instantiation to disable redirects or restrict their number.  By default, requests and botocore users are not affected.  ## Impact  Redirects are often used to exploit SSRF vulnerabilities. An application attempting to mitigate SSRF or open redirect vulnerabilities by disabling redirects at the PoolManager level will remain vulnerable.  ## Remediation  You can remediate this vulnerability with the following steps:   * Upgrade to a patched version of urllib3. If your organization would benefit from the continued support of urllib3 1.x, please contact [sethmichaellarson@gmail.com](mailto:sethmichaellarson@gmail.com) to discuss sponsorship or contribution opportunities.  * Disable redirects at the `request()` level instead of the `PoolManager()` level.\"}]}, {\"name\": \"uvicorn\", \"version\": \"0.34.0\", \"vulns\": []}, {\"name\": \"vine\", \"version\": \"5.1.0\", \"vulns\": []}, {\"name\": \"voluptuous\", \"version\": \"0.15.2\", \"vulns\": []}, {\"name\": \"wasabi\", \"version\": \"1.1.3\", \"vulns\": []}, {\"name\": \"watchdog\", \"version\": \"6.0.0\", \"vulns\": []}, {\"name\": \"wcwidth\", \"version\": \"0.2.13\", \"vulns\": []}, {\"name\": \"weasel\", \"version\": \"0.4.1\", \"vulns\": []}, {\"name\": \"webob\", \"version\": \"1.8.9\", \"vulns\": []}, {\"name\": \"werkzeug\", \"version\": \"3.1.3\", \"vulns\": []}, {\"name\": \"wheel\", \"version\": \"0.45.1\", \"vulns\": []}, {\"name\": \"wrapt\", \"version\": \"1.17.2\", \"vulns\": []}, {\"name\": \"wsme\", \"version\": \"0.12.1\", \"vulns\": []}, {\"name\": \"yappi\", \"version\": \"1.6.10\", \"vulns\": []}, {\"name\": \"yaql\", \"version\": \"3.0.0\", \"vulns\": []}, {\"name\": \"yarl\", \"version\": \"1.18.3\", \"vulns\": []}, {\"name\": \"zipp\", \"version\": \"3.21.0\", \"vulns\": []}], \"fixes\": []}\n",
      "error": "Found 15 known vulnerabilities in 9 packages\n",
      "duration": 0.8125731945037842,
      "critical": true
    }
  ]
}